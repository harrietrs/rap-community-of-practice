{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RAP Community of Practice This material is maintained by the NHS Digital Data Science team . These resources are intended for those interested in adopting Reproducible Analytical Pipelines (RAP) . RAP is becoming the standard for creating analytical outputs in government; combining a number of ways of working that help to improve the reliability, transparency, and speed of statistics publications. Learn more on our Why is RAP important page. RAP in the NHS The Goldacre Review , tasked with finding ways to deliver better, broader, and safer use of NHS data for analysis and research, identified RAP as the essential element to ensure high-quality analysis. The Data Science team at NHS Digital have been championing RAP practices and providing support for analytical teams across our organisation. We have published these resources in the spirit of openness and transparency, and in the hope that other teams in other organisations may find them useful. Learn more about our RAP service .","title":"Home"},{"location":"#rap-community-of-practice","text":"This material is maintained by the NHS Digital Data Science team . These resources are intended for those interested in adopting Reproducible Analytical Pipelines (RAP) . RAP is becoming the standard for creating analytical outputs in government; combining a number of ways of working that help to improve the reliability, transparency, and speed of statistics publications. Learn more on our Why is RAP important page.","title":"RAP Community of Practice"},{"location":"#rap-in-the-nhs","text":"The Goldacre Review , tasked with finding ways to deliver better, broader, and safer use of NHS data for analysis and research, identified RAP as the essential element to ensure high-quality analysis. The Data Science team at NHS Digital have been championing RAP practices and providing support for analytical teams across our organisation. We have published these resources in the spirit of openness and transparency, and in the hope that other teams in other organisations may find them useful. Learn more about our RAP service .","title":"RAP in the NHS"},{"location":"about/","text":"RAP Community of Practice Aims This community of practice aims to support teams in adopting RAP practices through: Developing learning materials including reusable templates Offering support as teams establish new working practices See more about our aims . Support If your team is embarking upon a RAP journey, you should look at our why RAP is important page and think about which levels of RAP that you think is most realistic. There are several forums where you can introduce yourself, ask for help, or discuss different approaches: NHS Digital RAP [MS Teams page][rap community of practice teams page] (internal to NHS Digital). RAP Slack channel from the NHS-R community. RAP collaboration Slack channel on the cross-government data science Slack Workspace (open to all civil servants, including NHS staff). See more about how we can support you and your team. Tutorials and resources As we work alongside teams at NHS Digital, we try to produce reusable learning materials. We try to avoid reproducing guidance that is easily available online. Instead, we link to lots of external resources where you can self-serve. We also have created some bespoke guidance that lays out how you would accomplish these practices in the NHS Digital setting, which are available internally. The 'Implementing RAP' folder contains guides on how to start using RAP principles within your own team such as, using code reviews , publishing your code or useful tools . The 'training resources' folder contains resources on: Git Python PySpark R Contribute These resources are intended to help people get started with RAP. We welcome all feedback. If you think of something worth including, improving, or want to contribute, please raise an issue on GitHub or contact the team via the MS Teams page (internal to NHS Digital), or email . Licence Unless stated otherwise, the codebase is released under the MIT Licence. This covers both the codebase and any sample code in the documentation. This collection of resources is \u00a9 Crown copyright and available under the terms of the Open Government 3.0 licence.","title":"About"},{"location":"about/#rap-community-of-practice","text":"","title":"RAP Community of Practice"},{"location":"about/#aims","text":"This community of practice aims to support teams in adopting RAP practices through: Developing learning materials including reusable templates Offering support as teams establish new working practices See more about our aims .","title":"Aims"},{"location":"about/#support","text":"If your team is embarking upon a RAP journey, you should look at our why RAP is important page and think about which levels of RAP that you think is most realistic. There are several forums where you can introduce yourself, ask for help, or discuss different approaches: NHS Digital RAP [MS Teams page][rap community of practice teams page] (internal to NHS Digital). RAP Slack channel from the NHS-R community. RAP collaboration Slack channel on the cross-government data science Slack Workspace (open to all civil servants, including NHS staff). See more about how we can support you and your team.","title":"Support"},{"location":"about/#tutorials-and-resources","text":"As we work alongside teams at NHS Digital, we try to produce reusable learning materials. We try to avoid reproducing guidance that is easily available online. Instead, we link to lots of external resources where you can self-serve. We also have created some bespoke guidance that lays out how you would accomplish these practices in the NHS Digital setting, which are available internally. The 'Implementing RAP' folder contains guides on how to start using RAP principles within your own team such as, using code reviews , publishing your code or useful tools . The 'training resources' folder contains resources on: Git Python PySpark R","title":"Tutorials and resources"},{"location":"about/#contribute","text":"These resources are intended to help people get started with RAP. We welcome all feedback. If you think of something worth including, improving, or want to contribute, please raise an issue on GitHub or contact the team via the MS Teams page (internal to NHS Digital), or email .","title":"Contribute"},{"location":"about/#licence","text":"Unless stated otherwise, the codebase is released under the MIT Licence. This covers both the codebase and any sample code in the documentation. This collection of resources is \u00a9 Crown copyright and available under the terms of the Open Government 3.0 licence.","title":"Licence"},{"location":"support/","text":"Support If your team is embarking upon a RAP journey, you should look at our benefits and aims of RAP page and think about which levels of RAP that you want to target. From there, we recommend reaching out for some in-person support. Support for NHS Digital colleagues If you work at NHS Digital , the Data Science RAP squad are your main RAP Champions: get in touch! We you can offer some in-person support in several forms: Reviewing your RAP work and assessing your progress against the levels of RAP Peer review of code Workshops for a specific RAP capability Consultancy style engagement where we plan a migration strategy Pair coding Shadowing another team There is also a growing community of RAP experts outside the Data Science RAP squad - you can find them on our [RAP Community of Practice Teams Page] (NHS Digital only) Support for other colleagues If you're not in NHS Digital , the RAP Champions Network can offer support through: helping you understand why and how they implemented RAP in your department and why it worked for them sharing code via platforms like GitHub so that others can learn from and adapt what has been done shadowing & mentoring opportunities peer review They also have more guidance on how to get started, and get help here . There are several forums where you can introduce yourself, ask for help, or discuss different approaches: NHS Digital RAP [MS Teams page][rap community of practice teams page] (internal to NHS Digital). RAP Slack channel from the NHS-R community. RAP collaboration Slack channel on the cross-government data science Slack Workspace (open to all civil servants, including NHS staff).","title":"Support"},{"location":"support/#support","text":"If your team is embarking upon a RAP journey, you should look at our benefits and aims of RAP page and think about which levels of RAP that you want to target. From there, we recommend reaching out for some in-person support.","title":"Support"},{"location":"support/#support-for-nhs-digital-colleagues","text":"If you work at NHS Digital , the Data Science RAP squad are your main RAP Champions: get in touch! We you can offer some in-person support in several forms: Reviewing your RAP work and assessing your progress against the levels of RAP Peer review of code Workshops for a specific RAP capability Consultancy style engagement where we plan a migration strategy Pair coding Shadowing another team There is also a growing community of RAP experts outside the Data Science RAP squad - you can find them on our [RAP Community of Practice Teams Page] (NHS Digital only)","title":"Support for NHS Digital colleagues"},{"location":"support/#support-for-other-colleagues","text":"If you're not in NHS Digital , the RAP Champions Network can offer support through: helping you understand why and how they implemented RAP in your department and why it worked for them sharing code via platforms like GitHub so that others can learn from and adapt what has been done shadowing & mentoring opportunities peer review They also have more guidance on how to get started, and get help here . There are several forums where you can introduce yourself, ask for help, or discuss different approaches: NHS Digital RAP [MS Teams page][rap community of practice teams page] (internal to NHS Digital). RAP Slack channel from the NHS-R community. RAP collaboration Slack channel on the cross-government data science Slack Workspace (open to all civil servants, including NHS staff).","title":"Support for other colleagues"},{"location":"useful_links/","text":"Useful links This is a list, we have collated, of RAP resources throughout the Government and other areas. Let us know if you have any links to useful information, resources or guides that could be added to the list. Strategic The Government Analysis Function has published their RAP strategy . It's very good for understanding what role we all have to play in RAP. The Goldacre Review mentions RAP extensively as a key tool in improving the way we use patient data. Here is a helpful summary of the above documents and others which helps understand RAP and why it is important in the context of UK data policy developments. Guidance and Standards The AQUA book of analytical standards . The ONS best practice team have a useful website - often called the Quack book covering many of the same topics we cover here. Their best-practice checklist is particularly useful. The Turing Institute has published The Turing Way handbook to reproducible, ethical and collaborative data science Examples, Challenges, Benefits Here is a good high-level overview of RAP . It explains the context of RAP in government statistics and gives some history to the work. The GSS have produced a list of the benefits that come from RAP . The Statistics Authority has published this excellent report on overcoming barriers to RAP adoption . Alston, J. M., and Rick, J. A.. 2020. A Beginner's Guide to Conducting Reproducible Research. Bull Ecol Soc Am 102(2):e01801. https://doi.org/10.1002/bes2.1801 The GSS training portal offers an Introduction to RAP course The Government Analysis Function also did some research on the rollout of RAP across different deparments . Community spaces There are several slack channels that discuss RAP and related topics: the govdatascience.slack.com RAP channel , the NHS-R community , and the NHS-pycom community We have an MS Teams page (internal to NHS Digital)","title":"Useful links"},{"location":"useful_links/#useful-links","text":"This is a list, we have collated, of RAP resources throughout the Government and other areas. Let us know if you have any links to useful information, resources or guides that could be added to the list.","title":"Useful links"},{"location":"useful_links/#strategic","text":"The Government Analysis Function has published their RAP strategy . It's very good for understanding what role we all have to play in RAP. The Goldacre Review mentions RAP extensively as a key tool in improving the way we use patient data. Here is a helpful summary of the above documents and others which helps understand RAP and why it is important in the context of UK data policy developments.","title":"Strategic"},{"location":"useful_links/#guidance-and-standards","text":"The AQUA book of analytical standards . The ONS best practice team have a useful website - often called the Quack book covering many of the same topics we cover here. Their best-practice checklist is particularly useful. The Turing Institute has published The Turing Way handbook to reproducible, ethical and collaborative data science","title":"Guidance and Standards"},{"location":"useful_links/#examples-challenges-benefits","text":"Here is a good high-level overview of RAP . It explains the context of RAP in government statistics and gives some history to the work. The GSS have produced a list of the benefits that come from RAP . The Statistics Authority has published this excellent report on overcoming barriers to RAP adoption . Alston, J. M., and Rick, J. A.. 2020. A Beginner's Guide to Conducting Reproducible Research. Bull Ecol Soc Am 102(2):e01801. https://doi.org/10.1002/bes2.1801 The GSS training portal offers an Introduction to RAP course The Government Analysis Function also did some research on the rollout of RAP across different deparments .","title":"Examples, Challenges, Benefits"},{"location":"useful_links/#community-spaces","text":"There are several slack channels that discuss RAP and related topics: the govdatascience.slack.com RAP channel , the NHS-R community , and the NHS-pycom community We have an MS Teams page (internal to NHS Digital)","title":"Community spaces"},{"location":"implementing_RAP/code-review/","text":"Code review Why should I care? Code review is one of the most important practices that a team can adopt. The benefits include: Ensuring the overall code health of the codebase is improving over time. Sharing knowledge and building skill across the team. Reducing turnover risk by gradually making the team familiar with all aspects of the code. Reduce the likelihood of breaking the code by making sure a second pair of eyes has checked. Questions to ask during code review Does the code work as expected? Does the work actually meet the acceptance criteria in the user story? Is the code well structured or would it benefit from redesign/refactoring? Could the code be made simpler? Is this level of complexity justified? Would another developer be able to easily understand and use this code when they come across it in the future? Does the code include appropriate tests? Does the code follow our style guides? These questions are not intended to be comprehensive but rather to give you a starting point for the discussion. Other considerations Code should be reviewed with someone after submitting a merge request and before merging to the destination branch. The reviewer should consider whether the code needs to be refactored or redesigned. Code review is not about more experienced people telling others what to do. Code review should be reviewed by everyone in the team - that is how we develop together. In fact, you will often get more benefit from a situation where a less experienced analyst reviews the code of a more experienced person. Be kind. The process is intended to be a constructive dialogue where both parties leave more knowledgable than they began. Your discussion will be most effective if undertaken in the spirit of collaboration, not criticism. Code review takes time. You should ensure your sprint planning gives this important process the time it deserves. You should expect that code review will lead to several requests for changes. A key point here is that there is no such thing as \u201cperfect\u201d code \u2014 there is only better code. Reviewers should not require the author to polish every tiny piece of a merge request before granting approval. Rather, the reviewer should balance the need to make forward progress compared to the importance of the changes they are suggesting. Instead of seeking perfection, what a reviewer should seek is continuous improvement. Quality assurance checklist This checklist from the ONS best practice team might be a useful guide for thinking about code review: Quality assurance checklist for analysis and research guidance . You do not need to tick every box but rather use them as a prompt. You can also view our own checklist in the Quality Assuring Analytical Outputs file. External links Atlassian Code Reviews","title":"Code review"},{"location":"implementing_RAP/code-review/#code-review","text":"","title":"Code review"},{"location":"implementing_RAP/code-review/#why-should-i-care","text":"Code review is one of the most important practices that a team can adopt. The benefits include: Ensuring the overall code health of the codebase is improving over time. Sharing knowledge and building skill across the team. Reducing turnover risk by gradually making the team familiar with all aspects of the code. Reduce the likelihood of breaking the code by making sure a second pair of eyes has checked.","title":"Why should I care?"},{"location":"implementing_RAP/code-review/#questions-to-ask-during-code-review","text":"Does the code work as expected? Does the work actually meet the acceptance criteria in the user story? Is the code well structured or would it benefit from redesign/refactoring? Could the code be made simpler? Is this level of complexity justified? Would another developer be able to easily understand and use this code when they come across it in the future? Does the code include appropriate tests? Does the code follow our style guides? These questions are not intended to be comprehensive but rather to give you a starting point for the discussion.","title":"Questions to ask during code review"},{"location":"implementing_RAP/code-review/#other-considerations","text":"Code should be reviewed with someone after submitting a merge request and before merging to the destination branch. The reviewer should consider whether the code needs to be refactored or redesigned. Code review is not about more experienced people telling others what to do. Code review should be reviewed by everyone in the team - that is how we develop together. In fact, you will often get more benefit from a situation where a less experienced analyst reviews the code of a more experienced person. Be kind. The process is intended to be a constructive dialogue where both parties leave more knowledgable than they began. Your discussion will be most effective if undertaken in the spirit of collaboration, not criticism. Code review takes time. You should ensure your sprint planning gives this important process the time it deserves. You should expect that code review will lead to several requests for changes. A key point here is that there is no such thing as \u201cperfect\u201d code \u2014 there is only better code. Reviewers should not require the author to polish every tiny piece of a merge request before granting approval. Rather, the reviewer should balance the need to make forward progress compared to the importance of the changes they are suggesting. Instead of seeking perfection, what a reviewer should seek is continuous improvement.","title":"Other considerations"},{"location":"implementing_RAP/code-review/#quality-assurance-checklist","text":"This checklist from the ONS best practice team might be a useful guide for thinking about code review: Quality assurance checklist for analysis and research guidance . You do not need to tick every box but rather use them as a prompt. You can also view our own checklist in the Quality Assuring Analytical Outputs file.","title":"Quality assurance checklist"},{"location":"implementing_RAP/code-review/#external-links","text":"Atlassian Code Reviews","title":"External links"},{"location":"implementing_RAP/coding-best-practice/","text":"Coding Best Practice This page is still under construction, however you can find guidance on this at these pages: General Code Development Tidy data and Notebook vs IDE development Python Our general Python guidance incl. guidance on why to use functions ! PySpark style guide Very comprehensive Python guidance here on almost every aspect of Python Google Python style guide , which our PySpark style guide is heavily based on R R guidance - in progress","title":"Coding best practice"},{"location":"implementing_RAP/coding-best-practice/#coding-best-practice","text":"This page is still under construction, however you can find guidance on this at these pages: General Code Development Tidy data and Notebook vs IDE development Python Our general Python guidance incl. guidance on why to use functions ! PySpark style guide Very comprehensive Python guidance here on almost every aspect of Python Google Python style guide , which our PySpark style guide is heavily based on R R guidance - in progress","title":"Coding Best Practice"},{"location":"implementing_RAP/how-to-publish-your-code-in-the-open/","text":"How to Publish your Code in the Open In NHS Digital we have committed to publishing more and more of our code over time to improve the transparency of our analytical work. Why publish your code? The Government's Digital Service Standard 12th principle states that all publicly funded code should be open, reusable and available under appropriate licences. The main benefits of publishing your code are: It increases transparency on NHS work. This can include publication/dashboard methodologies, but also code ownership. It increases collaboration and knowledge sharing across cross-department teams, external users and developers. No time is wasted on requesting permissions or access to code repositories. Easier to share and align on standards across the health sector. Knowing that your code will be published will lead to your overall code health increasing as analysts and developers will take greater care in ensuring best coding practices and standards are applied. Reduce burden by sharing and reusing code. Internal and external users can learn from your code repositories and templates, and apply it to their work or personal projects. The Government Digital Service released an informative video on the benefits of coding in the open, from the start of a project. Analytical leadership commitment to RAP code publishing Releasing code externally is part of the RAP project, the targets below are agreed by the analytical leadership. The set goal of publishing open code as a standard is October 2023. % of publications on RAP Month Published code Jan-22 0% Apr-22 5% Jul-22 10% Oct-22 20% Jan-23 40% Apr-23 60% Jul-23 80% Oct-23 100% How to prepare your code and repository We have designed a process that will guide teams from the start of a project through its core development, its publishing assurance phase and reaching the final step \"Publish code on NHS Digital Github repository\". The process in place will ensure no sensitive data or algorithms will be published, removing any risks associated with any type of sensitive information being released. Projects and publications with the aim to be published should follow the Fit-for-publishing workflow : Workflow initial steps After designing and developing the source code on NHS Digital's internal Gitlab platform, you should ensure at this step that your repository, code and tests are all set and ready for external review, having addressed any security concerns. Your project is ready to go through Senior Manager approval for publishing and enter the Publishing Assurance phase, once approval is granted. At the beginning of this phase a snapshot of the project's repository is taken (download the zip file of the branch) or a separate GitLab branch is created for the review. The purpose of the snapshot is to remove all repository history and previous commits. After creating the snapshot, the next step is to apply the Fit-for-publishing checklist : Fit for publishing checklist This checklist covers the review process in place to ensure that the code is: Fit for purpose Well documented No credentials or personally identifiable information left in the repository No data should be stored in the snapshot. To access the checklist click on the link below and select the Download option: PDF version for viewing: Fit for publishing checklist PDF Download the Work doc version to edit: Fit for publishing checklist Word For each subsection of the checklist: an internal reviewer (someone who worked on the project) is assigned by the development team to check and add comments and suggestions to be implemented. Once those are reviewed and implemented the checklist is passed on to an external reviewer (outside of the development team) to carry out checks and add any comments and suggestions in the external review columns. Once the external review is complete, each checklist section is assigned a RAG status by the project's lead analyst ( Confirmation of Readiness by Lead Analyst step in the workflow diagram above ). Once the repository is granted the final confirmation for publishing from a Senior Manager (as per the workflow diagram shown above), you can then proceed to the final step, which is going live with your Github repository on the NHS Digital public repository . Please note: Should the workflow at any point reach to a failed step, follow the workflow diagram towards the revision and iteration steps. Further reading Sharing Code in the Open by NHSX Be open and use open source The benefits of coding in the open Open source repositories by the Government Digital Service","title":"How to publish your code in the open"},{"location":"implementing_RAP/how-to-publish-your-code-in-the-open/#how-to-publish-your-code-in-the-open","text":"In NHS Digital we have committed to publishing more and more of our code over time to improve the transparency of our analytical work.","title":"How to Publish your Code in the Open"},{"location":"implementing_RAP/how-to-publish-your-code-in-the-open/#why-publish-your-code","text":"The Government's Digital Service Standard 12th principle states that all publicly funded code should be open, reusable and available under appropriate licences. The main benefits of publishing your code are: It increases transparency on NHS work. This can include publication/dashboard methodologies, but also code ownership. It increases collaboration and knowledge sharing across cross-department teams, external users and developers. No time is wasted on requesting permissions or access to code repositories. Easier to share and align on standards across the health sector. Knowing that your code will be published will lead to your overall code health increasing as analysts and developers will take greater care in ensuring best coding practices and standards are applied. Reduce burden by sharing and reusing code. Internal and external users can learn from your code repositories and templates, and apply it to their work or personal projects. The Government Digital Service released an informative video on the benefits of coding in the open, from the start of a project.","title":"Why publish your code?"},{"location":"implementing_RAP/how-to-publish-your-code-in-the-open/#analytical-leadership-commitment-to-rap-code-publishing","text":"Releasing code externally is part of the RAP project, the targets below are agreed by the analytical leadership. The set goal of publishing open code as a standard is October 2023. % of publications on RAP Month Published code Jan-22 0% Apr-22 5% Jul-22 10% Oct-22 20% Jan-23 40% Apr-23 60% Jul-23 80% Oct-23 100%","title":"Analytical leadership commitment to RAP code publishing"},{"location":"implementing_RAP/how-to-publish-your-code-in-the-open/#how-to-prepare-your-code-and-repository","text":"We have designed a process that will guide teams from the start of a project through its core development, its publishing assurance phase and reaching the final step \"Publish code on NHS Digital Github repository\". The process in place will ensure no sensitive data or algorithms will be published, removing any risks associated with any type of sensitive information being released. Projects and publications with the aim to be published should follow the Fit-for-publishing workflow :","title":"How to prepare your code and repository"},{"location":"implementing_RAP/how-to-publish-your-code-in-the-open/#workflow-initial-steps","text":"After designing and developing the source code on NHS Digital's internal Gitlab platform, you should ensure at this step that your repository, code and tests are all set and ready for external review, having addressed any security concerns. Your project is ready to go through Senior Manager approval for publishing and enter the Publishing Assurance phase, once approval is granted. At the beginning of this phase a snapshot of the project's repository is taken (download the zip file of the branch) or a separate GitLab branch is created for the review. The purpose of the snapshot is to remove all repository history and previous commits. After creating the snapshot, the next step is to apply the Fit-for-publishing checklist :","title":"Workflow initial steps"},{"location":"implementing_RAP/how-to-publish-your-code-in-the-open/#fit-for-publishing-checklist","text":"This checklist covers the review process in place to ensure that the code is: Fit for purpose Well documented No credentials or personally identifiable information left in the repository No data should be stored in the snapshot. To access the checklist click on the link below and select the Download option: PDF version for viewing: Fit for publishing checklist PDF Download the Work doc version to edit: Fit for publishing checklist Word For each subsection of the checklist: an internal reviewer (someone who worked on the project) is assigned by the development team to check and add comments and suggestions to be implemented. Once those are reviewed and implemented the checklist is passed on to an external reviewer (outside of the development team) to carry out checks and add any comments and suggestions in the external review columns. Once the external review is complete, each checklist section is assigned a RAG status by the project's lead analyst ( Confirmation of Readiness by Lead Analyst step in the workflow diagram above ). Once the repository is granted the final confirmation for publishing from a Senior Manager (as per the workflow diagram shown above), you can then proceed to the final step, which is going live with your Github repository on the NHS Digital public repository . Please note: Should the workflow at any point reach to a failed step, follow the workflow diagram towards the revision and iteration steps.","title":"Fit for publishing checklist"},{"location":"implementing_RAP/how-to-publish-your-code-in-the-open/#further-reading","text":"Sharing Code in the Open by NHSX Be open and use open source The benefits of coding in the open Open source repositories by the Government Digital Service","title":"Further reading"},{"location":"implementing_RAP/notebooks_versus_ide_development/","text":"Development tools What is this guide for? This guide compares common tools used by data analysts/scientists when developing code: iPython notebooks, such as Jupyter or Databricks - this guide will focus primarily on the latter Integrated Development Environments (IDEs), such as Visual Studio Code, PyCharm, Spyder Specifically this guide compares writing notebooks in browser-based environments (such as Jupyter or Databricks) with writing text-based files (e.g. Python files with a .py extension) in IDEs. At the end of this guide, the reader have a better understanding of the relative pros/cons of developing in browser-based notebooks versus IDEs and when/why they might choose one over the other for a given project. What if I don't have a choice? Clearly, in some cases / projects, due to pre-existing infrastructure limitations, this choice is made for you. Hopefully this guide will still prove useful as: You can refer back to it in future projects when you have more flexibility You can use the information to support requests for more / improved options when it comes to tooling Why should I care? A core component of the development process of a reproducible analytical pipeline is the tool used to actually develop the code. The appropriate tool(s) will support the work being done - e.g. by spotting syntax errors or potential bugs - while inappropriate tools may prove a hindrance - e.g. autocompletion designed for a different language to the one being written. The choice of development tools can have consequences for many aspects of the project including, but not limited to: Implementation of deliverables : are you producing a software package or a piece of analysis? Who is the target audience? How will they interact with the outputs? QA / review & testing processes : how will your outputs be reviewed / checked? Will your code need to be updated and reviewed again later? Will everything be supervised by a human or do you need automated checks? Reproducibility : how will someone else reproduce your work? Are there any risks based on the tools you've used? E.g. think of notebook cells run out of order Delivery speed & timescales : do you need to quickly produce some charts / statistics? Will your project benefit from a longer time to set up over the lifespan? Ultimately, while there is not necessarily a 'right' or 'wrong' choice, it is worth taking the time to consider the options and pick the most appropriate tool for the job. Notebooks vs. IDEs The table below gives a summary of the features of notebooks compared to IDEs. For more details on each point see the detailed breakdown in the following sections. Feature Notebooks IDEs Interactive outputs (tables, plots, etc) O X* Storytelling and sharing results O X Benefits out-of-the-box (minimal configuration) O X Deterministic outputs (i.e. same result every run) X O Supports modular code structure & packages X O Supports unit testing frameworks X O Nice version control (e.g. readable git diffs) X O Autocomplete / auto-formatting & syntax highlighting X O Compatible with sharing code external to NHS Digital X O Key X: Unavailable / hard to configure O: Built-in / easy to set up *See Interactive cells in IDEs? Benefits of notebooks Notebooks are a great tool for data exploration and quick ad hoc discovery. However they are not great for collaborative projects, and sharing notebooks can lead to accidental PII disclosure . Since notebooks are bundled with outputs from the most recent run, you will need to be extremely careful about accidental PII disclosure if you are sharing notebooks. Interactive outputs The primary feature of notebooks of any flavour is the interactivity. Code in notebooks is written in snippets within sections called 'cells' where each cell has its own output; often interactive in some way - e.g. tables where the user can scroll through and select rows and columns, plots which can be zoomed or panned, and so on. This is useful for the exploration, analysis and prototyping phases of a project , where the aim is to understand a dataset, either by creating visualisations or by digging deeper into some of the row-level data. Interactivity ith scripts/modules/etc in an IDE isn't always possible or especially easy, which makes notebooks a good choice for ad hoc analytical work, especially earlier on in the project timeline. Markdown cells Notebook cells can contain different types of content: Python, R and SQL but also Markdown which allows you to format text. This means you can add additional non-code content to notebooks, for example adding headings, bullet points, inserting images, and many other things. Cells containing code can be hidden to reveal only their outputs so that you can create a fully interactive, code-free presentation of analysis- great for presenting findings to stakeholders. Notebooks can be exported as HTML files and the full analysis shared online (i.e. embedded in a webpage) or offline (i.e. as a static file). This saves you from having to create a separate report, and manually copying outputs over, and outputs automatically updating if changes are made to the code. Quick to get up and running Notebooks are generally set up to work out of the box, enabling the users to connect to the data and get started working right away. IDEs on the other hand either come in two flavours: big, feature-heavy ones take a lot of set up and plugins to get going; lightweight ones like Atom or Visual Studio Code are often little more than text editors out-of-the-box. While IDEs offer almost limitless customisability (you can even write your own themes and extensions for a lot of them!) and powerful, development-enhancing functionality, they often take more time to get set up in the first place. IDEs require you to have an adequate environment for them to be installed, used and customised (i.e. installing useful extensions). This can be prohibitive, for instance at NHS Digital where installing software to local machines is restricted. Benefits of IDEs The best IDEs incorporate some of the benefits of notebooks, while also allowing better version control, and easier development of reproducible code. Deterministic code For data pipelines which are intended to be rerun multiple times, potentially by different systems/users, scripts in an IDE adhere to the RAP principles more readily than notebooks . In a notebook, it is possible for two analysts to generate different results from the same notebooks, based on the order in which cells are executed. In long notebooks it can be hard to spot this happening or where it originated. Writing a script within an IDE makes it difficult for this to occur as the same set of logical steps will be executed in the same order every time the script runs. Modularisation In an IDE it's easy to write reusable pieces of code and separate these out into clearly named files and folders that can be imported into the main pipeline. For larger projects, writing code in a modular structure within an IDE improves code quality, readability and maintainability compared to having extensive notebooks. This is more challenging with notebooks, because importing functionality from notebooks can be complicated: Importing code in Jupyter notebooks Jupyter notebooks require quite a bit of arduous coding gymnastics to perform what in Python files would be simple imports ( importing Jupyter notebooks ) Importing code in Databricks notebooks Databricks provides a magic %run command, which is essentially equivalent of inserting all the cells from the notebook being run into the current notebook ( %run command in Databricks ) When %run is called, the two notebooks share the same scope ; this means that any objects (variables, functions etc.) available for reading and writing in the cells of one notebook are available for reading and writing in the cells of the other. The parent (left) errors because the child (right) causes hidden changes within the scope After the %run there can be references to objects that are hidden in the child notebook. This can make code harder to understand, introduce unintended side effects and lead to bugs . Unit testing Please see the unit tests guide for more information about unit testing. It is much easier to implement unit tests with scripts in an IDE than in notebooks. This is partly due to the modular structure : code can be broken down into functions and classes contained within small, manageable files and subdirectories; each test then only interacts with the relevant part of the code which makes debugging easier. Testing frameworks, such as unittest and pytest, do not properly support notebooks. While testing is possible in a notebook workflow, it is hard to isolate code in each cell and thus to trace back errors. Version control Notebooks are inherently difficult to review and audit through version control software like Git . They are stored in a JSON format, which is rendered nicely on screen for the user when viewed through the tool (e.g. Jupyter / Databricks), but which makes it hard to see differences between versions. Git diff from a (simple!) Jupyter notebook: lots to look at! Files in IDEs are generally simpler to version control. It is easy to compare a line in a file between two given versions since everything is designed to be read by humans. This makes it easy to understand what has changed between each version of a file and to rollback to previous versions if required. Git diff from a Python file: much clearer! Package building Python packages are essentially a collection of files bundled together containing the code and its dependencies. To build a Python package that other developers can install, use of other packages like setuptools or poetry is required. Unfortunately, these tools offer poor support for notebooks, so if one of the deliverables for a project you're working on is the code packaged up in a way that others can reuse and implement in their projects, then an IDE workflow is realistically the only way to achieve this. Additional features Notebooks are focused on analytical work, but IDEs are designed for software development: they offer features to support the person writing the code, such as auto-completion & formatting as well as syntax highlighting and linting. These features can make writing code in IDEs faster and easier with the result that code is easier to read and of higher quality . While some such features exist in notebooks, such as auto-closure of brackets and quotations, the scope of the functionality is quite limited. Interactive cells in IDEs Some IDEs offer ways to use Jupyter-like cells within scripts/modules: special comments in the code (e.g. VSCode and Spyder use # %% ) are used to mark different 'cells'; the IDE or some extensions (e.g. the Python extension in VSCode) will identify these comments and allow code within a cell to be run 'interactively'; results from running interactive cells are stored in an IPython kernel that the IDE is running, but no outputs are saved within the code or the metadata as they with Jupyter notebooks. Using interactive cells in IDEs can offer a nice balance for analysts implementing RAP in their working practices. This comes with all the benefits of an IDE workflow , without risking accidental PII disclosure. External links Quality Assurance of Code for Analysis and Research Importing Jupyter notebooks %run command in Databricks See also this video by Joel Grus: I don't like notebooks . Where do I go from here? Hopefully, it is clear that there is no 'right' or 'wrong' choice when it comes to workflow: rather it is a question of which workflow is going to better support the needs of the project. We have created a suggested workflow that is not intended to be seen as the recommended or best approach, but offers a good starting point for analysts & data scientists looking to get started setting up a workflow. img[src*=\"#bigimg\"] { width:100%; } img[src*=\"#smallimg\"]{ width:45%; }","title":"Notebooks vs IDE development"},{"location":"implementing_RAP/notebooks_versus_ide_development/#development-tools","text":"","title":"Development tools"},{"location":"implementing_RAP/notebooks_versus_ide_development/#what-is-this-guide-for","text":"This guide compares common tools used by data analysts/scientists when developing code: iPython notebooks, such as Jupyter or Databricks - this guide will focus primarily on the latter Integrated Development Environments (IDEs), such as Visual Studio Code, PyCharm, Spyder Specifically this guide compares writing notebooks in browser-based environments (such as Jupyter or Databricks) with writing text-based files (e.g. Python files with a .py extension) in IDEs. At the end of this guide, the reader have a better understanding of the relative pros/cons of developing in browser-based notebooks versus IDEs and when/why they might choose one over the other for a given project. What if I don't have a choice? Clearly, in some cases / projects, due to pre-existing infrastructure limitations, this choice is made for you. Hopefully this guide will still prove useful as: You can refer back to it in future projects when you have more flexibility You can use the information to support requests for more / improved options when it comes to tooling","title":"What is this guide for?"},{"location":"implementing_RAP/notebooks_versus_ide_development/#why-should-i-care","text":"A core component of the development process of a reproducible analytical pipeline is the tool used to actually develop the code. The appropriate tool(s) will support the work being done - e.g. by spotting syntax errors or potential bugs - while inappropriate tools may prove a hindrance - e.g. autocompletion designed for a different language to the one being written. The choice of development tools can have consequences for many aspects of the project including, but not limited to: Implementation of deliverables : are you producing a software package or a piece of analysis? Who is the target audience? How will they interact with the outputs? QA / review & testing processes : how will your outputs be reviewed / checked? Will your code need to be updated and reviewed again later? Will everything be supervised by a human or do you need automated checks? Reproducibility : how will someone else reproduce your work? Are there any risks based on the tools you've used? E.g. think of notebook cells run out of order Delivery speed & timescales : do you need to quickly produce some charts / statistics? Will your project benefit from a longer time to set up over the lifespan? Ultimately, while there is not necessarily a 'right' or 'wrong' choice, it is worth taking the time to consider the options and pick the most appropriate tool for the job.","title":"Why should I care?"},{"location":"implementing_RAP/notebooks_versus_ide_development/#notebooks-vs-ides","text":"The table below gives a summary of the features of notebooks compared to IDEs. For more details on each point see the detailed breakdown in the following sections. Feature Notebooks IDEs Interactive outputs (tables, plots, etc) O X* Storytelling and sharing results O X Benefits out-of-the-box (minimal configuration) O X Deterministic outputs (i.e. same result every run) X O Supports modular code structure & packages X O Supports unit testing frameworks X O Nice version control (e.g. readable git diffs) X O Autocomplete / auto-formatting & syntax highlighting X O Compatible with sharing code external to NHS Digital X O Key X: Unavailable / hard to configure O: Built-in / easy to set up *See Interactive cells in IDEs?","title":"Notebooks vs. IDEs"},{"location":"implementing_RAP/notebooks_versus_ide_development/#benefits-of-notebooks","text":"Notebooks are a great tool for data exploration and quick ad hoc discovery. However they are not great for collaborative projects, and sharing notebooks can lead to accidental PII disclosure . Since notebooks are bundled with outputs from the most recent run, you will need to be extremely careful about accidental PII disclosure if you are sharing notebooks.","title":"Benefits of notebooks"},{"location":"implementing_RAP/notebooks_versus_ide_development/#interactive-outputs","text":"The primary feature of notebooks of any flavour is the interactivity. Code in notebooks is written in snippets within sections called 'cells' where each cell has its own output; often interactive in some way - e.g. tables where the user can scroll through and select rows and columns, plots which can be zoomed or panned, and so on. This is useful for the exploration, analysis and prototyping phases of a project , where the aim is to understand a dataset, either by creating visualisations or by digging deeper into some of the row-level data. Interactivity ith scripts/modules/etc in an IDE isn't always possible or especially easy, which makes notebooks a good choice for ad hoc analytical work, especially earlier on in the project timeline.","title":"Interactive outputs"},{"location":"implementing_RAP/notebooks_versus_ide_development/#markdown-cells","text":"Notebook cells can contain different types of content: Python, R and SQL but also Markdown which allows you to format text. This means you can add additional non-code content to notebooks, for example adding headings, bullet points, inserting images, and many other things. Cells containing code can be hidden to reveal only their outputs so that you can create a fully interactive, code-free presentation of analysis- great for presenting findings to stakeholders. Notebooks can be exported as HTML files and the full analysis shared online (i.e. embedded in a webpage) or offline (i.e. as a static file). This saves you from having to create a separate report, and manually copying outputs over, and outputs automatically updating if changes are made to the code.","title":"Markdown cells"},{"location":"implementing_RAP/notebooks_versus_ide_development/#quick-to-get-up-and-running","text":"Notebooks are generally set up to work out of the box, enabling the users to connect to the data and get started working right away. IDEs on the other hand either come in two flavours: big, feature-heavy ones take a lot of set up and plugins to get going; lightweight ones like Atom or Visual Studio Code are often little more than text editors out-of-the-box. While IDEs offer almost limitless customisability (you can even write your own themes and extensions for a lot of them!) and powerful, development-enhancing functionality, they often take more time to get set up in the first place. IDEs require you to have an adequate environment for them to be installed, used and customised (i.e. installing useful extensions). This can be prohibitive, for instance at NHS Digital where installing software to local machines is restricted.","title":"Quick to get up and running"},{"location":"implementing_RAP/notebooks_versus_ide_development/#benefits-of-ides","text":"The best IDEs incorporate some of the benefits of notebooks, while also allowing better version control, and easier development of reproducible code.","title":"Benefits of IDEs"},{"location":"implementing_RAP/notebooks_versus_ide_development/#deterministic-code","text":"For data pipelines which are intended to be rerun multiple times, potentially by different systems/users, scripts in an IDE adhere to the RAP principles more readily than notebooks . In a notebook, it is possible for two analysts to generate different results from the same notebooks, based on the order in which cells are executed. In long notebooks it can be hard to spot this happening or where it originated. Writing a script within an IDE makes it difficult for this to occur as the same set of logical steps will be executed in the same order every time the script runs.","title":"Deterministic code"},{"location":"implementing_RAP/notebooks_versus_ide_development/#modularisation","text":"In an IDE it's easy to write reusable pieces of code and separate these out into clearly named files and folders that can be imported into the main pipeline. For larger projects, writing code in a modular structure within an IDE improves code quality, readability and maintainability compared to having extensive notebooks. This is more challenging with notebooks, because importing functionality from notebooks can be complicated:","title":"Modularisation"},{"location":"implementing_RAP/notebooks_versus_ide_development/#importing-code-in-jupyter-notebooks","text":"Jupyter notebooks require quite a bit of arduous coding gymnastics to perform what in Python files would be simple imports ( importing Jupyter notebooks )","title":"Importing code in Jupyter notebooks"},{"location":"implementing_RAP/notebooks_versus_ide_development/#importing-code-in-databricks-notebooks","text":"Databricks provides a magic %run command, which is essentially equivalent of inserting all the cells from the notebook being run into the current notebook ( %run command in Databricks ) When %run is called, the two notebooks share the same scope ; this means that any objects (variables, functions etc.) available for reading and writing in the cells of one notebook are available for reading and writing in the cells of the other. The parent (left) errors because the child (right) causes hidden changes within the scope After the %run there can be references to objects that are hidden in the child notebook. This can make code harder to understand, introduce unintended side effects and lead to bugs .","title":"Importing code in Databricks notebooks"},{"location":"implementing_RAP/notebooks_versus_ide_development/#unit-testing","text":"Please see the unit tests guide for more information about unit testing. It is much easier to implement unit tests with scripts in an IDE than in notebooks. This is partly due to the modular structure : code can be broken down into functions and classes contained within small, manageable files and subdirectories; each test then only interacts with the relevant part of the code which makes debugging easier. Testing frameworks, such as unittest and pytest, do not properly support notebooks. While testing is possible in a notebook workflow, it is hard to isolate code in each cell and thus to trace back errors.","title":"Unit testing"},{"location":"implementing_RAP/notebooks_versus_ide_development/#version-control","text":"Notebooks are inherently difficult to review and audit through version control software like Git . They are stored in a JSON format, which is rendered nicely on screen for the user when viewed through the tool (e.g. Jupyter / Databricks), but which makes it hard to see differences between versions. Git diff from a (simple!) Jupyter notebook: lots to look at! Files in IDEs are generally simpler to version control. It is easy to compare a line in a file between two given versions since everything is designed to be read by humans. This makes it easy to understand what has changed between each version of a file and to rollback to previous versions if required. Git diff from a Python file: much clearer!","title":"Version control"},{"location":"implementing_RAP/notebooks_versus_ide_development/#package-building","text":"Python packages are essentially a collection of files bundled together containing the code and its dependencies. To build a Python package that other developers can install, use of other packages like setuptools or poetry is required. Unfortunately, these tools offer poor support for notebooks, so if one of the deliverables for a project you're working on is the code packaged up in a way that others can reuse and implement in their projects, then an IDE workflow is realistically the only way to achieve this.","title":"Package building"},{"location":"implementing_RAP/notebooks_versus_ide_development/#additional-features","text":"Notebooks are focused on analytical work, but IDEs are designed for software development: they offer features to support the person writing the code, such as auto-completion & formatting as well as syntax highlighting and linting. These features can make writing code in IDEs faster and easier with the result that code is easier to read and of higher quality . While some such features exist in notebooks, such as auto-closure of brackets and quotations, the scope of the functionality is quite limited.","title":"Additional features"},{"location":"implementing_RAP/notebooks_versus_ide_development/#interactive-cells-in-ides","text":"Some IDEs offer ways to use Jupyter-like cells within scripts/modules: special comments in the code (e.g. VSCode and Spyder use # %% ) are used to mark different 'cells'; the IDE or some extensions (e.g. the Python extension in VSCode) will identify these comments and allow code within a cell to be run 'interactively'; results from running interactive cells are stored in an IPython kernel that the IDE is running, but no outputs are saved within the code or the metadata as they with Jupyter notebooks. Using interactive cells in IDEs can offer a nice balance for analysts implementing RAP in their working practices. This comes with all the benefits of an IDE workflow , without risking accidental PII disclosure.","title":"Interactive cells in IDEs"},{"location":"implementing_RAP/notebooks_versus_ide_development/#external-links","text":"Quality Assurance of Code for Analysis and Research Importing Jupyter notebooks %run command in Databricks See also this video by Joel Grus: I don't like notebooks .","title":"External links"},{"location":"implementing_RAP/notebooks_versus_ide_development/#where-do-i-go-from-here","text":"Hopefully, it is clear that there is no 'right' or 'wrong' choice when it comes to workflow: rather it is a question of which workflow is going to better support the needs of the project. We have created a suggested workflow that is not intended to be seen as the recommended or best approach, but offers a good starting point for analysts & data scientists looking to get started setting up a workflow. img[src*=\"#bigimg\"] { width:100%; } img[src*=\"#smallimg\"]{ width:45%; }","title":"Where do I go from here?"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/","text":"Quality Assuring Analytical Outputs RAP Quality Assurance Plan The key resource for tools, guidance and templates in this section is derived from the \" Aqua Book: guidance on producing quality analysis for the government \". It sets out the best practice in quality assurance, drawn from engagement with organisations across the UK public and private sectors. The book lists several assurance activities to provide confidence to the model developers, assurers and users that a given version of the model is fit for purpose (with 'purpose' defined in the model scope documentation). One way to assess the level of confidence in the model is through the below checklist. It helps to identify omissions or areas for improvement so that analysts and decision-makers can judge the risks of using outputs from the model as it stands. You can also create the QA log to audit if the QA plan has been followed \u2013 tracking who did the work, who reviewed, etc. This should be completed during model development by the developer and post-development by those performing QA work. The QA checklist and log should be sent alongside any analysis that goes for clearance to assess the risk associated with any evidence generated by the model. Quality assurance checklist Here is a quality assurance checklist which we adapted from ONS's Quality Assurance of Code for Analysis and Research and updated to apply in Data Science team for some previous projects. You can select the relevant steps for your project depending on its complexity and the required level of quality assurance. (you can check the boxes below by adding an 'x' in between the square braces: - [ ] becomes - [x] in your markdown) Governance and IG Do we have an approved commission? Do we have IG approval to access the data? Do we have agreed scope and requirements? Do we have a clear and comprehensive project specification? Do we know the key stakeholders? Do we have a plausible delivery roadmap? Do we know who will sign off on the project Do we have a QA plan? Do we have a QA log? Project management The roles and responsibilities of team members are clearly defined. An issue tracker (e.g Jira or Trello) is used to record development tasks. New issues or tasks are guided by users\u2019 needs and stories. Issues templates are used to ensure proper logging of the title, description, labels and comments. Acceptance criteria are noted for issues and tasks in JIRA board. Decision log, explaining why we made various choices Quality assurance standards and processes for the project are defined. Data management Do we have agreed data specifications for all inputs? Do data owners agree that the data is fit for purpose? Do we validate all input data? Do we check for extreme and marginal values? Input data are stored safely and are treated as read-only. Input data are versioned. All changes to the data result in new versions being created, or changes are recorded as new records Input data is documented in a data register if possible, including where they come from and their importance to the analysis. Input data has been profiled and checked with the users' expectations Outputs from your analysis are disposable and are regularly deleted and regenerated while analysis develops. Your analysis code can reproduce them at any time. Non-sensitive data are made available to users. If data are sensitive, dummy data is made available so that the code can be run by others. Data quality is monitored, as per the government data quality framework . Fields within input and output datasets are documented in a data dictionary. Large or complex data are stored in a database. Project structure and clarity Do we have a process flow diagram? (data flow, diagram, etc. as appropriate) Do we have a README.md in the package that explains how it works and where to start? (This file details the project purpose, basic installation instructions, and examples of usage). A clear, standard directory structure is used to separate input data, outputs, code and documentation. Where appropriate, guidance for prospective contributors is available including a code of conduct. If the code's users are not familiar with the code, more instructions should be provided to guide lead users through example use cases. Good coding practices Do we have agreed coding standards? Names used in the code are informative and concise. Names used in the code are explicit, rather than implicit. Code logic is clear and avoids unnecessary complexity. Code follows a standard style, e.g. PEP8 for Python and Google or tidyverse for R. Version control Code is version controlled using Git . Code is committed regularly, preferably when a discrete unit of work has been completed. An appropriate branching strategy is defined and used throughout development. Code is open-sourced. Any sensitive data are omitted or replaced with dummy data. Committing standards are followed such as appropriate commit summary and message supplied. Commits are tagged at significant stages. This is used to indicate the state of code for specific releases or model versions. Continuous integration is applied through tools such as GitHub Actions , to ensure that each change is integrated into the workflow smoothly. Modular code Individual pieces of logic are written as functions. Classes are used if more appropriate. Code is grouped in themed files (modules) and is packaged for easier use. Main analysis scripts import and run high level functions from the package. Low level functions and classes carry out one specific task. As such, there is only one reason to change each function. Repetition in the code is minimised. For example, by moving reusable code into functions or classes. Objects and functions are open for extension but closed for modification; functionality can be extended without modifying the source code. Subclasses retain the functionality of their parent class while adding new functionality. Parent class objects can be replaced with instances of the subclass and still work as expected. Code documentation Has a static copy of the code been made available on confluence and shared with stakeholders? (the code version used for QA) Comments are used to describe why code is written in a particular way, rather than describing what the code is doing. Comments are kept up to date, so they do not confuse the reader. Code is not commented out to adjust which lines of code run. All functions and classes are documented to describe what they do, what inputs they take and what they return. Python code is documented using docstrings . Human-readable (preferably HTML) documentation is generated automatically from code documentation. Documentation is hosted for easy access. GitHub Pages and Read the Docs provide a free service for hosting documentation publicly. Configuration Credentials and other secrets are not written in code but are configured as environment variables. Configuration is written as code and is clearly separated from code used for analysis. The configuration used to generate particular outputs, releases and publications is recorded. If appropriate, multiple configuration files are used and interchangeable depending on system/local/user. Peer review Code authors should annotate source code before the review Peer review is conducted and recorded near to the code. Merge or pull requests are used to document review, when relevant. Pair programming is used to review code and share knowledge. Establish a process for fixing defects found during review process Users are encouraged to participate in peer review as a team building activity. Testing Core functionality is unit tested as code. See pytest for Python and testthat for R . Code based tests are run regularly. Bug fixes include implementing new unit tests to ensure that the same bug does not reoccur. Informal tests are recorded near to the code. Stakeholder or user acceptance signoffs are recorded near to the code. Tests are automatically run and recorded using continuous integration or git hooks. The whole process is tested from start to finish using one or more realistic end-to-end tests. Test code is clean and readable. Tests make use of fixtures and parametrisation to reduce repetition. Formal user acceptance testing is conducted and recorded. Integration tests ensure that multiple units of code work together as expected. Dependency management Required passwords, secrets and tokens are documented, but are stored outside of version control. Required libraries and packages are documented, including their versions. Working operating system environments are documented. Example configuration files are provided. Where appropriate, code runs independent of operating system (e.g. suitable management of file paths). Dependencies are managed separately for users, developers, and testers. There are as few dependencies as possible. Package dependencies are managed using an environment manager such as conda env , virtualenv for Python or renv for R . Docker containers or virtual machine builds are available for the code execution environment and these are version controlled. Logging Misuse or failure in the code produces informative error messages. Create error exception handling when founding out critical error Code configuration is recorded when the code is run. Pipeline route is recorded if decisions are made in code. Filter out sensitive data before save and share it Project documentation Does the methodology reflect the latest state of the code? Have we explained the agreed uses and scope of the data - internal, analytical? Have we documented and explained all inputs in the databases? Have we included links to relevant external documentation? Have we written all appropriate caveats? The extent of analytical quality assurance conducted on the project is clearly documented. Assumptions in the analysis and their quality are documented next to the code that implements them. These are also made available to users. Copyright and licenses are specified for both documentation and code. Instructions for how to cite the project are given. Releases of the project used for reports, publications, or other outputs are versioned using a standard pattern such as semantic versioning . A summary of changes to functionality are documented in a changelog following releases. The changelog is available to users. Example usage of packages and underlying functionality is documented for developers and users. Design certificates confirm that the design is compliant with requirements. If appropriate, the software is fully specified. Output Do the users (analytical team) have the skills to run and operate the code? Have we documented all the outputs? Show the details of the table view in database that we make available for the users Explanation of how to use the pipeline and product Signoffs from reviewers and senior managers. External links The Aqua book ONS's Quality Assurance of Code for Analysis and Research NAO's framework to review models","title":"Quality assuring analytical outputs"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#quality-assuring-analytical-outputs","text":"","title":"Quality Assuring Analytical Outputs"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#rap-quality-assurance-plan","text":"The key resource for tools, guidance and templates in this section is derived from the \" Aqua Book: guidance on producing quality analysis for the government \". It sets out the best practice in quality assurance, drawn from engagement with organisations across the UK public and private sectors. The book lists several assurance activities to provide confidence to the model developers, assurers and users that a given version of the model is fit for purpose (with 'purpose' defined in the model scope documentation). One way to assess the level of confidence in the model is through the below checklist. It helps to identify omissions or areas for improvement so that analysts and decision-makers can judge the risks of using outputs from the model as it stands. You can also create the QA log to audit if the QA plan has been followed \u2013 tracking who did the work, who reviewed, etc. This should be completed during model development by the developer and post-development by those performing QA work. The QA checklist and log should be sent alongside any analysis that goes for clearance to assess the risk associated with any evidence generated by the model.","title":"RAP Quality Assurance Plan"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#quality-assurance-checklist","text":"Here is a quality assurance checklist which we adapted from ONS's Quality Assurance of Code for Analysis and Research and updated to apply in Data Science team for some previous projects. You can select the relevant steps for your project depending on its complexity and the required level of quality assurance. (you can check the boxes below by adding an 'x' in between the square braces: - [ ] becomes - [x] in your markdown)","title":"Quality assurance checklist"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#governance-and-ig","text":"Do we have an approved commission? Do we have IG approval to access the data? Do we have agreed scope and requirements? Do we have a clear and comprehensive project specification? Do we know the key stakeholders? Do we have a plausible delivery roadmap? Do we know who will sign off on the project Do we have a QA plan? Do we have a QA log?","title":"Governance and IG"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#project-management","text":"The roles and responsibilities of team members are clearly defined. An issue tracker (e.g Jira or Trello) is used to record development tasks. New issues or tasks are guided by users\u2019 needs and stories. Issues templates are used to ensure proper logging of the title, description, labels and comments. Acceptance criteria are noted for issues and tasks in JIRA board. Decision log, explaining why we made various choices Quality assurance standards and processes for the project are defined.","title":"Project management"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#data-management","text":"Do we have agreed data specifications for all inputs? Do data owners agree that the data is fit for purpose? Do we validate all input data? Do we check for extreme and marginal values? Input data are stored safely and are treated as read-only. Input data are versioned. All changes to the data result in new versions being created, or changes are recorded as new records Input data is documented in a data register if possible, including where they come from and their importance to the analysis. Input data has been profiled and checked with the users' expectations Outputs from your analysis are disposable and are regularly deleted and regenerated while analysis develops. Your analysis code can reproduce them at any time. Non-sensitive data are made available to users. If data are sensitive, dummy data is made available so that the code can be run by others. Data quality is monitored, as per the government data quality framework . Fields within input and output datasets are documented in a data dictionary. Large or complex data are stored in a database.","title":"Data management"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#project-structure-and-clarity","text":"Do we have a process flow diagram? (data flow, diagram, etc. as appropriate) Do we have a README.md in the package that explains how it works and where to start? (This file details the project purpose, basic installation instructions, and examples of usage). A clear, standard directory structure is used to separate input data, outputs, code and documentation. Where appropriate, guidance for prospective contributors is available including a code of conduct. If the code's users are not familiar with the code, more instructions should be provided to guide lead users through example use cases.","title":"Project structure and clarity"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#good-coding-practices","text":"Do we have agreed coding standards? Names used in the code are informative and concise. Names used in the code are explicit, rather than implicit. Code logic is clear and avoids unnecessary complexity. Code follows a standard style, e.g. PEP8 for Python and Google or tidyverse for R.","title":"Good coding practices"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#version-control","text":"Code is version controlled using Git . Code is committed regularly, preferably when a discrete unit of work has been completed. An appropriate branching strategy is defined and used throughout development. Code is open-sourced. Any sensitive data are omitted or replaced with dummy data. Committing standards are followed such as appropriate commit summary and message supplied. Commits are tagged at significant stages. This is used to indicate the state of code for specific releases or model versions. Continuous integration is applied through tools such as GitHub Actions , to ensure that each change is integrated into the workflow smoothly.","title":"Version control"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#modular-code","text":"Individual pieces of logic are written as functions. Classes are used if more appropriate. Code is grouped in themed files (modules) and is packaged for easier use. Main analysis scripts import and run high level functions from the package. Low level functions and classes carry out one specific task. As such, there is only one reason to change each function. Repetition in the code is minimised. For example, by moving reusable code into functions or classes. Objects and functions are open for extension but closed for modification; functionality can be extended without modifying the source code. Subclasses retain the functionality of their parent class while adding new functionality. Parent class objects can be replaced with instances of the subclass and still work as expected.","title":"Modular code"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#code-documentation","text":"Has a static copy of the code been made available on confluence and shared with stakeholders? (the code version used for QA) Comments are used to describe why code is written in a particular way, rather than describing what the code is doing. Comments are kept up to date, so they do not confuse the reader. Code is not commented out to adjust which lines of code run. All functions and classes are documented to describe what they do, what inputs they take and what they return. Python code is documented using docstrings . Human-readable (preferably HTML) documentation is generated automatically from code documentation. Documentation is hosted for easy access. GitHub Pages and Read the Docs provide a free service for hosting documentation publicly.","title":"Code documentation"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#configuration","text":"Credentials and other secrets are not written in code but are configured as environment variables. Configuration is written as code and is clearly separated from code used for analysis. The configuration used to generate particular outputs, releases and publications is recorded. If appropriate, multiple configuration files are used and interchangeable depending on system/local/user.","title":"Configuration"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#peer-review","text":"Code authors should annotate source code before the review Peer review is conducted and recorded near to the code. Merge or pull requests are used to document review, when relevant. Pair programming is used to review code and share knowledge. Establish a process for fixing defects found during review process Users are encouraged to participate in peer review as a team building activity.","title":"Peer review"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#testing","text":"Core functionality is unit tested as code. See pytest for Python and testthat for R . Code based tests are run regularly. Bug fixes include implementing new unit tests to ensure that the same bug does not reoccur. Informal tests are recorded near to the code. Stakeholder or user acceptance signoffs are recorded near to the code. Tests are automatically run and recorded using continuous integration or git hooks. The whole process is tested from start to finish using one or more realistic end-to-end tests. Test code is clean and readable. Tests make use of fixtures and parametrisation to reduce repetition. Formal user acceptance testing is conducted and recorded. Integration tests ensure that multiple units of code work together as expected.","title":"Testing"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#dependency-management","text":"Required passwords, secrets and tokens are documented, but are stored outside of version control. Required libraries and packages are documented, including their versions. Working operating system environments are documented. Example configuration files are provided. Where appropriate, code runs independent of operating system (e.g. suitable management of file paths). Dependencies are managed separately for users, developers, and testers. There are as few dependencies as possible. Package dependencies are managed using an environment manager such as conda env , virtualenv for Python or renv for R . Docker containers or virtual machine builds are available for the code execution environment and these are version controlled.","title":"Dependency management"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#logging","text":"Misuse or failure in the code produces informative error messages. Create error exception handling when founding out critical error Code configuration is recorded when the code is run. Pipeline route is recorded if decisions are made in code. Filter out sensitive data before save and share it","title":"Logging"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#project-documentation","text":"Does the methodology reflect the latest state of the code? Have we explained the agreed uses and scope of the data - internal, analytical? Have we documented and explained all inputs in the databases? Have we included links to relevant external documentation? Have we written all appropriate caveats? The extent of analytical quality assurance conducted on the project is clearly documented. Assumptions in the analysis and their quality are documented next to the code that implements them. These are also made available to users. Copyright and licenses are specified for both documentation and code. Instructions for how to cite the project are given. Releases of the project used for reports, publications, or other outputs are versioned using a standard pattern such as semantic versioning . A summary of changes to functionality are documented in a changelog following releases. The changelog is available to users. Example usage of packages and underlying functionality is documented for developers and users. Design certificates confirm that the design is compliant with requirements. If appropriate, the software is fully specified.","title":"Project documentation"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#output","text":"Do the users (analytical team) have the skills to run and operate the code? Have we documented all the outputs? Show the details of the table view in database that we make available for the users Explanation of how to use the pipeline and product Signoffs from reviewers and senior managers.","title":"Output"},{"location":"implementing_RAP/quality-assuring-analytical-outputs/#external-links","text":"The Aqua book ONS's Quality Assurance of Code for Analysis and Research NAO's framework to review models","title":"External links"},{"location":"implementing_RAP/technical-workflow/","text":"Technical Working Setup & Best Practices Note: This guide (and most of this repository) is written primarily for Python development but some parts are generally applicable to other languages. When you are developing a codebase (perhaps for the first time), especially a RAP codebase, that uses functions stored in different directories , it can be tricky to know how to easily edit code, experiment with different sections of the pipeline, test other people's changes and debug errors. This guide includes sections on: What you will need to get started A typical workflow , including how to develop code Debugging tips If you run into issues with anything in this guide, one of the best places to find answers is by searching for your problem on StackOverflow . What is this guide for? Setting up a technical working environment is a skill that takes time and energy to learn , and so it may be tempting to stick to \"out-of-the-box\" solutions such as the interactive platform on your Python training course, Google Collab, or Jupyter. These are great tools for prototyping, learning and experimenting, but there are many pitfalls to relying solely on \"out-of-the-box\" solutions . At the end of this guide, the reader should feel more comfortable setting up and using their own technical development environment. Your employer/educator may have restricted your ability to change any part of your technical working environment. If this is the case, then hopefully this guide will at least help your understand more about your current setup. What you will need to get started You will need the following, either on your local machine, or on a virtual machine that you will need to connect to . How to set up and connect to a virtual machine is not in the scope of this guide. A working Python installation. We recommend using the Anaconda Python distribution if you are going to use a conda environment . Git This may need to be configured the first time you use it git config --global user.name \"Your Name Here\" git config --global user.email youremail@example.com Code should generally not run on your local machine Your local machine refers to your computer processing power on your laptop itself. Logging into a platform that allows you to access virtual computation, using AWS or similar does not count as a local machine, that is what we call a 'virtual machine'. Using a virtual machine ensures: Confidentiality: many projects at the NHS involve sensitive data and needs to be protected, which means only working with it in a secure computing environment. Computing Power and Scalability: We can use computing resources with more power than a single laptop, and scale them up and down as needed. Collaboration: Working with others in a shared environment can help reproducibility, especially at the start of a project. However, this does not mean that your local machine cannot be used for code development, as long as nothing sensitive is contained in the code. Typical Workflow There is no 'right' or 'wrong' choice when it comes to workflow, and it is important that you feel comfortable with the tools you use to develop your code. It is also important that the workflow supports the needs of the project. However, there are a lot of different setups and when you are new to Python/code, it can be reassuring to follow a suggested workflow, which we have included below. TL;DR : aim to start analytical pieces of work in notebooks. As the codebase / pipeline grows, refactor the code into functions and classes contained within modules alongside a full test suite - this will help ensure that the outputs are reproducible and the code is maintainable, and it will be necessary when it comes to automating pipelines in production Tip If you do use interactive notebooks (i.e. Jupyter notebooks, .ipynb files), make sure *.ipynb is added to your .gitignore file so that all interactive python notebooks are not tracked. Read more about why in this guide . Starting a project Here is a workflow to get you up and running with any new data science project. (Example code either shown or available via the links provided) Log in to virtual network. ( optional, but recommended ) Create a new conda environment for your project and install any packages you know you will need Activate your conda environment Create a new directory (folder) for your project in your preferred location Navigate inside the directory in the terminal , and initialise the Git repository cd your-project-name git init Open the directory in VS Code, or an editor of your choice code . Create a README.md and give your project a title Create a .gitignore . There is a useful VS Code extension for setting up your .gitignore file. Prototype code using interactive cells and/or notebooks Write code in .py files, separating out functions into modules (directories) where appropriate Test functions in interactive console Use linters to make any formatting corrections Export your package requirements & dependencies to an environment.yml file conda env export --from-history --no-builds | grep -v \"prefix\" > environment.yml Git add and commit the files you have created Create a new repo on GitHub or GitLab then follow the instructions to push your code from your project directory to that repo, this will probably look something like: git remote add origin https://www.github.com/yourname/your-repo-name.git git push origin main And now you're ready to go. Working on an existing project When you are working on an existing project, many of the above steps are no longer needed, and you may need to add in a few extras. (Example code either shown or available via the links provided) Log in to virtual network. ( optional, but recommended ) Navigate inside the directory in the terminal cd your-project-name Activate your conda environment Check the current state of the project and make sure you're on the right branch for what you're doing using git status (Make sure your local version of the project is up to date with GitHub using git pull ). Optional as you're likely the only one working on your branch Open the directory in VS Code, or an editor of your choice Prototype code using interactive cells and/or notebooks Write code in .py files, separating out functions into modules (directories) where appropriate Test functions in interactive console Use linters to make any formatting corrections If you've added any new packages, make sure to update your environment.yml file: conda env export --from-history --no-builds | grep -v \"prefix\" > environment.yml Git add and commit the files you have changed Git push the staged changes When you are ready to create a pull/merge request, you will need to check that your working branch is up-to-date with the main branch and deal with any merge conflicts . ( optional, only create a pull request once you are happy with your changes and want them to be reflected in the main branch ) git pull origin main Create a pull/merge request ( optional, see above ) How to write code- general tips Intended as broad advice for people who may not be very confident in Python and are nervous about 'breaking' an existing codebase . You can't really break anything if you've version controlled it properly . You can always revert back to a previous point in the codebase history. Your code doesn't have to be clean while you're developing it . When they are just starting to get to grips with RAP, some people can be nervous about writing code that isn't all contained in functions, or otherwise \"nice and clean\". It is totally ok for your code to start out messy. Ideally you would make commits frequently when your code is a bit nicer, but to test and try out, messy code is completely fine. Get something working, and then refine it . The most important thing is that you get the output that you want. Then you can go back and refine your code. You can figure out where you've repeated code which should indicate that you can make use of, or create new functions. Don't attempt everything all at once . If you have to write a piece of code that produces a calculated output in a particular format for multiple years and regions, start simple! Start with the output, and maybe start with just one year and one region. Then plug in the formatting, and then expand to other years and regions. Debugging It is much easier to debug code to run through it step-by-step on a sample of the data. This is especially true when you are trying to run code made up of different functions coming from different modules. Interactive tools such as interactive cells and notebooks can provide an easy way to do this. Interactive cells have the benefit of being insertable within the code being developed itself, whereas at other times it's helpful to create an entirely new jupyter notebook to put bits of code in and interrogate each output. Another common strategy is to create a blank Python file , such as temp.py , and only include the bits of code that you want to run, perhaps making use of interactive cells without changing the body of code that is version controlled. You can also insert breakpoints , using breakpoint() in your code, to only run code up to a specified point, which helps you to interrogate your variables to figure out why something isn't working. To proceed to the next breakpoint, type continue into the terminal. An alternative to manually inserting breakpoints is built-in debuggers, something that most IDEs offer, including VS Code. However these can debuggers can require a steep learning curve and it can sometimes hinder progress initially to use them. If you do want to use any of these strategies, it's important to make sure that: temp.py is added to your .gitignore file all jupyter notebooks are automatically ignored. You can do this by adding *.ipynb to your .gitignore. This means that all files with the .ipynb extension will not be tracked. any interactive cells you create are not committed into your codebase. This is not as important as the previous two points, and is more best practice than something to avoid at all costs, but it is advisable, especially when working on collaborative projects to not commit anything interactive as you cannot be sure that the cells will operate in the same way on somebody else's machine due to the setup required. Acknowledgements Inspiration has been drawn from: Step-by-Step Guide to Setting Up a Professional Data Science Environment on Windows My Computer Setup for Data Science The Definitive Data Scientist Environment Setup Setup a Data Science Environment on your Computer VS Code documentation","title":"Technical workflow"},{"location":"implementing_RAP/technical-workflow/#technical-working-setup-best-practices","text":"Note: This guide (and most of this repository) is written primarily for Python development but some parts are generally applicable to other languages. When you are developing a codebase (perhaps for the first time), especially a RAP codebase, that uses functions stored in different directories , it can be tricky to know how to easily edit code, experiment with different sections of the pipeline, test other people's changes and debug errors. This guide includes sections on: What you will need to get started A typical workflow , including how to develop code Debugging tips If you run into issues with anything in this guide, one of the best places to find answers is by searching for your problem on StackOverflow .","title":"Technical Working Setup &amp; Best Practices"},{"location":"implementing_RAP/technical-workflow/#what-is-this-guide-for","text":"Setting up a technical working environment is a skill that takes time and energy to learn , and so it may be tempting to stick to \"out-of-the-box\" solutions such as the interactive platform on your Python training course, Google Collab, or Jupyter. These are great tools for prototyping, learning and experimenting, but there are many pitfalls to relying solely on \"out-of-the-box\" solutions . At the end of this guide, the reader should feel more comfortable setting up and using their own technical development environment. Your employer/educator may have restricted your ability to change any part of your technical working environment. If this is the case, then hopefully this guide will at least help your understand more about your current setup.","title":"What is this guide for?"},{"location":"implementing_RAP/technical-workflow/#what-you-will-need-to-get-started","text":"You will need the following, either on your local machine, or on a virtual machine that you will need to connect to . How to set up and connect to a virtual machine is not in the scope of this guide. A working Python installation. We recommend using the Anaconda Python distribution if you are going to use a conda environment . Git This may need to be configured the first time you use it git config --global user.name \"Your Name Here\" git config --global user.email youremail@example.com","title":"What you will need to get started"},{"location":"implementing_RAP/technical-workflow/#code-should-generally-not-run-on-your-local-machine","text":"Your local machine refers to your computer processing power on your laptop itself. Logging into a platform that allows you to access virtual computation, using AWS or similar does not count as a local machine, that is what we call a 'virtual machine'. Using a virtual machine ensures: Confidentiality: many projects at the NHS involve sensitive data and needs to be protected, which means only working with it in a secure computing environment. Computing Power and Scalability: We can use computing resources with more power than a single laptop, and scale them up and down as needed. Collaboration: Working with others in a shared environment can help reproducibility, especially at the start of a project. However, this does not mean that your local machine cannot be used for code development, as long as nothing sensitive is contained in the code.","title":"Code should generally not run on your local machine"},{"location":"implementing_RAP/technical-workflow/#typical-workflow","text":"There is no 'right' or 'wrong' choice when it comes to workflow, and it is important that you feel comfortable with the tools you use to develop your code. It is also important that the workflow supports the needs of the project. However, there are a lot of different setups and when you are new to Python/code, it can be reassuring to follow a suggested workflow, which we have included below. TL;DR : aim to start analytical pieces of work in notebooks. As the codebase / pipeline grows, refactor the code into functions and classes contained within modules alongside a full test suite - this will help ensure that the outputs are reproducible and the code is maintainable, and it will be necessary when it comes to automating pipelines in production Tip If you do use interactive notebooks (i.e. Jupyter notebooks, .ipynb files), make sure *.ipynb is added to your .gitignore file so that all interactive python notebooks are not tracked. Read more about why in this guide .","title":"Typical Workflow"},{"location":"implementing_RAP/technical-workflow/#starting-a-project","text":"Here is a workflow to get you up and running with any new data science project. (Example code either shown or available via the links provided) Log in to virtual network. ( optional, but recommended ) Create a new conda environment for your project and install any packages you know you will need Activate your conda environment Create a new directory (folder) for your project in your preferred location Navigate inside the directory in the terminal , and initialise the Git repository cd your-project-name git init Open the directory in VS Code, or an editor of your choice code . Create a README.md and give your project a title Create a .gitignore . There is a useful VS Code extension for setting up your .gitignore file. Prototype code using interactive cells and/or notebooks Write code in .py files, separating out functions into modules (directories) where appropriate Test functions in interactive console Use linters to make any formatting corrections Export your package requirements & dependencies to an environment.yml file conda env export --from-history --no-builds | grep -v \"prefix\" > environment.yml Git add and commit the files you have created Create a new repo on GitHub or GitLab then follow the instructions to push your code from your project directory to that repo, this will probably look something like: git remote add origin https://www.github.com/yourname/your-repo-name.git git push origin main And now you're ready to go.","title":"Starting a project"},{"location":"implementing_RAP/technical-workflow/#working-on-an-existing-project","text":"When you are working on an existing project, many of the above steps are no longer needed, and you may need to add in a few extras. (Example code either shown or available via the links provided) Log in to virtual network. ( optional, but recommended ) Navigate inside the directory in the terminal cd your-project-name Activate your conda environment Check the current state of the project and make sure you're on the right branch for what you're doing using git status (Make sure your local version of the project is up to date with GitHub using git pull ). Optional as you're likely the only one working on your branch Open the directory in VS Code, or an editor of your choice Prototype code using interactive cells and/or notebooks Write code in .py files, separating out functions into modules (directories) where appropriate Test functions in interactive console Use linters to make any formatting corrections If you've added any new packages, make sure to update your environment.yml file: conda env export --from-history --no-builds | grep -v \"prefix\" > environment.yml Git add and commit the files you have changed Git push the staged changes When you are ready to create a pull/merge request, you will need to check that your working branch is up-to-date with the main branch and deal with any merge conflicts . ( optional, only create a pull request once you are happy with your changes and want them to be reflected in the main branch ) git pull origin main Create a pull/merge request ( optional, see above )","title":"Working on an existing project"},{"location":"implementing_RAP/technical-workflow/#how-to-write-code-general-tips","text":"Intended as broad advice for people who may not be very confident in Python and are nervous about 'breaking' an existing codebase . You can't really break anything if you've version controlled it properly . You can always revert back to a previous point in the codebase history. Your code doesn't have to be clean while you're developing it . When they are just starting to get to grips with RAP, some people can be nervous about writing code that isn't all contained in functions, or otherwise \"nice and clean\". It is totally ok for your code to start out messy. Ideally you would make commits frequently when your code is a bit nicer, but to test and try out, messy code is completely fine. Get something working, and then refine it . The most important thing is that you get the output that you want. Then you can go back and refine your code. You can figure out where you've repeated code which should indicate that you can make use of, or create new functions. Don't attempt everything all at once . If you have to write a piece of code that produces a calculated output in a particular format for multiple years and regions, start simple! Start with the output, and maybe start with just one year and one region. Then plug in the formatting, and then expand to other years and regions.","title":"How to write code- general tips"},{"location":"implementing_RAP/technical-workflow/#debugging","text":"It is much easier to debug code to run through it step-by-step on a sample of the data. This is especially true when you are trying to run code made up of different functions coming from different modules. Interactive tools such as interactive cells and notebooks can provide an easy way to do this. Interactive cells have the benefit of being insertable within the code being developed itself, whereas at other times it's helpful to create an entirely new jupyter notebook to put bits of code in and interrogate each output. Another common strategy is to create a blank Python file , such as temp.py , and only include the bits of code that you want to run, perhaps making use of interactive cells without changing the body of code that is version controlled. You can also insert breakpoints , using breakpoint() in your code, to only run code up to a specified point, which helps you to interrogate your variables to figure out why something isn't working. To proceed to the next breakpoint, type continue into the terminal. An alternative to manually inserting breakpoints is built-in debuggers, something that most IDEs offer, including VS Code. However these can debuggers can require a steep learning curve and it can sometimes hinder progress initially to use them. If you do want to use any of these strategies, it's important to make sure that: temp.py is added to your .gitignore file all jupyter notebooks are automatically ignored. You can do this by adding *.ipynb to your .gitignore. This means that all files with the .ipynb extension will not be tracked. any interactive cells you create are not committed into your codebase. This is not as important as the previous two points, and is more best practice than something to avoid at all costs, but it is advisable, especially when working on collaborative projects to not commit anything interactive as you cannot be sure that the cells will operate in the same way on somebody else's machine due to the setup required.","title":"Debugging"},{"location":"implementing_RAP/technical-workflow/#acknowledgements","text":"Inspiration has been drawn from: Step-by-Step Guide to Setting Up a Professional Data Science Environment on Windows My Computer Setup for Data Science The Definitive Data Scientist Environment Setup Setup a Data Science Environment on your Computer VS Code documentation","title":"Acknowledgements"},{"location":"implementing_RAP/tidy-data/","text":"Tidy Data Adopting inconsistent data formats leads to a huge amount of wasted effort and can actually lead to very complex code. By adopting tidy data format for your work you can both improve your service to users and simplify your own production pipeline. Reference for this guide Hadley Wickham's original paper on tidy format (pdf) Tidy data: Why should I care? \"Tidy datasets are all alike but every messy dataset is messy in its own way\" - Hadley Wickham Teams spend a huge amount of effort reshaping and organising data to meet different needs. Tidy data is an attempt to align on a consistent shape for data that can meet most analytical use cases. By doing as much intermediate data processing as possible in tidy format, you can often achieve huge improvements in code complexity and difficulty of maintaining code. Note that adopting tidy data for stats production does not mean that your outputs need to be formatted this way. In fact, it is straightforward to get from a consistent tidy format to any unique format you require. A common bad practice in stats production is to combine data production and data formatting in the same steps. Teams will calculate statistics and at the same time organise that data to match an excel table. This is a natural thing to do as you write code with your output table in mind but this approach leads to multiple problems: Code often gets repeated for different tables - increasing the burden of updating the code each year and increasing the likelihood of errors. Code is messy and difficult to maintain. Code should do one thing only - either produce the data or format the data. Once the data has been formatted it becomes less useful for other analysts since it is not machine readable. Before another team can use the data, they need to reformat it. Tidy data offers a consistent format for data analysis purposes that is easy for analysts to produce, easy for humans to interpret, and easy for statistical tools like python and R to read. Tidy data offers more flexibility for readability and simplicity for data analysis purposes, as opposed to 'messy' or non-tidy data that can be have its own structure and rules. Tidy data involves \"structuring datasets to facilitate analysis.\" Having a consistent way of storing data, like in the tidy format, allows for analysts to easily use the same tools of analysis in a universal way, for example R or Python. Then, time is spent on producing all kinds of published outputs from the tidy structure, other pieces of analysis and data visualisation outputs such as dashboards. An example of how tidy data enables analysts is that keeping this consistent form of storing data allows analysts to apply longitudinal methods to large datasets that hold years worth of data, critical for healthcare analytics and research. What is tidy data See Hadley Wickham's excellent book for a great explanation It can be confusing in a large dataset which data is an observation (e.g. all measurements for a specific patient across all attributes) and which data is a variable (i.e. the values for a specific type of attribute so the treatment type or the results of a treatment). There are several conventions on how a tidy dataset is structured but the most common one follows Codd's 3rd normal form (Codd, 1990): Each variable forms a column Each observation forms a row Each type of observational unit forms a table Example of tidying up your data A 'messy' or non-tidy dataset displaying results to treatments applied to patients: Patient_ID Treatment A Treatment B 1 N/A 3 2 11 5 3 9 12 The same data but in a tidy format: Patient_ID Treatment Measured_Variable 1 a N/A 2 a 11 3 a 9 1 b 3 2 b 5 3 b 12 Notice how on this table: Each measured variable (i.e. Treatment, Measured Variable) has its own column. Each observation/measurement forms a row. The treatment variable observations form a table of their own, ordering the dataset in a way that connects to the next subtable (i.e. Treatment b). How to tidy your data It's easy to imagine tidy data with a small scale dataset however with real world data things can be trickier. Tidy data follow a similar structure but messy data are unique in their own 'messiness'. In the subsection above, an example of a messy dataset was transformed to a tidy format. Messy data will rarely be that simple and obviously structured in several different ways, depending on what the data is measuring. This subsection will cover the most common issues with messy data and how to apply the tidy format for each example: Note: It's perfectly valid and understandable to have your own structured way of presenting data in your published outputs, like in the table below (Occupation and Salaries). You should only format your data like this as a final output step - after producing your data in tidy format. Tidy data focuses not so much on the data formatting perspective but on the data production side, as tidy data enables the analyst to focus on analytical questions and not on data processing. > How to transform your tidy data into a publication style output example: From tidy to publication output . 1. BAD: Column headers as values, not as names Using values instead of names as a column header is a quite common form of messy data, however it can be useful for specific cases and computations. In the table below, is an example of various occupations and number of people for each occupation and salary: Occupation <\u00a320k \u00a321k-\u00a330k \u00a331k - \u00a340k \u00a341k - \u00a350k \u00a350k + IT 20 30 60 40 20 Teacher 30 40 50 40 35 Doctor 20 40 50 25 40 Software Dev 10 10 25 25 55 Solicitor 25 25 60 30 50 In order to transform the table above into a tidier format, the columns are transformed into rows and into 2 columns/variables, one for income and one for number of people (example showing first rows for IT): Occupation Income Count IT <\u00a320k 20 IT \u00a321k - \u00a330k 30 IT \u00a331k - \u00a340k 60 IT \u00a341k - \u00a350k 40 IT \u00a350k + 20 Teacher ... ... You can see that the data goes from wide to long. 2. BAD: Multiple variables stored in one column Sometimes a dataset can contain columns which are comprised of multiple underlying variables. In the example table below, data shows an imaginary count of patients in hospitals per region, grouped by gender and age, combined in one column. So for example, m1930 stands for male patients aged 19 to 30 years old. Region Year m018 m1930 m3145 m4665 m6680 m81o f018 f1930 West Midlands 2010 2 5 15 20 25 20 4 6 London 2010 5 10 15 20 25 40 5 10 Yorkshire and the Humber 2010 3 9 8 4 20 23 3 9 North West 2010 5 10 15 12 17 15 2 8 To break apart these columns, different approaches of processing can be used, depending on the column header such as breaking apart the string or if there are separators in the column name (e.g. _ or - or ;), capitals or any type of expression. The results of processing is show below, where the combined column of age and gender has broken into 2 separate variables and adding an extra column for the count of patients (first rows of West Midlands region): Region Year Gender Age Cases West Midlands 2000 m 0 - 18 2 West Midlands 2000 m 19 - 30 5 West Midlands 2000 m 31 - 45 15 West Midlands 2000 m 46 - 65 20 West Midlands 2000 m 66 - 80 25 West Midlands 2000 m 81 + 20 West Midlands 2000 f 0 - 18 4 West Midlands 2000 f 19 - 30 6 3. BAD: Variables stored in both rows and columns The next example can be confusing to tidy, as variables are stored in both rows and columns, so Weather station, Year, Month across Day1, Day2 columns and so forth, and tempmax/tempmin (Element column) stored in rows: Weather station Date Month Element Day1 Day2 Day3... LS123 2018 1 tempmax 29 - - LS123 2018 1 tempmin 10 - - LS123 2018 2 tempmax - 28 25 LS123 2018 2 tempmin - 13 13 LS123 2018 3 tempmax 30 - - LS123 2018 3 tempmin 18 - - LS123 2018 4... tempmax - - - To tidy this, a new column Date needs to be extracted from Year, Month and Day to correspond to each day of observation. Then variables minimun and maximun Temperatures for each date will be stored in two different columns: Weather station Date tempmin tempmax LS123 2018-01-01 10 23 LS123 2018-02-02 13 24 LS123 2018-02-03 13 25 LS123 2018-03-01 18 26 LS123 2018-04-24 17 27 LS123 2018-05-12 19 29 This table is in a tidy format, there is one row for one day's observations and two new columns, one for each variable of the column Element. 4. BAD: Multiple types of observations in one table The next example is subset of data from the top Billboard songs for each year. This data is not in a tidy format. Weeks 4 - 75 have been edited out for readability purposes. Here, there are several variables, Artist, Song Title, Duration, Date Entered, Week and Rank. (as both are the same for each week the song spent in the billboard.) Year Artist Song Title Duration Date Entered Week1 Week2 Week3 1966 The Beatles We Can Work It Out 3:00 1966-01-08 5 2 1 1966 The Beatles Paperback Writer 3:30 1966-06-25 6 3 1 1966 The Beach Boys Good Vibrations 3:00 1966-12-10 7 4 2 1966 The Lovin' Spoonful Summer in the City 3:20 1966-08-13 3 2 1 1966 Four Tops Reach Out I'll Be There 3:10 1966-10-15 9 3 2 Transforming this dataset into a tidy version would cause unnecessary duplication of observations (thus cancelling the 1 observation per row norm), in the table below. Imagine if Weeks 4-75 were not removed, so then we would have 72 rows of repeating the same Artist, Song Title and Duration variables. Date Entered has be edited to include the starting week of every week on the billboard: Year Artist Song Title Duration Date Entered Week Rank 1966 The Beach Boys Good Vibrations 3:00 1966-12-10 1 7 1966 The Beach Boys Good Vibrations 3:00 1966-12-17 2 4 1966 The Beach Boys Good Vibrations 3:00 1966-12-24 3 2 1966 The Lovin' Spoonful Summer in the City 3:20 1966-08-13 1 3 1966 The Lovin' Spoonful Summer in the City 3:20 1966-08-20 2 2 1966 The Lovin' Spoonful Summer in the City 3:20 1966-08-27 3 1 In this instance, it is recommended that the tidy dataset is split into two, one for each observational unit, the song title and the rank of the song: Table 1 ID Artist Song Title Duration 1 The Beatles We Can Work It Out 3:00 2 The Beatles Paperback Writer 3:30 3 The Beach Boys Good Vibrations 3:00 4 The Lovin' Spoonful Summer in the City 3:20 5 Four Tops Reach Out I'll Be There 3:10 Table 2 ID Date Entered Rank 3 1966-12-10 7 3 1966-12-17 4 3 1966-12-24 2 4 1966-08-13 3 4 1966-08-20 2 4 1966-08-27 1 This approach is useful for tidy data buts lacks in data processing tools' capabilities, so in that case these tables will have to be merged again. 5. One type of observational unit in multiple tables There are datasets that have multiple tables and variables and observational units that are spread throughout those tables. Or these tables change frequently over time. Or there are different approaches to data types, missing data, data formats for each of those tables. In these cases, it is recommended that each table is 'tidied' up, and then combine each tidy table. To avoid any potential data duplication, adding an extra IDcolumn in each table as well as a column that records the original table/file's name will ensure that each row of data in the merged tidy table is labelled with its source table/file. Diabetes example Diabetes tidy data example Disclaimer: the above video will not load for external users. In this video: SQL vs PySpark outputs comparison (messy vs tidy data) Data shown in suppressed/published form and dummy data","title":"Tidy data"},{"location":"implementing_RAP/tidy-data/#tidy-data","text":"Adopting inconsistent data formats leads to a huge amount of wasted effort and can actually lead to very complex code. By adopting tidy data format for your work you can both improve your service to users and simplify your own production pipeline.","title":"Tidy Data"},{"location":"implementing_RAP/tidy-data/#reference-for-this-guide","text":"Hadley Wickham's original paper on tidy format (pdf)","title":"Reference for this guide"},{"location":"implementing_RAP/tidy-data/#tidy-data-why-should-i-care","text":"\"Tidy datasets are all alike but every messy dataset is messy in its own way\" - Hadley Wickham Teams spend a huge amount of effort reshaping and organising data to meet different needs. Tidy data is an attempt to align on a consistent shape for data that can meet most analytical use cases. By doing as much intermediate data processing as possible in tidy format, you can often achieve huge improvements in code complexity and difficulty of maintaining code. Note that adopting tidy data for stats production does not mean that your outputs need to be formatted this way. In fact, it is straightforward to get from a consistent tidy format to any unique format you require. A common bad practice in stats production is to combine data production and data formatting in the same steps. Teams will calculate statistics and at the same time organise that data to match an excel table. This is a natural thing to do as you write code with your output table in mind but this approach leads to multiple problems: Code often gets repeated for different tables - increasing the burden of updating the code each year and increasing the likelihood of errors. Code is messy and difficult to maintain. Code should do one thing only - either produce the data or format the data. Once the data has been formatted it becomes less useful for other analysts since it is not machine readable. Before another team can use the data, they need to reformat it. Tidy data offers a consistent format for data analysis purposes that is easy for analysts to produce, easy for humans to interpret, and easy for statistical tools like python and R to read. Tidy data offers more flexibility for readability and simplicity for data analysis purposes, as opposed to 'messy' or non-tidy data that can be have its own structure and rules. Tidy data involves \"structuring datasets to facilitate analysis.\" Having a consistent way of storing data, like in the tidy format, allows for analysts to easily use the same tools of analysis in a universal way, for example R or Python. Then, time is spent on producing all kinds of published outputs from the tidy structure, other pieces of analysis and data visualisation outputs such as dashboards. An example of how tidy data enables analysts is that keeping this consistent form of storing data allows analysts to apply longitudinal methods to large datasets that hold years worth of data, critical for healthcare analytics and research.","title":"Tidy data: Why should I care?"},{"location":"implementing_RAP/tidy-data/#what-is-tidy-data","text":"See Hadley Wickham's excellent book for a great explanation It can be confusing in a large dataset which data is an observation (e.g. all measurements for a specific patient across all attributes) and which data is a variable (i.e. the values for a specific type of attribute so the treatment type or the results of a treatment). There are several conventions on how a tidy dataset is structured but the most common one follows Codd's 3rd normal form (Codd, 1990): Each variable forms a column Each observation forms a row Each type of observational unit forms a table Example of tidying up your data A 'messy' or non-tidy dataset displaying results to treatments applied to patients: Patient_ID Treatment A Treatment B 1 N/A 3 2 11 5 3 9 12 The same data but in a tidy format: Patient_ID Treatment Measured_Variable 1 a N/A 2 a 11 3 a 9 1 b 3 2 b 5 3 b 12 Notice how on this table: Each measured variable (i.e. Treatment, Measured Variable) has its own column. Each observation/measurement forms a row. The treatment variable observations form a table of their own, ordering the dataset in a way that connects to the next subtable (i.e. Treatment b).","title":"What is tidy data"},{"location":"implementing_RAP/tidy-data/#how-to-tidy-your-data","text":"It's easy to imagine tidy data with a small scale dataset however with real world data things can be trickier. Tidy data follow a similar structure but messy data are unique in their own 'messiness'. In the subsection above, an example of a messy dataset was transformed to a tidy format. Messy data will rarely be that simple and obviously structured in several different ways, depending on what the data is measuring. This subsection will cover the most common issues with messy data and how to apply the tidy format for each example: Note: It's perfectly valid and understandable to have your own structured way of presenting data in your published outputs, like in the table below (Occupation and Salaries). You should only format your data like this as a final output step - after producing your data in tidy format. Tidy data focuses not so much on the data formatting perspective but on the data production side, as tidy data enables the analyst to focus on analytical questions and not on data processing. > How to transform your tidy data into a publication style output example: From tidy to publication output .","title":"How to tidy your data"},{"location":"implementing_RAP/tidy-data/#1-bad-column-headers-as-values-not-as-names","text":"Using values instead of names as a column header is a quite common form of messy data, however it can be useful for specific cases and computations. In the table below, is an example of various occupations and number of people for each occupation and salary: Occupation <\u00a320k \u00a321k-\u00a330k \u00a331k - \u00a340k \u00a341k - \u00a350k \u00a350k + IT 20 30 60 40 20 Teacher 30 40 50 40 35 Doctor 20 40 50 25 40 Software Dev 10 10 25 25 55 Solicitor 25 25 60 30 50 In order to transform the table above into a tidier format, the columns are transformed into rows and into 2 columns/variables, one for income and one for number of people (example showing first rows for IT): Occupation Income Count IT <\u00a320k 20 IT \u00a321k - \u00a330k 30 IT \u00a331k - \u00a340k 60 IT \u00a341k - \u00a350k 40 IT \u00a350k + 20 Teacher ... ... You can see that the data goes from wide to long.","title":"1. BAD: Column headers as values, not as names"},{"location":"implementing_RAP/tidy-data/#2-bad-multiple-variables-stored-in-one-column","text":"Sometimes a dataset can contain columns which are comprised of multiple underlying variables. In the example table below, data shows an imaginary count of patients in hospitals per region, grouped by gender and age, combined in one column. So for example, m1930 stands for male patients aged 19 to 30 years old. Region Year m018 m1930 m3145 m4665 m6680 m81o f018 f1930 West Midlands 2010 2 5 15 20 25 20 4 6 London 2010 5 10 15 20 25 40 5 10 Yorkshire and the Humber 2010 3 9 8 4 20 23 3 9 North West 2010 5 10 15 12 17 15 2 8 To break apart these columns, different approaches of processing can be used, depending on the column header such as breaking apart the string or if there are separators in the column name (e.g. _ or - or ;), capitals or any type of expression. The results of processing is show below, where the combined column of age and gender has broken into 2 separate variables and adding an extra column for the count of patients (first rows of West Midlands region): Region Year Gender Age Cases West Midlands 2000 m 0 - 18 2 West Midlands 2000 m 19 - 30 5 West Midlands 2000 m 31 - 45 15 West Midlands 2000 m 46 - 65 20 West Midlands 2000 m 66 - 80 25 West Midlands 2000 m 81 + 20 West Midlands 2000 f 0 - 18 4 West Midlands 2000 f 19 - 30 6","title":"2. BAD: Multiple variables stored in one column"},{"location":"implementing_RAP/tidy-data/#3-bad-variables-stored-in-both-rows-and-columns","text":"The next example can be confusing to tidy, as variables are stored in both rows and columns, so Weather station, Year, Month across Day1, Day2 columns and so forth, and tempmax/tempmin (Element column) stored in rows: Weather station Date Month Element Day1 Day2 Day3... LS123 2018 1 tempmax 29 - - LS123 2018 1 tempmin 10 - - LS123 2018 2 tempmax - 28 25 LS123 2018 2 tempmin - 13 13 LS123 2018 3 tempmax 30 - - LS123 2018 3 tempmin 18 - - LS123 2018 4... tempmax - - - To tidy this, a new column Date needs to be extracted from Year, Month and Day to correspond to each day of observation. Then variables minimun and maximun Temperatures for each date will be stored in two different columns: Weather station Date tempmin tempmax LS123 2018-01-01 10 23 LS123 2018-02-02 13 24 LS123 2018-02-03 13 25 LS123 2018-03-01 18 26 LS123 2018-04-24 17 27 LS123 2018-05-12 19 29 This table is in a tidy format, there is one row for one day's observations and two new columns, one for each variable of the column Element.","title":"3. BAD: Variables stored in both rows and columns"},{"location":"implementing_RAP/tidy-data/#4-bad-multiple-types-of-observations-in-one-table","text":"The next example is subset of data from the top Billboard songs for each year. This data is not in a tidy format. Weeks 4 - 75 have been edited out for readability purposes. Here, there are several variables, Artist, Song Title, Duration, Date Entered, Week and Rank. (as both are the same for each week the song spent in the billboard.) Year Artist Song Title Duration Date Entered Week1 Week2 Week3 1966 The Beatles We Can Work It Out 3:00 1966-01-08 5 2 1 1966 The Beatles Paperback Writer 3:30 1966-06-25 6 3 1 1966 The Beach Boys Good Vibrations 3:00 1966-12-10 7 4 2 1966 The Lovin' Spoonful Summer in the City 3:20 1966-08-13 3 2 1 1966 Four Tops Reach Out I'll Be There 3:10 1966-10-15 9 3 2 Transforming this dataset into a tidy version would cause unnecessary duplication of observations (thus cancelling the 1 observation per row norm), in the table below. Imagine if Weeks 4-75 were not removed, so then we would have 72 rows of repeating the same Artist, Song Title and Duration variables. Date Entered has be edited to include the starting week of every week on the billboard: Year Artist Song Title Duration Date Entered Week Rank 1966 The Beach Boys Good Vibrations 3:00 1966-12-10 1 7 1966 The Beach Boys Good Vibrations 3:00 1966-12-17 2 4 1966 The Beach Boys Good Vibrations 3:00 1966-12-24 3 2 1966 The Lovin' Spoonful Summer in the City 3:20 1966-08-13 1 3 1966 The Lovin' Spoonful Summer in the City 3:20 1966-08-20 2 2 1966 The Lovin' Spoonful Summer in the City 3:20 1966-08-27 3 1 In this instance, it is recommended that the tidy dataset is split into two, one for each observational unit, the song title and the rank of the song: Table 1 ID Artist Song Title Duration 1 The Beatles We Can Work It Out 3:00 2 The Beatles Paperback Writer 3:30 3 The Beach Boys Good Vibrations 3:00 4 The Lovin' Spoonful Summer in the City 3:20 5 Four Tops Reach Out I'll Be There 3:10 Table 2 ID Date Entered Rank 3 1966-12-10 7 3 1966-12-17 4 3 1966-12-24 2 4 1966-08-13 3 4 1966-08-20 2 4 1966-08-27 1 This approach is useful for tidy data buts lacks in data processing tools' capabilities, so in that case these tables will have to be merged again.","title":"4. BAD: Multiple types of observations in one table"},{"location":"implementing_RAP/tidy-data/#5-one-type-of-observational-unit-in-multiple-tables","text":"There are datasets that have multiple tables and variables and observational units that are spread throughout those tables. Or these tables change frequently over time. Or there are different approaches to data types, missing data, data formats for each of those tables. In these cases, it is recommended that each table is 'tidied' up, and then combine each tidy table. To avoid any potential data duplication, adding an extra IDcolumn in each table as well as a column that records the original table/file's name will ensure that each row of data in the merged tidy table is labelled with its source table/file.","title":"5. One type of observational unit in multiple tables"},{"location":"implementing_RAP/tidy-data/#diabetes-example","text":"","title":"Diabetes example"},{"location":"implementing_RAP/tidy-data/#diabetes-tidy-data-example","text":"Disclaimer: the above video will not load for external users. In this video: SQL vs PySpark outputs comparison (messy vs tidy data) Data shown in suppressed/published form and dummy data","title":"Diabetes tidy data example"},{"location":"implementing_RAP/tools/","text":"Workflow tools explained Getting data science tools configured to your needs and working together is a core part of any data science project. Learning how to troubleshoot problems with these tools quickly is an important skill. At the end of this guide, the reader should feel more comfortable choosing & using tools to suit their needs. Note: This guide (and most of this repository) is written primarily for Python development but some parts are generally applicable to other languages. Most of the more specific tips and tricks are examples from VS Code (other IDEs are available!). The terminal A terminal is basically just a way of giving your computer commands via text (see the more detailed description in the intro to Git guide . This is important because you can do lots of things from the terminal: run Python scripts, launch interactive notebooks, etc. You may also see this referred to as the 'shell'- there is a difference but the terms are often used interchangeably. Many computers only have a terminal , so you have to know how to use the terminal if you want to do things on them. For example: if you set up an instance on Amazon Web Services (AWS) or Google Cloud, you'll need to interact with that machine via the terminal. There are many different terminals, but all of them do basically the same thing. Windows comes with a couple of default terminals that you can find using Windows search: CMD and PowerShell. Although these shells have some similarities with the shells that data scientists usually use (such as bash and zsh), CMD and PowerShell are different enough to sometimes cause issues. Many of the commands are different, and many of the programs that we might want to use can't be called through them. You're welcome to use these default shells if you want, but remember that the commands you find on StackOverflow may not always work. Different terminals use slightly different languages, and so you may find that if you're used to the terminal on a Mac, you need different commands when working on a Windows machine. Some terminals allow you to workaround this- such as Git Bash . Git Bash works on Windows, but emulates the most common terminal experience on a Mac or Linux, allowing to use the same commands. VS Code also comes with an integrated terminal which you'll be able to use. Git Version control is an incredibly important software engineering tool that data scientists use. Essentially, version control helps you do these three things: Save \"snapshots\" of your code, so that you can track your changes over time and go back to different versions of code if needed. (For example, if you break something). Develop new parts of your code, while keeping the primary version of your code clean (using \"branches\"). Collaborate with others on developing different parts of the same code. The most popular version control software is called Git, and there's a very useful website called GitHub which is used in conjunction with git to store your code online and collaborate with others. You can think of git as being local (on your computer), and GitHub as being in the cloud. Some workers in the NHS will find themselves working not on GitHub , but on GitLab . These two services are very similar, and for our purposes can be used interchangeably. Generally speaking, you'll want to create a new git \"repository\" (or repo, for short) for each project you work on. A git repo is simply where all of your code for a project lives. For more information on Git, and instructions on how to use it, see the intro to Git guide . Python The best programmers use the right tool for the job, and this tool may not necessarily be Python. The choice for many is between Python and R: we do not include many R resources in this repository as most people at NHS Digital use Python but RAPs are perfectly possible in R. If anything, technical setup is slightly more straightforward in R, as RStudio is a fully integrated development environment that can easily be version controlled. Other open-source languages, such as Rust , Go & Julia may also be options to consider. A note on Python versions It's very important to pay attention to the version of Python you are using to develop your code. This is because some versions of Python packages do not work in the same way with older/newer versions. A handy tool for managing multiple versions of Python is pyenv , or pyenv-win for Windows. If you are using Anaconda, you will be able to specify which version of Python you want to use when you create a conda environment . We appreciate that if you are using a centrally managed virtual machine, there will often only be one version of Python available, in which case, not to worry! Conda environment Virtual environments are a way to make sure your code will run correctly for you and others. By always coding inside a virtual environment, you make it more likely that your work will be usable by others. For more information about why it is highly recommended that you use virtual environments for all of your coding, see our virtual environments guide . We recommend using conda (bundled with Anaconda), an open-source package manager, because, in addition to all the standard benefits of package and environment management: It runs on Windows, MacOS and Linux It's fast and easy to use You can specify the Python version you want to use It was created for Python, but you can also use it for R and other languages The README file This will show up on GitHub when we push the code, written in Markdown (hence the .md extension). Markdown is a \"markup language\", which is basically just a way to write plain text that end up getting formatted nicely. It is used on GitHub and GitLab, and can also be used in an interactive python notebook cell. On the first line of your README.md file, you should write the title. You must put a # (hash) key followed by a space before the title of your project. The number of hashes sets the header level, with one hash being the title. Other headings in your README should be at least two hashes. On the second line, put a short, one-line description of the project. Code Editing Data scientists can have very strong feelings about their preferred IDE or code editor. Your choice can make a huge difference in your ability to perform your role effectively . IDEs and code editors are development environments that programmers use to write code, test it, debug it and then push changes to GitHub or another version control platform. The right IDE or code editor should work well with the programming languages you want to use, make it easy to organise your files, and allows you to share them without difficulty. IDEs and text editors offer many properties and options to customize your development environment. Some prefer full-on IDEs, while others like lightweight text editors. There are a lot of good editors out there and it doesn't really matter which one you go with, as long as you like it and know how to use it. Our preferred editor is VS Code, because: It's easy to install and setup The command palette makes it easy to customise your setup You can sync your settings to GitHub so you never lose them! You can use extensions to make it as powerful as you need it to be (this may be restricted if you are using your work computer). It has very useful functionality right out of the box with all of the features you would expect like an integrated terminal , syntax highlighting & code completion , linting and debugging tools . Jupyter is supported, via interactive cells and native notebook support If you don't have a favourite code editor, we would highly recommend giving VS Code a shot. Installation and setup Installing VS Code is simple: go to the VS Code website, click \"Download\", and install using the instructions. Using the terminal is the easiest way to launch VS Code. Open up a terminal, type the following, and hit Enter . code . By starting VS Code in a folder, that folder becomes your \"workspace\". If you want VS Code to open up in a particular location, add the file path to the instruction: code path/to/directory Command Palette Access the 'Command Palette' (Ctrl+Shift+P) from within VS Code to see a list of possible commands and settings. You can search for something particular by typing what you are looking for in search bar. For example, search for \"lowercase\" to show the command that you could use to lowercase a text selection. The command palette is a great place to look for any kind of code editing functionality you might need. Synchronise your settings You can configure settings just for your workspace - as well as set global settings for all projects you work on. A handy feature of VS Code is Settings Sync, which lets you sync VS Code configurations such as settings, keybindings, and installed extensions to your GitHub account so that you can have the same setup across different machines, and if you have to do a fresh install of VS Code your settings are not lost. You can turn on Settings Sync from the Manage gear menu at the bottom of the Activity Bar: Extensions VS Code is set up to work out-of-the-box, but that is just the start! VS Code extensions let you add languages, debuggers, and tools to your installation to support your development workflow. Find the \"extensions\" icon on the left-hand bar and click it. You should see a search bar where you can search for extensions. Search for the Python extension (you can just search \"python\"), and install it. Look at the other icons on the left and other menu items to get a feel for what all is in VS Code. Python syntax highlighting and IntelliSense VS Code is a great editing tool for Python code and includes several features to help you be productive when writing code. Creating a new file with a \".py\" extension, e.g. \"my_script.py\". tells VS Code that this file is a Python file. However, in order for VS Code to run Python files, you must tell VS Code which version of Python to use. Select a Python interpreter From within VS Code, select an interpreter by opening the Command Palette ( Ctrl+Shift+P ), start typing 'Python: Select Interpreter', then select the command. The command presents a list of available interpreters that VS Code can find automatically, including virtual environments . You will see your Python version in the bottom right-hand corner: Note: When using Anaconda, the correct interpreter should have the suffix ('base':conda) , for example Python 3.7.3 64-bit ('base':conda) . If you activate your conda environment before opening your workspace in VS Code from the terminal, this should be automatically selected. Once your Python interpreter has been selected, VS Code knows to highlight the text using Python syntax highlighting rules. IntelliSense IntelliSense is a code completion aid. The user is given a list of options when they begin to type the variable named, greeting. IntelliSense generally opens automatically in most useful situations. You can also trigger it by pressing ctrl + space . Linting in VS Code Linting helps you identify and correct programming errors or unconventional coding practices in your Python code. For example, linting detects use of undefined variables, package imports that are not used, calls to undefined functions, missing brackets etc. You can easily toggle between enabling and disabling your linter, through the Command Palette ( Ctrl+Shift+P ), selecting the Python: Enable/Disable Linting command. Running code in VS Code Once you have selected a Python interpreter, there are several different ways to run Python code. You can run your Python script by clicking the Run Python File in Terminal play button in the top-right side of the editor. The button opens a terminal in which your Python interpreter is automatically activated, then runs your script. Terminal in VS Code VS Code includes an integrated terminal that starts at the root (highest point) of your workspace. To open the terminal: Use the Ctrl+` keyboard shortcut with the backtick character. Use the View > Terminal menu command. From the Command Palette (Ctrl+Shift+P), use the View: Toggle Terminal command You can create a new terminal via the Terminal menu with Terminal > New Terminal. You'll see a terminal open up in the bottom of VS Code. The integrated terminal can use various shells installed on your machine, with the default PowerShell on Windows. You can select other available shells to use in terminal instances or as the default such as Git Bash. In the terminal, you can execute your Python script by running: python my_script.py You can also add a path to your conda executable ( conda.exe in the Anaconda installation folder) in settings of VS Code. This is useful as then you can add new packages to your environment from within VS Code. Note: it is not recommended to add conda to your path on Windows. Interactive cells in VS Code VS Code allows you to easily run bits of your code through its integration with Jupyter. Jupyter is an open-source project that lets you easily combine Markdown text and executable Python source code on one canvas called a notebook. To work with this integration, you must install the Jupyter package in your base Python, and install the ipykernel package in an activated environment . You can then create and run Jupyter-like code cells, defined within Python code using a # %% comment: # %% msg = \"Hello World\" print ( msg ) # %% msg = \"Hello again\" print ( msg ) When the Python extension detects a code cell, it adds Run Cell and Debug Cell CodeLens buttons for you to manually run sections. For more information on interactive cells in VS Code, see the documentation . Note: for importing functions from other scripts to work, you need to set your Jupyter notebook root to be ${workspaceFolder} . Find this via Command Palette ( Ctrl+Shift+P ) and opening Settings. You also need to make sure that the Jupyter 'kernel' that starts when you run a cell is referencing your virtual environment . Interactive Python notebooks Jupyter notebooks are fantastic for exploration, but terrible for version control and can often lead to errors (for a thorough discussion of how Jupyter notebooks do not always fit into a reproducible development workflow see our comparison of IDEs and notebooks . VS Code supports using Jupyter notebooks natively as well as in your Python code via interactive cells. You can create, open and run Jupyter notebooks ( .ipynb files) from inside VS Code. Handily, you can also convert Jupyter notebooks to Python code files very easily from within VS Code. This means that you had develop and try out bits of code in a Jupyter notebook, and then convert it to Python code when you've got it working. Use the convert icon on the toolbar to convert the Notebook (.ipynb) file to a Python file (.py). Once you've converted the file, you can run the code as you would with any other Python file. This offers a more convenient way to find and resolve code bugs, which is difficult to do directly in a Jupyter notebook.","title":"Tools"},{"location":"implementing_RAP/tools/#workflow-tools-explained","text":"Getting data science tools configured to your needs and working together is a core part of any data science project. Learning how to troubleshoot problems with these tools quickly is an important skill. At the end of this guide, the reader should feel more comfortable choosing & using tools to suit their needs. Note: This guide (and most of this repository) is written primarily for Python development but some parts are generally applicable to other languages. Most of the more specific tips and tricks are examples from VS Code (other IDEs are available!).","title":"Workflow tools explained"},{"location":"implementing_RAP/tools/#the-terminal","text":"A terminal is basically just a way of giving your computer commands via text (see the more detailed description in the intro to Git guide . This is important because you can do lots of things from the terminal: run Python scripts, launch interactive notebooks, etc. You may also see this referred to as the 'shell'- there is a difference but the terms are often used interchangeably. Many computers only have a terminal , so you have to know how to use the terminal if you want to do things on them. For example: if you set up an instance on Amazon Web Services (AWS) or Google Cloud, you'll need to interact with that machine via the terminal. There are many different terminals, but all of them do basically the same thing. Windows comes with a couple of default terminals that you can find using Windows search: CMD and PowerShell. Although these shells have some similarities with the shells that data scientists usually use (such as bash and zsh), CMD and PowerShell are different enough to sometimes cause issues. Many of the commands are different, and many of the programs that we might want to use can't be called through them. You're welcome to use these default shells if you want, but remember that the commands you find on StackOverflow may not always work. Different terminals use slightly different languages, and so you may find that if you're used to the terminal on a Mac, you need different commands when working on a Windows machine. Some terminals allow you to workaround this- such as Git Bash . Git Bash works on Windows, but emulates the most common terminal experience on a Mac or Linux, allowing to use the same commands. VS Code also comes with an integrated terminal which you'll be able to use.","title":"The terminal"},{"location":"implementing_RAP/tools/#git","text":"Version control is an incredibly important software engineering tool that data scientists use. Essentially, version control helps you do these three things: Save \"snapshots\" of your code, so that you can track your changes over time and go back to different versions of code if needed. (For example, if you break something). Develop new parts of your code, while keeping the primary version of your code clean (using \"branches\"). Collaborate with others on developing different parts of the same code. The most popular version control software is called Git, and there's a very useful website called GitHub which is used in conjunction with git to store your code online and collaborate with others. You can think of git as being local (on your computer), and GitHub as being in the cloud. Some workers in the NHS will find themselves working not on GitHub , but on GitLab . These two services are very similar, and for our purposes can be used interchangeably. Generally speaking, you'll want to create a new git \"repository\" (or repo, for short) for each project you work on. A git repo is simply where all of your code for a project lives. For more information on Git, and instructions on how to use it, see the intro to Git guide .","title":"Git"},{"location":"implementing_RAP/tools/#python","text":"The best programmers use the right tool for the job, and this tool may not necessarily be Python. The choice for many is between Python and R: we do not include many R resources in this repository as most people at NHS Digital use Python but RAPs are perfectly possible in R. If anything, technical setup is slightly more straightforward in R, as RStudio is a fully integrated development environment that can easily be version controlled. Other open-source languages, such as Rust , Go & Julia may also be options to consider. A note on Python versions It's very important to pay attention to the version of Python you are using to develop your code. This is because some versions of Python packages do not work in the same way with older/newer versions. A handy tool for managing multiple versions of Python is pyenv , or pyenv-win for Windows. If you are using Anaconda, you will be able to specify which version of Python you want to use when you create a conda environment . We appreciate that if you are using a centrally managed virtual machine, there will often only be one version of Python available, in which case, not to worry!","title":"Python"},{"location":"implementing_RAP/tools/#conda-environment","text":"Virtual environments are a way to make sure your code will run correctly for you and others. By always coding inside a virtual environment, you make it more likely that your work will be usable by others. For more information about why it is highly recommended that you use virtual environments for all of your coding, see our virtual environments guide . We recommend using conda (bundled with Anaconda), an open-source package manager, because, in addition to all the standard benefits of package and environment management: It runs on Windows, MacOS and Linux It's fast and easy to use You can specify the Python version you want to use It was created for Python, but you can also use it for R and other languages","title":"Conda environment"},{"location":"implementing_RAP/tools/#the-readme-file","text":"This will show up on GitHub when we push the code, written in Markdown (hence the .md extension). Markdown is a \"markup language\", which is basically just a way to write plain text that end up getting formatted nicely. It is used on GitHub and GitLab, and can also be used in an interactive python notebook cell. On the first line of your README.md file, you should write the title. You must put a # (hash) key followed by a space before the title of your project. The number of hashes sets the header level, with one hash being the title. Other headings in your README should be at least two hashes. On the second line, put a short, one-line description of the project.","title":"The README file"},{"location":"implementing_RAP/tools/#code-editing","text":"Data scientists can have very strong feelings about their preferred IDE or code editor. Your choice can make a huge difference in your ability to perform your role effectively . IDEs and code editors are development environments that programmers use to write code, test it, debug it and then push changes to GitHub or another version control platform. The right IDE or code editor should work well with the programming languages you want to use, make it easy to organise your files, and allows you to share them without difficulty. IDEs and text editors offer many properties and options to customize your development environment. Some prefer full-on IDEs, while others like lightweight text editors. There are a lot of good editors out there and it doesn't really matter which one you go with, as long as you like it and know how to use it. Our preferred editor is VS Code, because: It's easy to install and setup The command palette makes it easy to customise your setup You can sync your settings to GitHub so you never lose them! You can use extensions to make it as powerful as you need it to be (this may be restricted if you are using your work computer). It has very useful functionality right out of the box with all of the features you would expect like an integrated terminal , syntax highlighting & code completion , linting and debugging tools . Jupyter is supported, via interactive cells and native notebook support If you don't have a favourite code editor, we would highly recommend giving VS Code a shot.","title":"Code Editing"},{"location":"implementing_RAP/tools/#installation-and-setup","text":"Installing VS Code is simple: go to the VS Code website, click \"Download\", and install using the instructions. Using the terminal is the easiest way to launch VS Code. Open up a terminal, type the following, and hit Enter . code . By starting VS Code in a folder, that folder becomes your \"workspace\". If you want VS Code to open up in a particular location, add the file path to the instruction: code path/to/directory","title":"Installation and setup"},{"location":"implementing_RAP/tools/#command-palette","text":"Access the 'Command Palette' (Ctrl+Shift+P) from within VS Code to see a list of possible commands and settings. You can search for something particular by typing what you are looking for in search bar. For example, search for \"lowercase\" to show the command that you could use to lowercase a text selection. The command palette is a great place to look for any kind of code editing functionality you might need.","title":"Command Palette"},{"location":"implementing_RAP/tools/#synchronise-your-settings","text":"You can configure settings just for your workspace - as well as set global settings for all projects you work on. A handy feature of VS Code is Settings Sync, which lets you sync VS Code configurations such as settings, keybindings, and installed extensions to your GitHub account so that you can have the same setup across different machines, and if you have to do a fresh install of VS Code your settings are not lost. You can turn on Settings Sync from the Manage gear menu at the bottom of the Activity Bar:","title":"Synchronise your settings"},{"location":"implementing_RAP/tools/#extensions","text":"VS Code is set up to work out-of-the-box, but that is just the start! VS Code extensions let you add languages, debuggers, and tools to your installation to support your development workflow. Find the \"extensions\" icon on the left-hand bar and click it. You should see a search bar where you can search for extensions. Search for the Python extension (you can just search \"python\"), and install it. Look at the other icons on the left and other menu items to get a feel for what all is in VS Code.","title":"Extensions"},{"location":"implementing_RAP/tools/#python-syntax-highlighting-and-intellisense","text":"VS Code is a great editing tool for Python code and includes several features to help you be productive when writing code. Creating a new file with a \".py\" extension, e.g. \"my_script.py\". tells VS Code that this file is a Python file. However, in order for VS Code to run Python files, you must tell VS Code which version of Python to use.","title":"Python syntax highlighting and IntelliSense"},{"location":"implementing_RAP/tools/#select-a-python-interpreter","text":"From within VS Code, select an interpreter by opening the Command Palette ( Ctrl+Shift+P ), start typing 'Python: Select Interpreter', then select the command. The command presents a list of available interpreters that VS Code can find automatically, including virtual environments . You will see your Python version in the bottom right-hand corner: Note: When using Anaconda, the correct interpreter should have the suffix ('base':conda) , for example Python 3.7.3 64-bit ('base':conda) . If you activate your conda environment before opening your workspace in VS Code from the terminal, this should be automatically selected. Once your Python interpreter has been selected, VS Code knows to highlight the text using Python syntax highlighting rules.","title":"Select a Python interpreter"},{"location":"implementing_RAP/tools/#intellisense","text":"IntelliSense is a code completion aid. The user is given a list of options when they begin to type the variable named, greeting. IntelliSense generally opens automatically in most useful situations. You can also trigger it by pressing ctrl + space .","title":"IntelliSense"},{"location":"implementing_RAP/tools/#linting-in-vs-code","text":"Linting helps you identify and correct programming errors or unconventional coding practices in your Python code. For example, linting detects use of undefined variables, package imports that are not used, calls to undefined functions, missing brackets etc. You can easily toggle between enabling and disabling your linter, through the Command Palette ( Ctrl+Shift+P ), selecting the Python: Enable/Disable Linting command.","title":"Linting in VS Code"},{"location":"implementing_RAP/tools/#running-code-in-vs-code","text":"Once you have selected a Python interpreter, there are several different ways to run Python code. You can run your Python script by clicking the Run Python File in Terminal play button in the top-right side of the editor. The button opens a terminal in which your Python interpreter is automatically activated, then runs your script.","title":"Running code in VS Code"},{"location":"implementing_RAP/tools/#terminal-in-vs-code","text":"VS Code includes an integrated terminal that starts at the root (highest point) of your workspace. To open the terminal: Use the Ctrl+` keyboard shortcut with the backtick character. Use the View > Terminal menu command. From the Command Palette (Ctrl+Shift+P), use the View: Toggle Terminal command You can create a new terminal via the Terminal menu with Terminal > New Terminal. You'll see a terminal open up in the bottom of VS Code. The integrated terminal can use various shells installed on your machine, with the default PowerShell on Windows. You can select other available shells to use in terminal instances or as the default such as Git Bash. In the terminal, you can execute your Python script by running: python my_script.py You can also add a path to your conda executable ( conda.exe in the Anaconda installation folder) in settings of VS Code. This is useful as then you can add new packages to your environment from within VS Code. Note: it is not recommended to add conda to your path on Windows.","title":"Terminal in VS Code"},{"location":"implementing_RAP/tools/#interactive-cells-in-vs-code","text":"VS Code allows you to easily run bits of your code through its integration with Jupyter. Jupyter is an open-source project that lets you easily combine Markdown text and executable Python source code on one canvas called a notebook. To work with this integration, you must install the Jupyter package in your base Python, and install the ipykernel package in an activated environment . You can then create and run Jupyter-like code cells, defined within Python code using a # %% comment: # %% msg = \"Hello World\" print ( msg ) # %% msg = \"Hello again\" print ( msg ) When the Python extension detects a code cell, it adds Run Cell and Debug Cell CodeLens buttons for you to manually run sections. For more information on interactive cells in VS Code, see the documentation . Note: for importing functions from other scripts to work, you need to set your Jupyter notebook root to be ${workspaceFolder} . Find this via Command Palette ( Ctrl+Shift+P ) and opening Settings. You also need to make sure that the Jupyter 'kernel' that starts when you run a cell is referencing your virtual environment .","title":"Interactive cells in VS Code"},{"location":"implementing_RAP/tools/#interactive-python-notebooks","text":"Jupyter notebooks are fantastic for exploration, but terrible for version control and can often lead to errors (for a thorough discussion of how Jupyter notebooks do not always fit into a reproducible development workflow see our comparison of IDEs and notebooks . VS Code supports using Jupyter notebooks natively as well as in your Python code via interactive cells. You can create, open and run Jupyter notebooks ( .ipynb files) from inside VS Code. Handily, you can also convert Jupyter notebooks to Python code files very easily from within VS Code. This means that you had develop and try out bits of code in a Jupyter notebook, and then convert it to Python code when you've got it working. Use the convert icon on the toolbar to convert the Notebook (.ipynb) file to a Python file (.py). Once you've converted the file, you can run the code as you would with any other Python file. This offers a more convenient way to find and resolve code bugs, which is difficult to do directly in a Jupyter notebook.","title":"Interactive Python notebooks"},{"location":"introduction_to_RAP/levels_of_RAP/","text":"Levels of RAP These levels of RAP are the outcome of discussions on the cross-govt RAP group, as well as input from the head of the Analytics Function, the head of Data Science, the Chief Statisician, the Statistics Regulator, and NHS Digital team leads. These levels aim to offer teams a recommended maturity map for adopting RAP practices. We have seen that teams who skip the early capabilities struggle to make effective progress. These capabilities are independent and so you don't need to adopt all at once. Team leads should balance the BAU delivery, resourcing constraints, and RAP development as fits their agreed objectives. In order for a publication to be considered a reproducible analytical pipeline, it must at least meet all of the requirements of baseline RAP . Typically teams will go beyond this minimum baseline and we encourage this. The baseline serves to describe the absolute minimum needed to be considered RAP. Baseline RAP - the minimum to be considered RAP Data produced by code in an open-source language (e.g., Python, R, SQL) Code is version controlled. This should be the fundamental basis for all later improvements. See Git basics and Using Git collaboratively Code has been peer reviewed and adheres to agreed coding standards. E.g style guide for Pyspark . Code is published in the open. See process flow for publishing safely Open data published (if there is a publication) The publication should link to the code repo and vice versa (if there is a publication). Silver RAP - a recommended middle ground Are all outputs produced by code without the need for manual work? Code is well-organised following standard directory format Is your code well-documented including user guidance (aka desk notes or runbooks), a README to explain overall code structure and usage, docstrings for functions, commented code, and links to the main publication? Do you use reusable functions where appropriate? See Python functions Does your code include tests (could be unit tests, regression tests, or automated sense checks as appropriate). See unit tests Does your repo include dependency management? (i.e. requirements.txt, PipFile, environment.yml or equivalent) Gold RAP Is your code fully packaged? Do you run CI/CD on your code? Does your repo include environment management? Does your process run based on event-based triggers (e.g., new data in database) or on a schedule? Changes to the package clearly signposted. E.g. a changelog in the package, server for package versions, etc.","title":"Levels of RAP"},{"location":"introduction_to_RAP/levels_of_RAP/#levels-of-rap","text":"These levels of RAP are the outcome of discussions on the cross-govt RAP group, as well as input from the head of the Analytics Function, the head of Data Science, the Chief Statisician, the Statistics Regulator, and NHS Digital team leads. These levels aim to offer teams a recommended maturity map for adopting RAP practices. We have seen that teams who skip the early capabilities struggle to make effective progress. These capabilities are independent and so you don't need to adopt all at once. Team leads should balance the BAU delivery, resourcing constraints, and RAP development as fits their agreed objectives. In order for a publication to be considered a reproducible analytical pipeline, it must at least meet all of the requirements of baseline RAP . Typically teams will go beyond this minimum baseline and we encourage this. The baseline serves to describe the absolute minimum needed to be considered RAP.","title":"Levels of RAP"},{"location":"introduction_to_RAP/levels_of_RAP/#baseline-rap-the-minimum-to-be-considered-rap","text":"Data produced by code in an open-source language (e.g., Python, R, SQL) Code is version controlled. This should be the fundamental basis for all later improvements. See Git basics and Using Git collaboratively Code has been peer reviewed and adheres to agreed coding standards. E.g style guide for Pyspark . Code is published in the open. See process flow for publishing safely Open data published (if there is a publication) The publication should link to the code repo and vice versa (if there is a publication).","title":"Baseline RAP - the minimum to be considered RAP"},{"location":"introduction_to_RAP/levels_of_RAP/#silver-rap-a-recommended-middle-ground","text":"Are all outputs produced by code without the need for manual work? Code is well-organised following standard directory format Is your code well-documented including user guidance (aka desk notes or runbooks), a README to explain overall code structure and usage, docstrings for functions, commented code, and links to the main publication? Do you use reusable functions where appropriate? See Python functions Does your code include tests (could be unit tests, regression tests, or automated sense checks as appropriate). See unit tests Does your repo include dependency management? (i.e. requirements.txt, PipFile, environment.yml or equivalent)","title":"Silver RAP - a recommended middle ground"},{"location":"introduction_to_RAP/levels_of_RAP/#gold-rap","text":"Is your code fully packaged? Do you run CI/CD on your code? Does your repo include environment management? Does your process run based on event-based triggers (e.g., new data in database) or on a schedule? Changes to the package clearly signposted. E.g. a changelog in the package, server for package versions, etc.","title":"Gold RAP"},{"location":"introduction_to_RAP/why_is_RAP_important/","text":"What is RAP and why should I care? Scientific research has increasingly relied on code to conduct complex statisticial analyses in recent years. As code has become ubiquitous, new ways of working have emerged to ensure that scientific findings are rigorous and reproducible. These practices have been applied to government analytic work under the banner of Reproducible Analytical Pipelines (RAP) . RAPs bring together a number of good practices to help ensure all published statistics meet the highest standards of transparency and reproducibility. Over time, all analytical work in NHSD (and across government) will be expected to adopt the RAP practices. For example, RAP practices are a key element needed in order to meet the standards of quality analysis set out in the Aqua Book guidelines, which revolve around analysis being reproducible, auditable, transparent, and quality assured. This adoption will take time and should not be considered as all-or-nothing. There are stages to RAP. Teams can take the journey one step at a time with support from this community. The benefits of RAP in NHSD The NHS Digital mission statement says: Our goal is to maximise the accessibility, quality and utility of health and care data while respecting privacy, transparency and ethics. This goal aligns closely with the three main benefits of reproducible research from a scientific perspective Alston and Rick, 2021 : \"reproducible research allows others to learn from your work. Scientific research has a steep learning curve, and allowing others to access data and code gives them a head start on performing similar analyses.\" \"reproducible research allows others to understand and reproduce a researcher's work.\" \"reproducible research allows others to protect themselves from your mistakes.\" RAP therefore helps us to improve the transparency and ethics by showing our workings and giving users a way to feed back into our processes. improve the accessibility of data by outputting data in a consistent, predictable, and accessible format for users. improve the quality of data by ensuring good coding standards are applied. improve the utility of data by sharing code. This shows users how the data have been produced and allowing them to reuse our code. improve the reliability of data by automating manual steps where possible. Aims of RAP This community of practice aims to support teams in adopting RAP practices through: Developing learning materials including reusable templates Most of the learning materials were initially adapted for the NHS Digital analytical environment, but most are applicable elsewhere. Offering support as teams establish new working practices We offer in-person support to NHS Digital teams in the form of bespoke coaching sessions. We also co-ordinate informal weekly drop-in sessions (internal to NHS Digital) and host monthly workshops, where you can learn from others at different stages of RAP adoption, and get advice. If you would like to attend these please contact us . This work is prompted by the observations that teams can struggle to adopt RAP practices without support. While no one element of RAP is particularly difficult, learning several new skills at the same time as delivering BAU is challenging. Teams can struggle to protect time to embed these practices. (The Statistics Authority report on the barriers to RAP adoption is an excellent discussion of the challenges in rolling out RAP). Luckily, at NHS Digital we have strong senior support for RAP and several teams have already begun to adopt many of the practices included in RAP. Consequently, we already have a large pool of skilled, enthusiastic analysts who are willing to help others. These resources also aim to support the goals laid out in the Goldacre report Bringing NHS data analysis into the 21st century and to align with Tim Berners-Lee's five star data principles. Over time we hope to build up a community of people who can self-support and further develop these ways of working.","title":"Why RAP is important"},{"location":"introduction_to_RAP/why_is_RAP_important/#what-is-rap-and-why-should-i-care","text":"Scientific research has increasingly relied on code to conduct complex statisticial analyses in recent years. As code has become ubiquitous, new ways of working have emerged to ensure that scientific findings are rigorous and reproducible. These practices have been applied to government analytic work under the banner of Reproducible Analytical Pipelines (RAP) . RAPs bring together a number of good practices to help ensure all published statistics meet the highest standards of transparency and reproducibility. Over time, all analytical work in NHSD (and across government) will be expected to adopt the RAP practices. For example, RAP practices are a key element needed in order to meet the standards of quality analysis set out in the Aqua Book guidelines, which revolve around analysis being reproducible, auditable, transparent, and quality assured. This adoption will take time and should not be considered as all-or-nothing. There are stages to RAP. Teams can take the journey one step at a time with support from this community.","title":"What is RAP and why should I care?"},{"location":"introduction_to_RAP/why_is_RAP_important/#the-benefits-of-rap-in-nhsd","text":"The NHS Digital mission statement says: Our goal is to maximise the accessibility, quality and utility of health and care data while respecting privacy, transparency and ethics. This goal aligns closely with the three main benefits of reproducible research from a scientific perspective Alston and Rick, 2021 : \"reproducible research allows others to learn from your work. Scientific research has a steep learning curve, and allowing others to access data and code gives them a head start on performing similar analyses.\" \"reproducible research allows others to understand and reproduce a researcher's work.\" \"reproducible research allows others to protect themselves from your mistakes.\" RAP therefore helps us to improve the transparency and ethics by showing our workings and giving users a way to feed back into our processes. improve the accessibility of data by outputting data in a consistent, predictable, and accessible format for users. improve the quality of data by ensuring good coding standards are applied. improve the utility of data by sharing code. This shows users how the data have been produced and allowing them to reuse our code. improve the reliability of data by automating manual steps where possible.","title":"The benefits of RAP in NHSD"},{"location":"introduction_to_RAP/why_is_RAP_important/#aims-of-rap","text":"This community of practice aims to support teams in adopting RAP practices through: Developing learning materials including reusable templates Most of the learning materials were initially adapted for the NHS Digital analytical environment, but most are applicable elsewhere. Offering support as teams establish new working practices We offer in-person support to NHS Digital teams in the form of bespoke coaching sessions. We also co-ordinate informal weekly drop-in sessions (internal to NHS Digital) and host monthly workshops, where you can learn from others at different stages of RAP adoption, and get advice. If you would like to attend these please contact us . This work is prompted by the observations that teams can struggle to adopt RAP practices without support. While no one element of RAP is particularly difficult, learning several new skills at the same time as delivering BAU is challenging. Teams can struggle to protect time to embed these practices. (The Statistics Authority report on the barriers to RAP adoption is an excellent discussion of the challenges in rolling out RAP). Luckily, at NHS Digital we have strong senior support for RAP and several teams have already begun to adopt many of the practices included in RAP. Consequently, we already have a large pool of skilled, enthusiastic analysts who are willing to help others. These resources also aim to support the goals laid out in the Goldacre report Bringing NHS data analysis into the 21st century and to align with Tim Berners-Lee's five star data principles. Over time we hope to build up a community of people who can self-support and further develop these ways of working.","title":"Aims of RAP"},{"location":"our_RAP_service/","text":"Our RAP Service This section looks at the management, planning, and strategy for embedding RAP practices in a large organisation. We hope that sharing our approach and considerations will help others. The RAP team Following the recommendations in the Overcoming Barriers to RAP report, we have set up a central RAP team to coordinate efforts and set standards across NHS Digital. This team tends to be made up of about 5 data scientists and we flex the resource up and down according to demand. Over time, we hope that the central RAP team can be resourced with a mix of all kinds of roles in NHS Digital. We think that this collaborative approach to resourcing will make it more likely that the whole community owns the problem. There are already lots of people playing an informal 'RAP champion' role by supporting colleagues. This group of people would be the natural candidates to guide the NHS Digital RAP community over time. At NHS Digital, we have created a small RAP team made up of data scientists who are dedicated to supporting colleagues through the RAP adoption process. NHS Digital rollout strategy The single most valuable tool we have had in our work is the report from the ONS about overcoming barriers to RAP adoption . Almost every one of the considerations discussed in this report has been relevant for us in the past year. We would strongly encourage organisations who want to adopt RAP practices to read this report and share with senior leaders. You might also consider running a pre-mortem with the project sponsor and senior analytical leaders to anticipate and avoid some of the problems mentioned in this report.","title":"The RAP team"},{"location":"our_RAP_service/#our-rap-service","text":"This section looks at the management, planning, and strategy for embedding RAP practices in a large organisation. We hope that sharing our approach and considerations will help others.","title":"Our RAP Service"},{"location":"our_RAP_service/#the-rap-team","text":"Following the recommendations in the Overcoming Barriers to RAP report, we have set up a central RAP team to coordinate efforts and set standards across NHS Digital. This team tends to be made up of about 5 data scientists and we flex the resource up and down according to demand. Over time, we hope that the central RAP team can be resourced with a mix of all kinds of roles in NHS Digital. We think that this collaborative approach to resourcing will make it more likely that the whole community owns the problem. There are already lots of people playing an informal 'RAP champion' role by supporting colleagues. This group of people would be the natural candidates to guide the NHS Digital RAP community over time. At NHS Digital, we have created a small RAP team made up of data scientists who are dedicated to supporting colleagues through the RAP adoption process.","title":"The RAP team"},{"location":"our_RAP_service/#nhs-digital-rollout-strategy","text":"The single most valuable tool we have had in our work is the report from the ONS about overcoming barriers to RAP adoption . Almost every one of the considerations discussed in this report has been relevant for us in the past year. We would strongly encourage organisations who want to adopt RAP practices to read this report and share with senior leaders. You might also consider running a pre-mortem with the project sponsor and senior analytical leaders to anticipate and avoid some of the problems mentioned in this report.","title":"NHS Digital rollout strategy"},{"location":"our_RAP_service/building_team_capability/","text":"How to prepare my team for RAP This guide will detail what you need to consider before starting a RAP engagement. Initial considerations Identify a publication/product that needs taking through RAP process Fill out a pre-RAP questionnaire Identify people in the publication team who will learn RAP process (Product Owner in a team of ~4 analysts of varying skill levels) Identify people who can provide support (the RAP team at NHS Digital) through RAP transition Assess the team's existing capability Plan pre-RAP training Consider the appropriate level of RAP to aim for, taking skills into account Estimate timeline, consider pre-RAP training, and distance from BAU Set up a project management tool (e.g. Jira, Trello etc.) for tracking work and confluence space for documenting learning, agreed decisions Teams meet and ice breaker session to get to know each other. Set out the plan for the engagement, review roles and responsibilities, lessons learned from previous projects, etc. RAP pre-engagement questionnaire The analyst team needs to assess their ability to carry out a RAP project end-to-end by completing a RAP pre-engagement questionnaire(Link TBC). The questionnaire will contain a series of questions which will revolve around initial project considerations: Publication, state of the codebase Environment used for publication and RAP process Key pain points Team capability Issues with resources, time constraints, BAU Creating your RAP team Instead of embedding an entire team on a Python training course, it is recommended to split out the team into a smaller RAP taskforce, which can work faster and more efficiently on a RAP project. This can be beneficial when there's a larger team as the starting point causing BAU work to mount up to unmanageable levels if the entire team fully commits to training. The team's selection will be based as before on the varying skill levels as result of the RAP pre-engagement questionnaire. After the training and RAP project is completed, then analysts from the project team can carry over the new found RAP knowledge to other similar taskforces as RAP champions. Assessing capability Determine each team members ability to: Use Git (on either GitLab/GitHub) Python (or any other open source programming language) Code reviews This is recorded based on the basis of how experienced and comfortable they are using the above. This will help determine how many analysts require training and/or the amount of training that is required. Depending on the team's size, it might be challenging to train up 6 analysts at once for example. Skill requirements for Baseline RAP To achieve baseline RAP , we recommend analysts familiarise themselves with: An introduction to Git and version control Python (or R, SQL depending on your project) Using Pandas (or PySpark or any R package, depending on your project) Code quality and style guides Skill requirements for RAP in general As defined from the Government Analysis Function , there isn't a definite way to design a end-to-end RAP pipeline. Once baseline RAP has achieved the following are recommended as components to improving a RAP pipeline: Be able to use Git and GitHub/Gitlab, creating/cloning a repo, commits, pushing to a repo, handling branches, merge requests, merge conflicts Python (or R, SQL) Pandas or PySpark dataframes, merging tables, filtering, aggregation, grouping by Using code reviews Testing code (e.g. unit testing, backtesting etc.) Be able to create a repository with good project structure functions and/or code modularity error handling for functions documentation of functions project packaging code style, best coding practices input data validation logging of data and the analysis continuous integration (CI/CD) dependency management, environment management (yaml files, requirements.txt etc.) Training Training should start a few weeks at least before the RAP project's kick off date, depending on training requirements and modules to be taught. During this period, analysts who are new to Python or Git enrol on the respective course that introduces them to their topic of choice. If the RAP team is completely new to programming, we recommend to start training at least a month before the RAP engagement starts. Depending on the analyst team's capability, there are different training pathways responsive to different needs. For example, not every person on the analyst team will require training on Git as they might already be knowledgeable. Analysts new to programming and version control We recommend before the RAP engagement (example for Python/Pandas) for analysts to enrol on courses on: Git/version control Introduction to Python and programming (data types etc) Functions Pandas Familiarity with software engineering best practices, style guides (e.g. PEP8) Experienced users Object-oriented programming Unit tests Project structure Any from the beginner's training pathway During RAP Pair programming support During a RAP project (e.g. at the start of the thin slice it can be beneficial to set up coding buddy pairs, usually an experienced programmer with an inexperienced one, to form a mentoring style collaboration and in addition to help with troubleshooting code, version control, code reviews and any programming related questions. The level of engagement is up to the programming pairs, daily catch-ups for instance is often observed as a workable standard. We found this style of engagement beneficial to teams new to RAP and programming in general, as support from experienced programmers and experts on RAP can offer a significant impact on the success of RAP projects. How to run training during RAP project Once the RAP project commences, the first training session to carry out is: the Introduction to Git and Using Git Collaboratively . Each training session requires a support drop in session where analysts from the project team join in case they are stuck on any exercises from the guides. A repository set up for the purposes of practising Git is used and provided for practice. and basics of Python/Pandas. Using the project's data, we recommend to use data from your project to create a tutorial script that produces a basic output using our basics of Pandas guide and go over elementary Pandas operations that will beginners with their first coding steps. This could cover joins, group bys, filtering, selecting and renaming columns etc. Other training to consider running depending on your team's requirements. We suggest to use your project to create examples for any training you follow to make training feel natural and applicable to your team: Python functions . The thin slice refactoring stage offers a great opportunity to refactor reusable code into functions, and your team to be able to practice with writing functions. These functions can be used as example of an introduction to Python functions i.e. how to call functions, when to pass arguments, number of arguments, keyword arguments, return, pass, docstrings etc. How to do code reviews . Code reviews can be deceiving as someone who's new to code reviews understand that their work is being judged. It is recommended to hold a team code review session and review a few example scripts, using our guidelines and considerations to demonstrate how useful and invaluable code reviews can be in improving overall code quality while also developing your coding skills at the same time. Project structure session, how to package your code . A python package is a way to bundle your code into a single thing that can be shared and reused. Our goal is to be able to share and reuse code across NHS Digital as well as externally. PEP8 and Linting . Linting is a tool that highlights programming errors, stylistic errors and bugs in your code. Each programming software has its own linting application, it is recommended to enable this feature as it helps avoiding this type of errors. Session on the thin slice concept and design . The thin slice concept can be hard to understand sometimes for inexperienced analysts so a session covering the thin slice approach is recommended. Debugging code using an IDE . Like linting, debugging using a programming software can be a feature of its own, it is recommended to help your team familiarise themselves with debugging to improve troubleshooting efficiency. How to measure progress We recommend using the RAP maturity levels as performance indicators to best capture a team's progress. These levels aim to offer teams a recommended maturity map for adopting RAP practices. We have seen that teams who skip the early capabilities struggle to make effective progress. These capabilities are independent and so you don't need to adopt all at once. Team leads should balance the BAU delivery, resourcing constraints, and RAP development as fits their agreed objectives. For example, if all members of the RAP project team are comfortable using version control (i.e. merge requests, merge conflicts, Git commands workflow), then using Git and GitLab can be ticked off as a KPI.","title":"Building team capability"},{"location":"our_RAP_service/building_team_capability/#how-to-prepare-my-team-for-rap","text":"This guide will detail what you need to consider before starting a RAP engagement.","title":"How to prepare my team for RAP"},{"location":"our_RAP_service/building_team_capability/#initial-considerations","text":"Identify a publication/product that needs taking through RAP process Fill out a pre-RAP questionnaire Identify people in the publication team who will learn RAP process (Product Owner in a team of ~4 analysts of varying skill levels) Identify people who can provide support (the RAP team at NHS Digital) through RAP transition Assess the team's existing capability Plan pre-RAP training Consider the appropriate level of RAP to aim for, taking skills into account Estimate timeline, consider pre-RAP training, and distance from BAU Set up a project management tool (e.g. Jira, Trello etc.) for tracking work and confluence space for documenting learning, agreed decisions Teams meet and ice breaker session to get to know each other. Set out the plan for the engagement, review roles and responsibilities, lessons learned from previous projects, etc.","title":"Initial considerations"},{"location":"our_RAP_service/building_team_capability/#rap-pre-engagement-questionnaire","text":"The analyst team needs to assess their ability to carry out a RAP project end-to-end by completing a RAP pre-engagement questionnaire(Link TBC). The questionnaire will contain a series of questions which will revolve around initial project considerations: Publication, state of the codebase Environment used for publication and RAP process Key pain points Team capability Issues with resources, time constraints, BAU","title":"RAP pre-engagement questionnaire"},{"location":"our_RAP_service/building_team_capability/#creating-your-rap-team","text":"Instead of embedding an entire team on a Python training course, it is recommended to split out the team into a smaller RAP taskforce, which can work faster and more efficiently on a RAP project. This can be beneficial when there's a larger team as the starting point causing BAU work to mount up to unmanageable levels if the entire team fully commits to training. The team's selection will be based as before on the varying skill levels as result of the RAP pre-engagement questionnaire. After the training and RAP project is completed, then analysts from the project team can carry over the new found RAP knowledge to other similar taskforces as RAP champions.","title":"Creating your RAP team"},{"location":"our_RAP_service/building_team_capability/#assessing-capability","text":"Determine each team members ability to: Use Git (on either GitLab/GitHub) Python (or any other open source programming language) Code reviews This is recorded based on the basis of how experienced and comfortable they are using the above. This will help determine how many analysts require training and/or the amount of training that is required. Depending on the team's size, it might be challenging to train up 6 analysts at once for example.","title":"Assessing capability"},{"location":"our_RAP_service/building_team_capability/#skill-requirements-for-baseline-rap","text":"To achieve baseline RAP , we recommend analysts familiarise themselves with: An introduction to Git and version control Python (or R, SQL depending on your project) Using Pandas (or PySpark or any R package, depending on your project) Code quality and style guides","title":"Skill requirements for Baseline RAP"},{"location":"our_RAP_service/building_team_capability/#skill-requirements-for-rap-in-general","text":"As defined from the Government Analysis Function , there isn't a definite way to design a end-to-end RAP pipeline. Once baseline RAP has achieved the following are recommended as components to improving a RAP pipeline: Be able to use Git and GitHub/Gitlab, creating/cloning a repo, commits, pushing to a repo, handling branches, merge requests, merge conflicts Python (or R, SQL) Pandas or PySpark dataframes, merging tables, filtering, aggregation, grouping by Using code reviews Testing code (e.g. unit testing, backtesting etc.) Be able to create a repository with good project structure functions and/or code modularity error handling for functions documentation of functions project packaging code style, best coding practices input data validation logging of data and the analysis continuous integration (CI/CD) dependency management, environment management (yaml files, requirements.txt etc.)","title":"Skill requirements for RAP in general"},{"location":"our_RAP_service/building_team_capability/#training","text":"Training should start a few weeks at least before the RAP project's kick off date, depending on training requirements and modules to be taught. During this period, analysts who are new to Python or Git enrol on the respective course that introduces them to their topic of choice. If the RAP team is completely new to programming, we recommend to start training at least a month before the RAP engagement starts. Depending on the analyst team's capability, there are different training pathways responsive to different needs. For example, not every person on the analyst team will require training on Git as they might already be knowledgeable.","title":"Training"},{"location":"our_RAP_service/building_team_capability/#analysts-new-to-programming-and-version-control","text":"We recommend before the RAP engagement (example for Python/Pandas) for analysts to enrol on courses on: Git/version control Introduction to Python and programming (data types etc) Functions Pandas Familiarity with software engineering best practices, style guides (e.g. PEP8)","title":"Analysts new to programming and version control"},{"location":"our_RAP_service/building_team_capability/#experienced-users","text":"Object-oriented programming Unit tests Project structure Any from the beginner's training pathway","title":"Experienced users"},{"location":"our_RAP_service/building_team_capability/#during-rap","text":"","title":"During RAP"},{"location":"our_RAP_service/building_team_capability/#pair-programming-support","text":"During a RAP project (e.g. at the start of the thin slice it can be beneficial to set up coding buddy pairs, usually an experienced programmer with an inexperienced one, to form a mentoring style collaboration and in addition to help with troubleshooting code, version control, code reviews and any programming related questions. The level of engagement is up to the programming pairs, daily catch-ups for instance is often observed as a workable standard. We found this style of engagement beneficial to teams new to RAP and programming in general, as support from experienced programmers and experts on RAP can offer a significant impact on the success of RAP projects.","title":"Pair programming support"},{"location":"our_RAP_service/building_team_capability/#how-to-run-training-during-rap-project","text":"Once the RAP project commences, the first training session to carry out is: the Introduction to Git and Using Git Collaboratively . Each training session requires a support drop in session where analysts from the project team join in case they are stuck on any exercises from the guides. A repository set up for the purposes of practising Git is used and provided for practice. and basics of Python/Pandas. Using the project's data, we recommend to use data from your project to create a tutorial script that produces a basic output using our basics of Pandas guide and go over elementary Pandas operations that will beginners with their first coding steps. This could cover joins, group bys, filtering, selecting and renaming columns etc. Other training to consider running depending on your team's requirements. We suggest to use your project to create examples for any training you follow to make training feel natural and applicable to your team: Python functions . The thin slice refactoring stage offers a great opportunity to refactor reusable code into functions, and your team to be able to practice with writing functions. These functions can be used as example of an introduction to Python functions i.e. how to call functions, when to pass arguments, number of arguments, keyword arguments, return, pass, docstrings etc. How to do code reviews . Code reviews can be deceiving as someone who's new to code reviews understand that their work is being judged. It is recommended to hold a team code review session and review a few example scripts, using our guidelines and considerations to demonstrate how useful and invaluable code reviews can be in improving overall code quality while also developing your coding skills at the same time. Project structure session, how to package your code . A python package is a way to bundle your code into a single thing that can be shared and reused. Our goal is to be able to share and reuse code across NHS Digital as well as externally. PEP8 and Linting . Linting is a tool that highlights programming errors, stylistic errors and bugs in your code. Each programming software has its own linting application, it is recommended to enable this feature as it helps avoiding this type of errors. Session on the thin slice concept and design . The thin slice concept can be hard to understand sometimes for inexperienced analysts so a session covering the thin slice approach is recommended. Debugging code using an IDE . Like linting, debugging using a programming software can be a feature of its own, it is recommended to help your team familiarise themselves with debugging to improve troubleshooting efficiency.","title":"How to run training during RAP project"},{"location":"our_RAP_service/building_team_capability/#how-to-measure-progress","text":"We recommend using the RAP maturity levels as performance indicators to best capture a team's progress. These levels aim to offer teams a recommended maturity map for adopting RAP practices. We have seen that teams who skip the early capabilities struggle to make effective progress. These capabilities are independent and so you don't need to adopt all at once. Team leads should balance the BAU delivery, resourcing constraints, and RAP development as fits their agreed objectives. For example, if all members of the RAP project team are comfortable using version control (i.e. merge requests, merge conflicts, Git commands workflow), then using Git and GitLab can be ticked off as a KPI.","title":"How to measure progress"},{"location":"our_RAP_service/programme-level-reporting/","text":"Programme level reporting The RAP team at NHS Digital offers a service to support teams to reach their RAP goals. We try to avoid being put in situations where we are chasing teams to complete RAP work - the dynamic works better if teams choose to approach us. Nevertheless, it is important for us to track the engagements that we take on so that we can account for our work. How we do this has taken on several forms as we have figured out what service we are providing and improved our understanding of the needs of teams. The table below shows a representative view of how we track engagements. In our experience, agreeing these fields with team leads ahead of time leads to far more effective engagements since everyone has sight of the trade-offs from the outset. Publication Lead analyst Pipeline risk Current tech Development Goal Support model Target RAP level Achieved RAP level pub 1 Jane Smith Medium SQL training Analyst team leads development Baseline Baseline pub 2 John Smith High SQL quality RAP team leads development Silver Baseline How many projects in flight? We are very careful not to spread ourselves too thin, particularly at this formative stage of the service. We've had great initial success by giving ourselves the time to work with teams to learn. That does provide an upper limit on how fast the rollout can move. One of the artefacts missing from the CoP is some kind of document that makes explicit the relationship between (1) speed of RAP rollout and (2) how many people you put behind it. Practically, we are supporting two projects at a time right now - one full engagement and one light touch support.","title":"Programme level reporting"},{"location":"our_RAP_service/programme-level-reporting/#programme-level-reporting","text":"The RAP team at NHS Digital offers a service to support teams to reach their RAP goals. We try to avoid being put in situations where we are chasing teams to complete RAP work - the dynamic works better if teams choose to approach us. Nevertheless, it is important for us to track the engagements that we take on so that we can account for our work. How we do this has taken on several forms as we have figured out what service we are providing and improved our understanding of the needs of teams. The table below shows a representative view of how we track engagements. In our experience, agreeing these fields with team leads ahead of time leads to far more effective engagements since everyone has sight of the trade-offs from the outset. Publication Lead analyst Pipeline risk Current tech Development Goal Support model Target RAP level Achieved RAP level pub 1 Jane Smith Medium SQL training Analyst team leads development Baseline Baseline pub 2 John Smith High SQL quality RAP team leads development Silver Baseline","title":"Programme level reporting"},{"location":"our_RAP_service/programme-level-reporting/#how-many-projects-in-flight","text":"We are very careful not to spread ourselves too thin, particularly at this formative stage of the service. We've had great initial success by giving ourselves the time to work with teams to learn. That does provide an upper limit on how fast the rollout can move. One of the artefacts missing from the CoP is some kind of document that makes explicit the relationship between (1) speed of RAP rollout and (2) how many people you put behind it. Practically, we are supporting two projects at a time right now - one full engagement and one light touch support.","title":"How many projects in flight?"},{"location":"our_RAP_service/service-design-and-user-research/","text":"Designing a RAP service As the roll-out of RAP has progressed in NHS Digital, we have come to recognise that we are offering a service. This is quite a change in direction for a team that previously was focussed on development. We've had to quickly reframe our approach to rolling out RAP in NHS Digital to reflect this change. The RAP service aims to support teams in adopting RAP practices through setting standards, providing resusable training and code, and through directly supporting teams as they progress along the RAP journey. This page discusses some of the tools and considerations we have used in trying to establish this new service. It has two main sections: service design and user research. Why treat RAP as a service? Many of the technical challenges around RAP are now largely solved. Version control, automated testing, peer review are all practices that are well understood and well documented. Nevertheless, adoption of these RAP practices across the public sector has been very slow. Barriers to RAP adoption are therefore likely to be caused by challenges and constraints faced by analytical teams themselves. It is vital for us to understand and address these barriers in order to achieve the RAP team's goal of supporting adoption of RAP practices. If we want to advance RAP across a large, resource-constrained organisation then we need a systematic way to tackle these barriers. The RAP service allows us to support teams to adopt RAP while also helping to prevent our work from being derailed by shifting scope or unrealistic expectations. Service design The government digital service has really nice guidance on how to design and establish a service. To start formalising the RAP service, we ran a series of guided workshops where we tackled these questions in sequence: What problem do we think our service is trying to solve? Who would be the users of the service? What are the needs of different users? A screenshot from the board where we tried to identify user needs. I've deliberately not made these images high-resolution enough to see the detail Once we reached a consensus on these users and their needs, we sketched out a service map for how the RAP team could meet the needs of these users. This service map runs from the first time users hear about our service all the way through to the point that we disengage from them. A screenshot from the board where we mapped out the service map including user experiences, pain-points, and our processes. I've deliberately not made these images high-resolution enough to see the detail At each stage in the journey, we set out the questions and feelings that the users might experience and we considered what tools or processes we might put in place to improve their experience. At the outset this was only a map - a proposed state of what we wanted to reach as a team. There was too much to implement all at once so we prioritised which elements of the service map were most important to tackle first. Critically, we do not treat the service map as a static artefact. After each engagement we conduct user research to identify which elements of the journey should be prioritised for improvement next. This iterative approach allows us to meet the immediate demand while also making progress towards our intended destination. The section below talks in more depth about the user research. User research The goal of our user research is to understand our users' challenges and needs. This discussion guide is intended to support that goal by enabling conversations engage with your participant and their interaction with the service. We have shared our template for user research below. The discussion guide is only a guide and conversations will often take their own course. More - the discussion guide should be adapted and improved to suit your own purposes. We usually have two people from the RAP team join the call. One person to lead the discussion and another to take notes. We also record the sessions so we can review it later as a team. Once we have gathered this user research, we review it as a team and revisit our service design map above. We think about the whole service journey in light of the findings and we consider what we need to improve for our next iteration. User research guides iterative improvements to the service So far, we have been able to make very substantial improvements after every single engagement. It feels very important that we continue to give ourselves the time and space to effectively concentrate on service improvement between engagements. We've included the discussion guide used for the user research sessions below. Are users of the RAP service the NHS Digital teams producing the publications or the end-users of the publications? This is a really tricky question that we've grappled with extensively. Where we've landed is that we are focussing on users of the RAP service - internal analysts. This decision is partly pragmatic - since we work with internal analysts everyday and can just ask them. Partly it reflects how NHS Digital is organised. We are part of an internal analytics function so other people have responsibility for the external user research. Nevertheless, the lack of user research is a failing of our approach currently. It would be extremely helpful to get more external user research as it motivates so much of the RAP work. The Department for Education led a very strong example of using research from the end users to motivate RAP adoption and drive development of their Explore Education Statistics tool . Download the Discussion guide Discussion guide (40 min) Day: Time: Interviewer: Note-taker: Interviewee: INTRODUCTION (2 minutes) Give some details about the purpose of the conversation and make sure your participant is comfortable with the set up, this will mainly be you talking at this stage. For example: Explain the purpose of the session: we want their input on how to improve the RAP service Let the participants know who is taking notes and who is leading the interview Let the participants know you are recording the session (if applicable) and make sure they consent Warm-up questions (5 minutes) Some straightforward questions to put the participant (and you) at ease and to make the conversation seem more natural; this section will establish the level of engagement your user has had with your service. For example: How long have you worked at [service]? Describe your role within the organisation? How did you first hear about RAP? Do you think this is something that we need to improve? What could we do to make it easier for people to find out? In your own words, what is RAP? How have you engaged with RAP overall and with the RAP team specifically? At the start of the project, did you have a clear understanding of what would happen during the engagement with the RAP team? What would help to give people a better understanding of the process in future? Engagement style (30 minutes) The main body of the conversation, where you will learn about your user\u2019s experience of working with your service, their needs, their pain points and where things work well. The RAP team provided support in the form of structured training sessions, pair coding support, and (help with) designing the codebase for the project (delete as appropriate). We are now going to ask you some questions about each of those engagements: What did you like about the training from the RAP team? What didn\u2019t you like about the training from the RAP team? What did you like about the pair coding support offered by the RAP team? What didn\u2019t you like about the pair coding support offered by the RAP team? [Question on support model, description of the support model, how successful was this approach?] [prompt- what did you like, what didn\u2019t you like?] Was the balance between the training/pair-coding/design (delete as appropriate) correct? Which would you like to have had more or less of? Resources Did you engage with any guides? Would you recommend any of these to others? What could we add to our current resources which would make them more helpful? Was there anything missing which you needed? Final thoughts What would you change about the overall approach? [prompt for any obstacles e.g. time, senior support] If relevant: How confident do you feel about maintaining the project? How confident would you feel about starting a new RAP project? Wrap up (3 minutes) Thank your participant for sharing their experience, knowledge, and time with you, making sure they are comfortable with what they have told you and that they can follow up with you at any time. Ask them if they are able to ask further questions by email, if there is anything you might have missed. If applicable, offer your participant the opportunity to be involved in later stages of the user-design process (e.g. prototyping). Note-taking tips It is important to make a record of the conversation that takes place. You should use a document that has plenty of space to take note of both the questions asked and the responses from your participant. You should try to record the exact words that your participant uses and try not to paraphrase or infer meaning into what they have said. It is important for the richness and value of your research that you take note of direct quotes from your participant.","title":"Service design and user research"},{"location":"our_RAP_service/service-design-and-user-research/#designing-a-rap-service","text":"As the roll-out of RAP has progressed in NHS Digital, we have come to recognise that we are offering a service. This is quite a change in direction for a team that previously was focussed on development. We've had to quickly reframe our approach to rolling out RAP in NHS Digital to reflect this change. The RAP service aims to support teams in adopting RAP practices through setting standards, providing resusable training and code, and through directly supporting teams as they progress along the RAP journey. This page discusses some of the tools and considerations we have used in trying to establish this new service. It has two main sections: service design and user research.","title":"Designing a RAP service"},{"location":"our_RAP_service/service-design-and-user-research/#why-treat-rap-as-a-service","text":"Many of the technical challenges around RAP are now largely solved. Version control, automated testing, peer review are all practices that are well understood and well documented. Nevertheless, adoption of these RAP practices across the public sector has been very slow. Barriers to RAP adoption are therefore likely to be caused by challenges and constraints faced by analytical teams themselves. It is vital for us to understand and address these barriers in order to achieve the RAP team's goal of supporting adoption of RAP practices. If we want to advance RAP across a large, resource-constrained organisation then we need a systematic way to tackle these barriers. The RAP service allows us to support teams to adopt RAP while also helping to prevent our work from being derailed by shifting scope or unrealistic expectations.","title":"Why treat RAP as a service?"},{"location":"our_RAP_service/service-design-and-user-research/#service-design","text":"The government digital service has really nice guidance on how to design and establish a service. To start formalising the RAP service, we ran a series of guided workshops where we tackled these questions in sequence: What problem do we think our service is trying to solve? Who would be the users of the service? What are the needs of different users? A screenshot from the board where we tried to identify user needs. I've deliberately not made these images high-resolution enough to see the detail Once we reached a consensus on these users and their needs, we sketched out a service map for how the RAP team could meet the needs of these users. This service map runs from the first time users hear about our service all the way through to the point that we disengage from them. A screenshot from the board where we mapped out the service map including user experiences, pain-points, and our processes. I've deliberately not made these images high-resolution enough to see the detail At each stage in the journey, we set out the questions and feelings that the users might experience and we considered what tools or processes we might put in place to improve their experience. At the outset this was only a map - a proposed state of what we wanted to reach as a team. There was too much to implement all at once so we prioritised which elements of the service map were most important to tackle first. Critically, we do not treat the service map as a static artefact. After each engagement we conduct user research to identify which elements of the journey should be prioritised for improvement next. This iterative approach allows us to meet the immediate demand while also making progress towards our intended destination. The section below talks in more depth about the user research.","title":"Service design"},{"location":"our_RAP_service/service-design-and-user-research/#user-research","text":"The goal of our user research is to understand our users' challenges and needs. This discussion guide is intended to support that goal by enabling conversations engage with your participant and their interaction with the service. We have shared our template for user research below. The discussion guide is only a guide and conversations will often take their own course. More - the discussion guide should be adapted and improved to suit your own purposes. We usually have two people from the RAP team join the call. One person to lead the discussion and another to take notes. We also record the sessions so we can review it later as a team. Once we have gathered this user research, we review it as a team and revisit our service design map above. We think about the whole service journey in light of the findings and we consider what we need to improve for our next iteration.","title":"User research"},{"location":"our_RAP_service/service-design-and-user-research/#user-research-guides-iterative-improvements-to-the-service","text":"So far, we have been able to make very substantial improvements after every single engagement. It feels very important that we continue to give ourselves the time and space to effectively concentrate on service improvement between engagements. We've included the discussion guide used for the user research sessions below.","title":"User research guides iterative improvements to the service"},{"location":"our_RAP_service/service-design-and-user-research/#are-users-of-the-rap-service-the-nhs-digital-teams-producing-the-publications-or-the-end-users-of-the-publications","text":"This is a really tricky question that we've grappled with extensively. Where we've landed is that we are focussing on users of the RAP service - internal analysts. This decision is partly pragmatic - since we work with internal analysts everyday and can just ask them. Partly it reflects how NHS Digital is organised. We are part of an internal analytics function so other people have responsibility for the external user research. Nevertheless, the lack of user research is a failing of our approach currently. It would be extremely helpful to get more external user research as it motivates so much of the RAP work. The Department for Education led a very strong example of using research from the end users to motivate RAP adoption and drive development of their Explore Education Statistics tool . Download the Discussion guide","title":"Are users of the RAP service the NHS Digital teams producing the publications or the end-users of the publications?"},{"location":"our_RAP_service/service-design-and-user-research/#discussion-guide-40-min","text":"Day: Time: Interviewer: Note-taker: Interviewee:","title":"Discussion guide (40 min)"},{"location":"our_RAP_service/service-design-and-user-research/#introduction","text":"(2 minutes) Give some details about the purpose of the conversation and make sure your participant is comfortable with the set up, this will mainly be you talking at this stage. For example: Explain the purpose of the session: we want their input on how to improve the RAP service Let the participants know who is taking notes and who is leading the interview Let the participants know you are recording the session (if applicable) and make sure they consent","title":"INTRODUCTION"},{"location":"our_RAP_service/service-design-and-user-research/#warm-up-questions","text":"(5 minutes) Some straightforward questions to put the participant (and you) at ease and to make the conversation seem more natural; this section will establish the level of engagement your user has had with your service. For example: How long have you worked at [service]? Describe your role within the organisation? How did you first hear about RAP? Do you think this is something that we need to improve? What could we do to make it easier for people to find out? In your own words, what is RAP? How have you engaged with RAP overall and with the RAP team specifically? At the start of the project, did you have a clear understanding of what would happen during the engagement with the RAP team? What would help to give people a better understanding of the process in future?","title":"Warm-up questions"},{"location":"our_RAP_service/service-design-and-user-research/#engagement-style","text":"(30 minutes) The main body of the conversation, where you will learn about your user\u2019s experience of working with your service, their needs, their pain points and where things work well. The RAP team provided support in the form of structured training sessions, pair coding support, and (help with) designing the codebase for the project (delete as appropriate). We are now going to ask you some questions about each of those engagements: What did you like about the training from the RAP team? What didn\u2019t you like about the training from the RAP team? What did you like about the pair coding support offered by the RAP team? What didn\u2019t you like about the pair coding support offered by the RAP team? [Question on support model, description of the support model, how successful was this approach?] [prompt- what did you like, what didn\u2019t you like?] Was the balance between the training/pair-coding/design (delete as appropriate) correct? Which would you like to have had more or less of?","title":"Engagement style"},{"location":"our_RAP_service/service-design-and-user-research/#resources","text":"Did you engage with any guides? Would you recommend any of these to others? What could we add to our current resources which would make them more helpful? Was there anything missing which you needed?","title":"Resources"},{"location":"our_RAP_service/service-design-and-user-research/#final-thoughts","text":"What would you change about the overall approach? [prompt for any obstacles e.g. time, senior support] If relevant: How confident do you feel about maintaining the project? How confident would you feel about starting a new RAP project?","title":"Final thoughts"},{"location":"our_RAP_service/service-design-and-user-research/#wrap-up","text":"(3 minutes) Thank your participant for sharing their experience, knowledge, and time with you, making sure they are comfortable with what they have told you and that they can follow up with you at any time. Ask them if they are able to ask further questions by email, if there is anything you might have missed. If applicable, offer your participant the opportunity to be involved in later stages of the user-design process (e.g. prototyping). Note-taking tips It is important to make a record of the conversation that takes place. You should use a document that has plenty of space to take note of both the questions asked and the responses from your participant. You should try to record the exact words that your participant uses and try not to paraphrase or infer meaning into what they have said. It is important for the richness and value of your research that you take note of direct quotes from your participant.","title":"Wrap up"},{"location":"our_RAP_service/support-models/","text":"RAP squad support models Every team approaches RAP from a different starting position - different BAU pressures, different team make-up and coding proficiency, different delivery cadence. We have spent a lot of time trying to figure out how best to support teams to engage with RAP. One of our main take-aways is that the support model of the RAP team should be tailored to the context of the team being supported. We often identify three competing demands for RAP projects. Each of these goals is valid but would suggest different support models: Goal Solution Consequence We want to build this pipeline as quickly as possible Accept the most basic code that is functional despite lower code quality Code will be more difficult to run and maintain. Risk that tech debt may snowball until BAU delivery becomes impossible We want to build the best possible pipeline Hire a team of external specialists to do it for us Risk that the team becomes demotivated or does not understand the code We want to train our analysts to write good code Give analysts lots of defended time away from their day job with high-quality in-person training BAU delivery will slow down. May not be possible given resource constraints Different goals suggest different trade-offs between improvements to code, building capacity in the team, and the resourcing constraints on the RAP team We\u2019ve now tried out three different models of support: The RAP team provides technical leadership and training with analysts teams being mentored as part of the project The RAP team provides up-front training but analyst teams lead technical development A RAP champion in the analyst team provides training and technical leadership with support from RAP team You can think of these models along a spectrum with more hands on support for model 1 and much less support in model 3. The models are described in more detail here and in the section below, some considerations that can guide which approach is best are explained. 1. RAP team leads development In this model, we embedded fully with the team. The RAP team use our expertise to lead the planning and development process. We teach the team how to apply RAP practices to code through input sessions and pair coding. More than this - the analyst team adopts the working practices of the RAP team to also learn the workflow that goes along with these practices, e.g., breaking down code development into appropriately sized chunks of work for a short-lived branch. This model has worked very well \u2013 the analyst team get very good training and the RAP team get to apply their skills to improve the quality of the pipeline. The problem with this approach is how resource intensive it is as we might expect the RAP team to be embedded with a team for a couple of months. When we have hundreds of publications, this model doesn't scale well unless the teams then cascade their learning to others. 2. Analyst team leads development An alternative approach that we have tried was to offer tailored training and pair-coding support but to leave the technical design and leadership to the analyst team. A key benefit of this approach is the analyst team was really forced got to grapple with the code and think carefully about how to apply what they had learned from the RAP team. As a result I think they learned more effectively than would have been the case if they were shown. Conversely, in this model the quality of the code hit a ceiling. The analyst team did an excellent job at picking up skills and applying what they had learned to the code. Nevertheless, the experience to organise an entire codebase is not something that can be taught quickly. This outcome is peculiar - the team learned more effectively than they would have if given more guidance but also reached an upper limit sooner. One way for us to reconcile these contrasting situations would be to plan another round of engagement with the RAP team once the first round of learning has been consolidated. The most beneficial activity we undertook in this engagement was pair-coding. Supporting each person individually allowed us to manage the very different skill levels in the team more effectively, making sure each person was learning and making valuable contributions. 3. RAP Champion leads with support The third approach we have used is to identify a highly skilled individual in a team who wanted to lead on development work and then we focus our efforts on supporting that individual. This has been a very effective approach from the perspective of the RAP team as it is much less resource intensive. By focussing our engagement, we find that we can make faster progress towards a strong code outcome. The risk of focussing the engagement on one person is that they then assume primary responsibility for cascading the learning to the rest of the team. If they do not have time or inclination then you might end up leaving the rest of the team behind. The other problem with this approach is that it depends on teams already having that skilled individual. Offering a range of support based on the team's situation In order to choose the right support model for teams, we recommend that the team leads and RAP lead should discuss the considerations below. The discussion can frame the decision about whether to choose a support model that is more or less hands-on. Consideration Detail Decision Prioritise training the team or improving the code There is a tension between two outcomes of RAP - improving pipelines and training teams. The senior leadership team should make an honest, up-front decision about which to prioritise as this should guide the support model. Existing development skill in the team We have worked with teams at different stages of their learning journey. If the team is new to modern development practices then that would lean towards a more involved engagement from the RAP team. Team morale and enthusiasm for change Learning and implementing change is hard. For teams where morale is low, we recommend more engagement from the RAP team. Urgency for rebuilding the pipeline In situations where a pipeline is very burdensome or if there is a 'burning-platform' problem then you may choose to prioritise improving the pipeline, with team training coming later. The RAP team can work much faster to rebuild when not delivering training in parallel. Complexity of the existing pipeline For very complex pipelines, the RAP team will be more likely to make effective progress if they can lead development and tackle the pipeline directly. The stats team will likely learn most effectively in this scenario by following along with the refactoring process. Perceived risk of the existing pipeline If there have been a number of errors with a pipeline then you may want more intensive engagement from the RAP team. Resourcing constraints on RAP team We have seen that there is always more demand for RAP support than can be met. At the same time, spreading the RAP team too thin will lead to poor outcomes and consequently may risk the success of RAP overall in the organisation. You should consider carefully how many engagements the RAP team can support and prioritise publications appropriately. RAP is not a once-and-done process so it makes sense to build capacity out over time.","title":"Support models"},{"location":"our_RAP_service/support-models/#rap-squad-support-models","text":"Every team approaches RAP from a different starting position - different BAU pressures, different team make-up and coding proficiency, different delivery cadence. We have spent a lot of time trying to figure out how best to support teams to engage with RAP. One of our main take-aways is that the support model of the RAP team should be tailored to the context of the team being supported. We often identify three competing demands for RAP projects. Each of these goals is valid but would suggest different support models: Goal Solution Consequence We want to build this pipeline as quickly as possible Accept the most basic code that is functional despite lower code quality Code will be more difficult to run and maintain. Risk that tech debt may snowball until BAU delivery becomes impossible We want to build the best possible pipeline Hire a team of external specialists to do it for us Risk that the team becomes demotivated or does not understand the code We want to train our analysts to write good code Give analysts lots of defended time away from their day job with high-quality in-person training BAU delivery will slow down. May not be possible given resource constraints Different goals suggest different trade-offs between improvements to code, building capacity in the team, and the resourcing constraints on the RAP team We\u2019ve now tried out three different models of support: The RAP team provides technical leadership and training with analysts teams being mentored as part of the project The RAP team provides up-front training but analyst teams lead technical development A RAP champion in the analyst team provides training and technical leadership with support from RAP team You can think of these models along a spectrum with more hands on support for model 1 and much less support in model 3. The models are described in more detail here and in the section below, some considerations that can guide which approach is best are explained.","title":"RAP squad support models"},{"location":"our_RAP_service/support-models/#1-rap-team-leads-development","text":"In this model, we embedded fully with the team. The RAP team use our expertise to lead the planning and development process. We teach the team how to apply RAP practices to code through input sessions and pair coding. More than this - the analyst team adopts the working practices of the RAP team to also learn the workflow that goes along with these practices, e.g., breaking down code development into appropriately sized chunks of work for a short-lived branch. This model has worked very well \u2013 the analyst team get very good training and the RAP team get to apply their skills to improve the quality of the pipeline. The problem with this approach is how resource intensive it is as we might expect the RAP team to be embedded with a team for a couple of months. When we have hundreds of publications, this model doesn't scale well unless the teams then cascade their learning to others.","title":"1. RAP team leads development"},{"location":"our_RAP_service/support-models/#2-analyst-team-leads-development","text":"An alternative approach that we have tried was to offer tailored training and pair-coding support but to leave the technical design and leadership to the analyst team. A key benefit of this approach is the analyst team was really forced got to grapple with the code and think carefully about how to apply what they had learned from the RAP team. As a result I think they learned more effectively than would have been the case if they were shown. Conversely, in this model the quality of the code hit a ceiling. The analyst team did an excellent job at picking up skills and applying what they had learned to the code. Nevertheless, the experience to organise an entire codebase is not something that can be taught quickly. This outcome is peculiar - the team learned more effectively than they would have if given more guidance but also reached an upper limit sooner. One way for us to reconcile these contrasting situations would be to plan another round of engagement with the RAP team once the first round of learning has been consolidated. The most beneficial activity we undertook in this engagement was pair-coding. Supporting each person individually allowed us to manage the very different skill levels in the team more effectively, making sure each person was learning and making valuable contributions.","title":"2. Analyst team leads development"},{"location":"our_RAP_service/support-models/#3-rap-champion-leads-with-support","text":"The third approach we have used is to identify a highly skilled individual in a team who wanted to lead on development work and then we focus our efforts on supporting that individual. This has been a very effective approach from the perspective of the RAP team as it is much less resource intensive. By focussing our engagement, we find that we can make faster progress towards a strong code outcome. The risk of focussing the engagement on one person is that they then assume primary responsibility for cascading the learning to the rest of the team. If they do not have time or inclination then you might end up leaving the rest of the team behind. The other problem with this approach is that it depends on teams already having that skilled individual.","title":"3. RAP Champion leads with support"},{"location":"our_RAP_service/support-models/#offering-a-range-of-support-based-on-the-teams-situation","text":"In order to choose the right support model for teams, we recommend that the team leads and RAP lead should discuss the considerations below. The discussion can frame the decision about whether to choose a support model that is more or less hands-on. Consideration Detail Decision Prioritise training the team or improving the code There is a tension between two outcomes of RAP - improving pipelines and training teams. The senior leadership team should make an honest, up-front decision about which to prioritise as this should guide the support model. Existing development skill in the team We have worked with teams at different stages of their learning journey. If the team is new to modern development practices then that would lean towards a more involved engagement from the RAP team. Team morale and enthusiasm for change Learning and implementing change is hard. For teams where morale is low, we recommend more engagement from the RAP team. Urgency for rebuilding the pipeline In situations where a pipeline is very burdensome or if there is a 'burning-platform' problem then you may choose to prioritise improving the pipeline, with team training coming later. The RAP team can work much faster to rebuild when not delivering training in parallel. Complexity of the existing pipeline For very complex pipelines, the RAP team will be more likely to make effective progress if they can lead development and tackle the pipeline directly. The stats team will likely learn most effectively in this scenario by following along with the refactoring process. Perceived risk of the existing pipeline If there have been a number of errors with a pipeline then you may want more intensive engagement from the RAP team. Resourcing constraints on RAP team We have seen that there is always more demand for RAP support than can be met. At the same time, spreading the RAP team too thin will lead to poor outcomes and consequently may risk the success of RAP overall in the organisation. You should consider carefully how many engagements the RAP team can support and prioritise publications appropriately. RAP is not a once-and-done process so it makes sense to build capacity out over time.","title":"Offering a range of support based on the team's situation"},{"location":"our_RAP_service/thin-slice-strategy/","text":"Thin-slice strategy for building RAP pipelines The RAP team at NHS Digital often adopts a 'thin-slice' strategy when we work with teams to migrate legacy pipelines. This guide explains the rationale behind the approach and gives a rough outline of how it plays out in practice This approach is inspired by DevOps practices in software development where rather than tackling the whole project at once, you build the smallest possible bit of functionality end-to-end. By building something end-to-end you get a lot of benefits: You get to quickly demonstrate progress - good for morale and convincing leaders to keep the project going You get to identify major problems at the beginning of the project - e.g. the database doesn't work or the firewall is blocking access. By uncovering these things early you are more likely to be able to solve them and avoid unpleasant surprises right before the end You get to understand the shape of the problem. When you first start on a project, you don't know what you are doing. If you work through the project in a linear sequence - then you finally understand how all the parts fit together right at the very end. That leads to poor design. By building something end-to-end you can see how all the parts fit together and make radical improvements and iterations at very low cost. This strategy is particularly relevant for the work we are trying to do here. We are trying to achieve a number of competing goals and the thin-slice helps us to mediate between them. For example: Goal Solution Consequence We want to build this pipeline as quickly as possible Accept the most basic code that is functional despite lower code quality Code will be more difficult to run and maintain. Risk that tech debt may snowball until BAU delivery becomes impossible We want to build the best possible pipeline Hire a team of external specialists to do it for us Risk that the team becomes demotivated or does not understand the code We want to train our analysts to write good code Give analysts lots of defended time away from their day job with high-quality in-person training BAU delivery will slow down. May not be possible given resource constraints Depending on which of these you want to prioritise - do it quickly, do it well, or build capacity - you would take an entirely different approach. All three are valid but each entails negative consequences that need to be acknowledged. Thin-slice pipeline In reality, we want some strategy that enables us to mediate between these competing demands to achieve all three outcomes - a high-quality pipeline that is developed quickly and with analysts feeling like they have improved their skills each time. The thin-slice strategy is the best way I've come up with to do this. It fits well with the support model the RAP team offers - where we work alongside a team who understand the data well. The typical workflow looks something like this: Minimal outputs. We identify some minimal outputs that we will try to replicate for the thin-slice. For example, we will reproduce the national numerator and denominator for one measure. Reverse engineer the process. We look at the existing code and work backwards to identify the minimal input data needed to calculate those outputs. Replicate the target outputs. We try to replicate those outputs as quickly as possible. At this phase we don't focus on code quality. Instead, we try to understand the logic of the process. The goal is just to recreate those numbers. Since we are only dealing with a small subset of the overall publication, we avoid getting totally swamped in complexity. Nevertheless - even this simplified thin-slice can be a substantial challenge. This phase forces us to grapple with a lot of the complexities of the process - e.g., funny joins, complex derivations, or logic that is spread across multiple sections of code. This is the most uncertain part of the process - but that is the whole point. We want to tackle the hard stuff up front. Refactor and improve. Once we can accurately calculate the target outputs, we move into a more interesting, iterative mode. We step back and review the end-to-end data flow. We ask ourselves how it can be simplified or reorganised to be easier to maintain. We look for sections of code that could be made into reusable functions. We set up unit tests and regression tests where appropriate. We discuss on style and naming conventions. Eventually the whole team converges on a consensus. This design phase is iterative. We might review and improve the code several times before we decide to move forward. We often draw the analogy with writing the text for a publication. You would not publish the first draft of your publication - instead you go through several rounds of review and rewriting. Why would we treat our code any differently? In short - we give ourselves the time and space to make this thin-slice as good as we can possibly make it. Because this is such a small sliver of the overall project, it does not take long to make these improvements. Usually you only need to change a tens of lines of code rather than thousands. By doing this design phase collaboratively, we gain a few very important benefits: First, it gives both teams a chance to contribute different elements. The publication team bring an expert understanding why the process works a certain way while the RAP team bring experience of similar processes across multiple projects. Next, because the publication team is actively contributing to overall design decisions, they end up owning the code. They understand every decision that went into the final makeup of the code. This is totally different to the situation where a super-advanced coder writes some opaque, complex code and then just throws it over the fence. In that situation you are terrified to change anything in case you break it. Finally, because we all do this together, we emerge from this process with a shared understanding of how we are going to write code. Once our improvements to the thin-slice pipeline start to tail off, we should think about how we expand the thin-slice to include all of the other fields and breakdowns in the publication. Scaling out from the thin-slice. We now move out of the thin-slice phase to start building out the rest of the pipeline/publication. We try to choose the next fields strategically. E.g., we tackle a breakdown (a group-by) next, then a complex derived field, and then a field that requires joining some reference data. Again - the goal is to drive out any tricky bits as early in the process as possible. Once we are sure that the whole team is working effectively, we add each of the remaining fields and breakdowns to the backlog. Analysts pick up these tickets and tackle them in sequence. The tickets are considered complete when the new field gives the same outputs as the historic outputs and when another analyst has completed peer review on the code. This phase tends to go remarkably quickly. This is because (1) we already tackled all the hard bits up front and (2) everyone is working from a high-quality template - the thin-slice code. This is also a really good phase for cementing the learning in the team since now each person is working individually but repeating the same logic multiple times as they add lots of fields. FAQs What is the difference between a 'thin-slice' and an MVP? The thin-slice is the minimal piece of functionality that allows us to build an end-to-end pipeline. This early phase of development is helpful for training a team and improving code quality. The thin-slice is therefore motivated by helping the delivery team to deliver effectively. It does not represent something to be delivered to customers. By contrast, a minimal viable product (MVP) is the most basic thing that a team could deliver to customers. MVP has a really peculiar meaning in the context of a pipeline migration since the 'minimal' thing to be delivered will typically be the full publication. What happens if rebuilding the pipeline uncovers errors in the existing pipeline (and hence published statistics)? It is a near certainty that some minor errors will be discovered as part of any migration. This is particularly the case for building RAP pipelines since you are adding additional tests and safeguards to spot errors. We consider this a natural and important opportunity to improve the publication. We sit down with the team and discuss where the issue may have arisen and how we can prevent it in the future. Likewise, if there is an opportunity to make a substantial improvement to an existing publication we address that in collaboration with the team. We do not hold too tight to the idea of replicating historical outputs but instead aim for a pragmatic approach. What happens if we discover more problems after we have moved out of the thin-slice phase? You will almost certainly discover more problems after the thin-slice phase. Hopefully the high code quality that comes from the thin-slice phase will make it more easy to resolve the problem, e.g., by refactoring the code. How would this strategy work in a situation where you were working on a brand new project, rather than migrating a legacy pipeline? The same logic would apply. By identifying a very small piece of functionality and trying to implement it end-to-end, you will get a better sense of the problem.","title":"Thin slice strategy"},{"location":"our_RAP_service/thin-slice-strategy/#thin-slice-strategy-for-building-rap-pipelines","text":"The RAP team at NHS Digital often adopts a 'thin-slice' strategy when we work with teams to migrate legacy pipelines. This guide explains the rationale behind the approach and gives a rough outline of how it plays out in practice This approach is inspired by DevOps practices in software development where rather than tackling the whole project at once, you build the smallest possible bit of functionality end-to-end. By building something end-to-end you get a lot of benefits: You get to quickly demonstrate progress - good for morale and convincing leaders to keep the project going You get to identify major problems at the beginning of the project - e.g. the database doesn't work or the firewall is blocking access. By uncovering these things early you are more likely to be able to solve them and avoid unpleasant surprises right before the end You get to understand the shape of the problem. When you first start on a project, you don't know what you are doing. If you work through the project in a linear sequence - then you finally understand how all the parts fit together right at the very end. That leads to poor design. By building something end-to-end you can see how all the parts fit together and make radical improvements and iterations at very low cost. This strategy is particularly relevant for the work we are trying to do here. We are trying to achieve a number of competing goals and the thin-slice helps us to mediate between them. For example: Goal Solution Consequence We want to build this pipeline as quickly as possible Accept the most basic code that is functional despite lower code quality Code will be more difficult to run and maintain. Risk that tech debt may snowball until BAU delivery becomes impossible We want to build the best possible pipeline Hire a team of external specialists to do it for us Risk that the team becomes demotivated or does not understand the code We want to train our analysts to write good code Give analysts lots of defended time away from their day job with high-quality in-person training BAU delivery will slow down. May not be possible given resource constraints Depending on which of these you want to prioritise - do it quickly, do it well, or build capacity - you would take an entirely different approach. All three are valid but each entails negative consequences that need to be acknowledged.","title":"Thin-slice strategy for building RAP pipelines"},{"location":"our_RAP_service/thin-slice-strategy/#thin-slice-pipeline","text":"In reality, we want some strategy that enables us to mediate between these competing demands to achieve all three outcomes - a high-quality pipeline that is developed quickly and with analysts feeling like they have improved their skills each time. The thin-slice strategy is the best way I've come up with to do this. It fits well with the support model the RAP team offers - where we work alongside a team who understand the data well. The typical workflow looks something like this: Minimal outputs. We identify some minimal outputs that we will try to replicate for the thin-slice. For example, we will reproduce the national numerator and denominator for one measure. Reverse engineer the process. We look at the existing code and work backwards to identify the minimal input data needed to calculate those outputs. Replicate the target outputs. We try to replicate those outputs as quickly as possible. At this phase we don't focus on code quality. Instead, we try to understand the logic of the process. The goal is just to recreate those numbers. Since we are only dealing with a small subset of the overall publication, we avoid getting totally swamped in complexity. Nevertheless - even this simplified thin-slice can be a substantial challenge. This phase forces us to grapple with a lot of the complexities of the process - e.g., funny joins, complex derivations, or logic that is spread across multiple sections of code. This is the most uncertain part of the process - but that is the whole point. We want to tackle the hard stuff up front. Refactor and improve. Once we can accurately calculate the target outputs, we move into a more interesting, iterative mode. We step back and review the end-to-end data flow. We ask ourselves how it can be simplified or reorganised to be easier to maintain. We look for sections of code that could be made into reusable functions. We set up unit tests and regression tests where appropriate. We discuss on style and naming conventions. Eventually the whole team converges on a consensus. This design phase is iterative. We might review and improve the code several times before we decide to move forward. We often draw the analogy with writing the text for a publication. You would not publish the first draft of your publication - instead you go through several rounds of review and rewriting. Why would we treat our code any differently? In short - we give ourselves the time and space to make this thin-slice as good as we can possibly make it. Because this is such a small sliver of the overall project, it does not take long to make these improvements. Usually you only need to change a tens of lines of code rather than thousands. By doing this design phase collaboratively, we gain a few very important benefits: First, it gives both teams a chance to contribute different elements. The publication team bring an expert understanding why the process works a certain way while the RAP team bring experience of similar processes across multiple projects. Next, because the publication team is actively contributing to overall design decisions, they end up owning the code. They understand every decision that went into the final makeup of the code. This is totally different to the situation where a super-advanced coder writes some opaque, complex code and then just throws it over the fence. In that situation you are terrified to change anything in case you break it. Finally, because we all do this together, we emerge from this process with a shared understanding of how we are going to write code. Once our improvements to the thin-slice pipeline start to tail off, we should think about how we expand the thin-slice to include all of the other fields and breakdowns in the publication. Scaling out from the thin-slice. We now move out of the thin-slice phase to start building out the rest of the pipeline/publication. We try to choose the next fields strategically. E.g., we tackle a breakdown (a group-by) next, then a complex derived field, and then a field that requires joining some reference data. Again - the goal is to drive out any tricky bits as early in the process as possible. Once we are sure that the whole team is working effectively, we add each of the remaining fields and breakdowns to the backlog. Analysts pick up these tickets and tackle them in sequence. The tickets are considered complete when the new field gives the same outputs as the historic outputs and when another analyst has completed peer review on the code. This phase tends to go remarkably quickly. This is because (1) we already tackled all the hard bits up front and (2) everyone is working from a high-quality template - the thin-slice code. This is also a really good phase for cementing the learning in the team since now each person is working individually but repeating the same logic multiple times as they add lots of fields.","title":"Thin-slice pipeline"},{"location":"our_RAP_service/thin-slice-strategy/#faqs","text":"What is the difference between a 'thin-slice' and an MVP? The thin-slice is the minimal piece of functionality that allows us to build an end-to-end pipeline. This early phase of development is helpful for training a team and improving code quality. The thin-slice is therefore motivated by helping the delivery team to deliver effectively. It does not represent something to be delivered to customers. By contrast, a minimal viable product (MVP) is the most basic thing that a team could deliver to customers. MVP has a really peculiar meaning in the context of a pipeline migration since the 'minimal' thing to be delivered will typically be the full publication. What happens if rebuilding the pipeline uncovers errors in the existing pipeline (and hence published statistics)? It is a near certainty that some minor errors will be discovered as part of any migration. This is particularly the case for building RAP pipelines since you are adding additional tests and safeguards to spot errors. We consider this a natural and important opportunity to improve the publication. We sit down with the team and discuss where the issue may have arisen and how we can prevent it in the future. Likewise, if there is an opportunity to make a substantial improvement to an existing publication we address that in collaboration with the team. We do not hold too tight to the idea of replicating historical outputs but instead aim for a pragmatic approach. What happens if we discover more problems after we have moved out of the thin-slice phase? You will almost certainly discover more problems after the thin-slice phase. Hopefully the high code quality that comes from the thin-slice phase will make it more easy to resolve the problem, e.g., by refactoring the code. How would this strategy work in a situation where you were working on a brand new project, rather than migrating a legacy pipeline? The same logic would apply. By identifying a very small piece of functionality and trying to implement it end-to-end, you will get a better sense of the problem.","title":"FAQs"},{"location":"our_RAP_service/typical-engagement-flow/","text":"Typical Engagement Flow This example describes a typical lifecycle of applying RAP principles to an existing product. It's important to remember that every team is different and so there is no really typical engagement, but hopefully the below gives you an idea of what's involved. The approach described is the support model where the RAP team leads the development as this is our most frequent approach. Please note that a direct engagement with the RAP team at NHS Digital is currently only available to NHS Digital teams. The below is intended to help other organisations that may be developing their own RAP capabilities and are looking for guidance. Initial outreach Hear about RAP through show-and-tells, internal advertising, slack, etc. Have a look over the RAP Community of Practice pages to get a sense of what it is all about. Reach out to the RAP team for a chat Together with someone from the RAP team, walk through the levels of RAP page and consider the team's scale of ambition. If you are keen to proceed, pass the request to the RAP product owner for prioritisation and resourcing Get a start date in the calendar. Think about publication timing, team capacity, etc. to choose a good time. Setting the foundation Identify a publication/product that needs taking through RAP process Identify people in the publication team who will learn RAP process (PO and min 2 analysts) Identify people in the RAP team, or those familiar with RAP practices, to support the publication team through RAP transition Set up a Jira board for tracking work and confluence space for documenting learning, agreed decisions Teams meet and ice breaker session to get to know each other. Set out the plan for the engagement, review roles and responsibilities, lessons learned from previous projects, etc. Getting stuck in Pair up analysts from publication and RAP teams. These coding buddies will work collaboratively for the rest of the project. Identify the 'thin slice' of the publication that we will aim to replicate in the first phase. Identify training needs of the publication team through conversations Dedicated training. The content is tailored to the specific team but a typical sequence might look like: Concept of a thin slice and how to choose the thin slice Access to off-the-shelf interactive training for self-led training PySpark style guide Version control Writing good functions Unit tests Buddy pairs work to replicate the thin slide outputs Set up automated code testing once the numbers are correct Group code review of buddy pair code as a learning exercise Decide how to structure code for publication as a whole Portion out the rest of the publication to be replicated i.e. geographical breakdowns Each analyst takes a portion at a time working independently to produce using RAP standards seeking buddy support as needed Set up QA and RS checks pre-publication Being transparent Publish code on github Add a link to the code repository in the publication and a link to the publication in the code repository Show and tell on what we have learned, documents added to github Close-down Review the codebase to confirm that all the elements of baseline RAP (or silver/gold) have been met. Reflect on the resources and materials available throughout the process and suggest improvements or additions. Do a user-research session with the RAP team to suggest improvements to the service for those who come later. Take some time to reflect on the process internally as a team and consider how you plan to take forward the work.","title":"Typical engagement flow"},{"location":"our_RAP_service/typical-engagement-flow/#typical-engagement-flow","text":"This example describes a typical lifecycle of applying RAP principles to an existing product. It's important to remember that every team is different and so there is no really typical engagement, but hopefully the below gives you an idea of what's involved. The approach described is the support model where the RAP team leads the development as this is our most frequent approach. Please note that a direct engagement with the RAP team at NHS Digital is currently only available to NHS Digital teams. The below is intended to help other organisations that may be developing their own RAP capabilities and are looking for guidance.","title":"Typical Engagement Flow"},{"location":"our_RAP_service/typical-engagement-flow/#initial-outreach","text":"Hear about RAP through show-and-tells, internal advertising, slack, etc. Have a look over the RAP Community of Practice pages to get a sense of what it is all about. Reach out to the RAP team for a chat Together with someone from the RAP team, walk through the levels of RAP page and consider the team's scale of ambition. If you are keen to proceed, pass the request to the RAP product owner for prioritisation and resourcing Get a start date in the calendar. Think about publication timing, team capacity, etc. to choose a good time.","title":"Initial outreach"},{"location":"our_RAP_service/typical-engagement-flow/#setting-the-foundation","text":"Identify a publication/product that needs taking through RAP process Identify people in the publication team who will learn RAP process (PO and min 2 analysts) Identify people in the RAP team, or those familiar with RAP practices, to support the publication team through RAP transition Set up a Jira board for tracking work and confluence space for documenting learning, agreed decisions Teams meet and ice breaker session to get to know each other. Set out the plan for the engagement, review roles and responsibilities, lessons learned from previous projects, etc.","title":"Setting the foundation"},{"location":"our_RAP_service/typical-engagement-flow/#getting-stuck-in","text":"Pair up analysts from publication and RAP teams. These coding buddies will work collaboratively for the rest of the project. Identify the 'thin slice' of the publication that we will aim to replicate in the first phase. Identify training needs of the publication team through conversations Dedicated training. The content is tailored to the specific team but a typical sequence might look like: Concept of a thin slice and how to choose the thin slice Access to off-the-shelf interactive training for self-led training PySpark style guide Version control Writing good functions Unit tests Buddy pairs work to replicate the thin slide outputs Set up automated code testing once the numbers are correct Group code review of buddy pair code as a learning exercise Decide how to structure code for publication as a whole Portion out the rest of the publication to be replicated i.e. geographical breakdowns Each analyst takes a portion at a time working independently to produce using RAP standards seeking buddy support as needed Set up QA and RS checks pre-publication","title":"Getting stuck in"},{"location":"our_RAP_service/typical-engagement-flow/#being-transparent","text":"Publish code on github Add a link to the code repository in the publication and a link to the publication in the code repository Show and tell on what we have learned, documents added to github","title":"Being transparent"},{"location":"our_RAP_service/typical-engagement-flow/#close-down","text":"Review the codebase to confirm that all the elements of baseline RAP (or silver/gold) have been met. Reflect on the resources and materials available throughout the process and suggest improvements or additions. Do a user-research session with the RAP team to suggest improvements to the service for those who come later. Take some time to reflect on the process internally as a team and consider how you plan to take forward the work.","title":"Close-down"},{"location":"training_resources/R/","text":"R We do not have many materials for R since the majority of our users use Python. The NHS-R community has excellent resources including: Introduction to R training R Markdown Training From the UK Government Data Science: R RAP Companion","title":"External resources"},{"location":"training_resources/R/#r","text":"We do not have many materials for R since the majority of our users use Python. The NHS-R community has excellent resources including: Introduction to R training R Markdown Training From the UK Government Data Science: R RAP Companion","title":"R"},{"location":"training_resources/R/git_with_RStudio/","text":"Git with RStudio Overview This page is intended as a starting point for R users who are new to using Git. It shows you the basic commands and the workflow to use Git on your own, in conjunction with using RStudio desktop or RStudio Cloud. To use Git as a team you should complete this section and the following section on using git collaboratively . What is version control? See our guidance on version control and common basic commands . Setup for Git Basics exercise Creating your access token on GitHub There is an excellent step by step guide on How to Create your Access Token on GitHub by GitHub Docs. The process is exactly the same as GitLab (see credentials for GitLab ), simply input your GitHub username and access token to verify your credentials. When selecting the access token's scope, the default options that should be ticked are: repo, admin:repo_hook and delete_repo. Do not forget to paste/save your access token somewhere safe as you won't be able to access it again. Video: How to create an access token on GitHub Here's a video on how to create the access token on GitHub: Access token video HTTPS vs SSH key You might come across SSH protocol keys, as it is essentially another option to create a password and verify your credentials. This section on HTTPS vs SSH protocol options of setting up credentials on GitHub explains the differences between these two options. There are pros and cons for both, we would recommend the HTTPS option for Git beginners. Troubleshooting credentials You might see something like this at any point: remote: HTTP Basic: Access denied remote: You must use a personal access with 'read_repository or 'write_repository' scope for Git over HTTP. remote: You can generate one at https:// etc etc... remote: Authentication failed for 'repository address you're attempting to git clone' This can mean a number of things: either your access token has expired or you have entered the wrong access token for your password (or username even). If your access token has expired or about to expire, you can check by going to Gitlab Profile -> Preferences -> Access Tokens then scroll all the way down and see if there\u2019s an Active Access Token and check the expiration date. These steps will help you fix your access to the repository: To fix this we need to create a new access token (don\u2019t assign a date this time, so it will never expire). In the command terminal type git pull -v (v stands for verification). This should prompt a new Git credentials window. If you don't have an existing repository, and can't do step 2, then: In the terminal type git clone <git repo's copied clone url> but with the new credentials included in the URL, so: C:\\My_documents>git clone https://<username>:<password>@<domain>/example.git So, in my case it would be: C:\\My_documents>git clone https://<username>:<MY_NEW_ACCESS_TOKEN>@<domain>/example.git Exercises 1 - 6 Accessing a Git repository, creating a branch, adding a new file, uploading your changes to GitHub Getting started - tasks 1 and 2 will be relevant only when accessing a repository for the first time . Solution to exercises 1 - 6 (Using RStudio Desktop): Here's the video solution to exercises 1-6: Video exercises solution Using RStudio Desktop 1. Creating your local Git repository folder i. Open a Command terminal. This can be any terminal, Anaconda Prompt or Git Bash etc. To access the terminal, click the Search option on the Windows toolbar and type \"Anaconda Prompt\" and open it. The file directory address that's displayed in the terminal will usually be the default top level C drive address or something similar. (base) C:\\Windows\\system32> ii. Create a folder in your local directory, this is where the GitHub (remote) repository will be copied/downloaded to. In the Anaconda Prompt terminal type cd <windows explorer repo folder address you just created> , cd stands for Change Directory, and what this means is that the default terminal address will be directed to the new directory, the new folder address. Any Git commands applied in the terminal will apply to the current directory. (base) C:\\Windows\\system32>cd C:\\Users\\<username>\\Documents\\my_project_folder\\demo (base) C:\\Users\\<username>\\Documents\\my_project_folder\\demo> 2. Git clone (copy a GitHub repository) If you're using GitLab instead of GitHub, these steps will help you set up. i. The repository's clone URL will be on the GitHub repository's main page (see image below). ii. Type git clone <repository's clone url> the git-demo repository using your local terminal. Enter your credentials when prompted (GitHub username and access token for the password.) (base) C:\\Users\\<username>\\Documents\\my_project_folder\\demo>git clone <paste the repo url address from the image above> iii. Type cd <windows explorer repo folder address> (open the newly downloaded git folder) in the terminal to assign the root level of the directory to the top level of the repository. This because the terminal is assigned to the folder containing the Git repository, not the folder that IS the Git repository. iv. Type git status in the terminal. You should see the this message: 3. Create and switch to a new branch Notice how in the image above, the first line says \"On branch main\". In Git, \"main\" is a naming convention for a branch. After cloning (downloading) a project from a remote server, the resulting local repository has a single local branch: the so-called \"main\" branch. This means that \"main\" can be seen as a repository's \"default\" branch. The main branch should always be locked, to protect the main build of your working code and/or documentation. To add new code/documentation/make any edits to the main branch, you need to submit a pull request . \"Branch\" is another word for \"version\". Usually when developing a document or slides for a presentation, we could have numerous versions of the same slides, with small or major differences. By using different branches (or versions) of the same code, we can safely work and test without breaking the publication code, that resides in the default master branch of the repository. i. To create a new branch , head over to GitHub, on the repository's main page, same webpage we used to find the Git clone repository's URL. To create a new branch on GitHub simply click on the \"main\" button underneath your repository's title. ii. Once in the dropdown menu, type in the branch's new name and hit Enter. You should now have a new branch, copy of the main branch. iii. In the above image notice how it points the original branch we are copying from, in grey letters: from \"main\". If you wish to create a new branch, which will be a copy from a different branch to \"main\", then set the repository to the branch you wish to copy from and then repeat steps i. and ii. iv. Back to the terminal (e.g. Command Prompt/Anaconda Prompt/Git Bash etc.), type git branch -a to view a list of all available branches existing in the repository. The branches that contain the remote/ path are the branches online, on GitHub. Have a look at the list. You will notice your new branch is not there yet. To update your local master branch with all the latest updates on Gitlab, type git pull . Type git branch -a and you should see your branch available in the list now. v. To start working on your branch and select, type in the terminal git checkout <your branch name> . vi. Type git status and this way you can confirm that you are in a new branch. Git will display a message showing the current branch name and the current branch's information. Reminder: any changes you make while in the main branch won't be uploaded to GitHub, as the main branch is protected and locked, so make sure you are in your own working branch! You can confirm this with step vi. above. Branch naming strategy Following an agreed naming convention will make it much easier to keep the team's work organised. We follow use this approach: <project_id>_<initials>_<ticket_number>_<brief-description> Here the project_id refers to our project ID in confluence (DS218). The initials are from my name. The ticket number refers to the jira ticket number. E.g.: DS218_CQ_167_Add-field-definitions Naming branches in this way helps to make it very clear what change should be happening in the branch. Sometimes code review will reveal that a branch that was intended to make one specific change in fact makes several changes. This should be avoided and the code review should reject the additional changes. 4. Add a new file By adding a new file to the project, you can avoid conflicts as each analyst will create and upload their own file. There are many ways to add a new file to your branch. One way is to upload a file directly to your branch on GitHub. The easiest way is to simply create the file (any type of file) in your working directory, while in your branch. You can also copy a file from another directory. The file you create or copy can be any type of file. For this exercise, create a file while in RStudio, with simply right-clicking on the repository's folder and then selecting the Create New File option: To view and edit the repository's folders and files in RStudio: i. Open RStudio ii. On the top toolbar go File -> New Project, this will open a Create New Project dialogue. iii. In the Create New Project dialogue, select existing directory (since we have already cloned the GitHub repository to your local machine and have a folder) and in Location select the Open Folder icon to select the existing folder repository and click Create Project. iv. You should be able to see the project structure on the lower right hand side window as it is displayed on GitHub. v. On the top toolbar select File -> New File -> Rscript and save the file somewhere in your repository (don't forget to name your Rscript!). vi. Add something into that file, a simple print statement or a comment. vii. Save it on RStudio. Tip: RStudio will have created a .Rproj/.Rhistory configuration file when you create an R project. To avoid having this uploaded to GitHub, a .gitignore file that contains those file formats will automatically block them from being pushed to the repository. See .gitignore for more information. 5. Commit your changes (follow standard Git command workflow) In the Anaconda Prompt (or any terminal of your choice) type the following. I suggest you read through the git messages displayed after each command entered in the terminal, to familiarise yourself with the logic. i. Type git status to see the modifications that haven't been staged yet (in red colour). ii. Type git add <filename> to stage your changes. Or git add . to simply stage all changes automatically (use with caution). iii. Type git status to see the file modifications now have a green colour. This means that Git add was successful. iv. Type git commit -m \"your commit message here\" to commit these changes. v. Type git status to view the status of your repo. There should be no modifications visible (or colours!). vi. Type git push . This command will upload all your committed changes to GitHub. 6. Check the repository on GitHub to view the changes updated Simply head over to GitHub, and the repository page on GitHub, select your branch and compare the changes between your branch and the master branch. Using RStudio Cloud To use GitHub with RStudio Cloud and not RStudio desktop, first create an account with RStudio Cloud. RStudio Cloud has a detailed guide on how to create projects, how to manage workspaces within the Cloud, how to handle permissions and teaching spaces. To create a project from GitHub, select the \"Your Workspace\" option on the left hand side once in RStudio Cloud and select New Project on the far right of the top toolbar, in the Your Projects space. This will open a new dialogue that will ask for the Git Repository URL, this is the URL you copy, as described in Step 2 of this guide. You will be prompted to enter your username (GitHub username) and your password, which is the access token created from the Create Access token step . Once the Git repository is deployed, you will notice the repository file structure on the right hand side panel. Steps 3 , 4 and 5 will be the same, the only change being that instead of using your local terminal to input Git commands, you will be using the RStudio Cloud built-in terminal. How to submit a pull request Pull request (PR) is the application you submit on the GitHub repository, to announce to other collaborators working on the repository that you have a new change ready to be merged/part of the main build, the main branch of the repository. Then, either your collaborators or someone you assign will review the change and the branch you have been working on to determine whether the change is ready to be merged with the main branch. The equivalent term for GitLab users is Merge Request (MR). To submit a pull request: Simply head over to GitHub and to the respective repository Select Pull requests on the top bar menu. Either select New Pull Request (big green button) on the right hand side or select Compare & pull request if you recently pushed new changes to the branch. Base ref will default to the main branch, head ref will be the branch you wish to merge with the main branch. You can change any of these options, there will be occasions you won't always aim to merge every branch to the main branch. Click Create Pull Request. Fill out any information related to the Pull request, title, description, any comments. On the right hand side you can assign yourself as the Assignee, and assign someone in your team to be the Reviewer. Then, select Create Pull Request. How to accept a pull request Once you are assigned the pull request's reviewer, you should receive an email notification from GitHub with a link to the pull request's webpage. You can also view any active pull requests by clicking on Pull Requests on the top bar menu. On the Pull request page, there are four tabs: Conversation, Commits, Checks and Files changed: Conversation Conversation is the main pull request page. It contains the pull request's description, timeline and comments left by the assignee or reviewer. To merge a pull request you have 3 presented options: Merge pull request: this will keep all commits history of the source branch and will be added to the target branch. Creates a symbolic commit. Squash and merge: ticking this will merge the history of commits for the source branch into one commit. This can be useful when the pull request is not a major update and you don't want your repository's history of commits to contain commits with messages such as \"fixed typo\", \"replaced incorrectly placed image\". Creates a symbolic commit. Rebase and merge: adds all source branch commits in front of the target branch's commits. Does not create a symbolic commit. Commits Commits tab displays a history of the pull request's commits. So for example, if the reviewer decides that minor adjustments are required and the assignee pushes these changes to Gitlab, then these commits will be captured in this tab. Files changes Files changed tab, here you can see all the new changes in the pull request's source branch. You can also opt for a side-by-side comparison, and see line-by-line how the master version and new branch version compare. You can also leave comments on each file's line, which will also be recorded in the Conversation tab's timeline. Checks Checks is part of the GitHub Actions functionality, which enhances your code review processes with apps and automated reviewing checks. This is inactive on default. If you are happy to merge the two branches, then select the type of Pull Request you wish to apply and the pull request should be completed in a few seconds. How to update your local main branch to the latest remote main version on GitHub/GitLab Open the Anaconda Prompt terminal or Git Bash or any other terminal you wish to use. This can be accessed by typing, for example Anaconda Prompt, in the search bar, after opening the search icon on your Windows toolbar. Type cd <paste your work directory address> in the terminal to switch to your working directory. This is where the Git repository is cloned/copied/downloaded to, on your local machine (or RDS environment or any environment you're using). This will cause the default terminal address to be directed at the new directory. (see Git Clone instructions above for more info) Type git status in the terminal to ensure that ii happened successfully and you are on the default main branch. You should see this message: Type git pull to update your local main branch with the latest version of the remote/online GitHub main branch. This might take a few seconds. (Optional) 5. If you're in a different branch and not the main branch, then type in the terminal git checkout main and then git status to confirm you are on the main branch. External links GitHub Docs - Pull requests","title":"Git with RStudio"},{"location":"training_resources/R/git_with_RStudio/#git-with-rstudio","text":"","title":"Git with RStudio"},{"location":"training_resources/R/git_with_RStudio/#overview","text":"This page is intended as a starting point for R users who are new to using Git. It shows you the basic commands and the workflow to use Git on your own, in conjunction with using RStudio desktop or RStudio Cloud. To use Git as a team you should complete this section and the following section on using git collaboratively .","title":"Overview"},{"location":"training_resources/R/git_with_RStudio/#what-is-version-control","text":"See our guidance on version control and common basic commands .","title":"What is version control?"},{"location":"training_resources/R/git_with_RStudio/#setup-for-git-basics-exercise","text":"","title":"Setup for Git Basics exercise"},{"location":"training_resources/R/git_with_RStudio/#creating-your-access-token-on-github","text":"There is an excellent step by step guide on How to Create your Access Token on GitHub by GitHub Docs. The process is exactly the same as GitLab (see credentials for GitLab ), simply input your GitHub username and access token to verify your credentials. When selecting the access token's scope, the default options that should be ticked are: repo, admin:repo_hook and delete_repo. Do not forget to paste/save your access token somewhere safe as you won't be able to access it again.","title":"Creating your access token on GitHub"},{"location":"training_resources/R/git_with_RStudio/#video-how-to-create-an-access-token-on-github","text":"Here's a video on how to create the access token on GitHub: Access token video","title":"Video: How to create an access token on GitHub"},{"location":"training_resources/R/git_with_RStudio/#https-vs-ssh-key","text":"You might come across SSH protocol keys, as it is essentially another option to create a password and verify your credentials. This section on HTTPS vs SSH protocol options of setting up credentials on GitHub explains the differences between these two options. There are pros and cons for both, we would recommend the HTTPS option for Git beginners.","title":"HTTPS vs SSH key"},{"location":"training_resources/R/git_with_RStudio/#troubleshooting-credentials","text":"You might see something like this at any point: remote: HTTP Basic: Access denied remote: You must use a personal access with 'read_repository or 'write_repository' scope for Git over HTTP. remote: You can generate one at https:// etc etc... remote: Authentication failed for 'repository address you're attempting to git clone' This can mean a number of things: either your access token has expired or you have entered the wrong access token for your password (or username even). If your access token has expired or about to expire, you can check by going to Gitlab Profile -> Preferences -> Access Tokens then scroll all the way down and see if there\u2019s an Active Access Token and check the expiration date. These steps will help you fix your access to the repository: To fix this we need to create a new access token (don\u2019t assign a date this time, so it will never expire). In the command terminal type git pull -v (v stands for verification). This should prompt a new Git credentials window. If you don't have an existing repository, and can't do step 2, then: In the terminal type git clone <git repo's copied clone url> but with the new credentials included in the URL, so: C:\\My_documents>git clone https://<username>:<password>@<domain>/example.git So, in my case it would be: C:\\My_documents>git clone https://<username>:<MY_NEW_ACCESS_TOKEN>@<domain>/example.git","title":"Troubleshooting credentials"},{"location":"training_resources/R/git_with_RStudio/#exercises-1-6","text":"Accessing a Git repository, creating a branch, adding a new file, uploading your changes to GitHub Getting started - tasks 1 and 2 will be relevant only when accessing a repository for the first time . Solution to exercises 1 - 6 (Using RStudio Desktop): Here's the video solution to exercises 1-6: Video exercises solution","title":"Exercises 1 - 6"},{"location":"training_resources/R/git_with_RStudio/#using-rstudio-desktop","text":"","title":"Using RStudio Desktop"},{"location":"training_resources/R/git_with_RStudio/#1-creating-your-local-git-repository-folder","text":"i. Open a Command terminal. This can be any terminal, Anaconda Prompt or Git Bash etc. To access the terminal, click the Search option on the Windows toolbar and type \"Anaconda Prompt\" and open it. The file directory address that's displayed in the terminal will usually be the default top level C drive address or something similar. (base) C:\\Windows\\system32> ii. Create a folder in your local directory, this is where the GitHub (remote) repository will be copied/downloaded to. In the Anaconda Prompt terminal type cd <windows explorer repo folder address you just created> , cd stands for Change Directory, and what this means is that the default terminal address will be directed to the new directory, the new folder address. Any Git commands applied in the terminal will apply to the current directory. (base) C:\\Windows\\system32>cd C:\\Users\\<username>\\Documents\\my_project_folder\\demo (base) C:\\Users\\<username>\\Documents\\my_project_folder\\demo>","title":"1. Creating your local Git repository folder"},{"location":"training_resources/R/git_with_RStudio/#2-git-clone-copy-a-github-repository","text":"If you're using GitLab instead of GitHub, these steps will help you set up. i. The repository's clone URL will be on the GitHub repository's main page (see image below). ii. Type git clone <repository's clone url> the git-demo repository using your local terminal. Enter your credentials when prompted (GitHub username and access token for the password.) (base) C:\\Users\\<username>\\Documents\\my_project_folder\\demo>git clone <paste the repo url address from the image above> iii. Type cd <windows explorer repo folder address> (open the newly downloaded git folder) in the terminal to assign the root level of the directory to the top level of the repository. This because the terminal is assigned to the folder containing the Git repository, not the folder that IS the Git repository. iv. Type git status in the terminal. You should see the this message:","title":"2. Git clone (copy a GitHub repository)"},{"location":"training_resources/R/git_with_RStudio/#3-create-and-switch-to-a-new-branch","text":"Notice how in the image above, the first line says \"On branch main\". In Git, \"main\" is a naming convention for a branch. After cloning (downloading) a project from a remote server, the resulting local repository has a single local branch: the so-called \"main\" branch. This means that \"main\" can be seen as a repository's \"default\" branch. The main branch should always be locked, to protect the main build of your working code and/or documentation. To add new code/documentation/make any edits to the main branch, you need to submit a pull request . \"Branch\" is another word for \"version\". Usually when developing a document or slides for a presentation, we could have numerous versions of the same slides, with small or major differences. By using different branches (or versions) of the same code, we can safely work and test without breaking the publication code, that resides in the default master branch of the repository. i. To create a new branch , head over to GitHub, on the repository's main page, same webpage we used to find the Git clone repository's URL. To create a new branch on GitHub simply click on the \"main\" button underneath your repository's title. ii. Once in the dropdown menu, type in the branch's new name and hit Enter. You should now have a new branch, copy of the main branch. iii. In the above image notice how it points the original branch we are copying from, in grey letters: from \"main\". If you wish to create a new branch, which will be a copy from a different branch to \"main\", then set the repository to the branch you wish to copy from and then repeat steps i. and ii. iv. Back to the terminal (e.g. Command Prompt/Anaconda Prompt/Git Bash etc.), type git branch -a to view a list of all available branches existing in the repository. The branches that contain the remote/ path are the branches online, on GitHub. Have a look at the list. You will notice your new branch is not there yet. To update your local master branch with all the latest updates on Gitlab, type git pull . Type git branch -a and you should see your branch available in the list now. v. To start working on your branch and select, type in the terminal git checkout <your branch name> . vi. Type git status and this way you can confirm that you are in a new branch. Git will display a message showing the current branch name and the current branch's information. Reminder: any changes you make while in the main branch won't be uploaded to GitHub, as the main branch is protected and locked, so make sure you are in your own working branch! You can confirm this with step vi. above.","title":"3. Create and switch to a new branch"},{"location":"training_resources/R/git_with_RStudio/#branch-naming-strategy","text":"Following an agreed naming convention will make it much easier to keep the team's work organised. We follow use this approach: <project_id>_<initials>_<ticket_number>_<brief-description> Here the project_id refers to our project ID in confluence (DS218). The initials are from my name. The ticket number refers to the jira ticket number. E.g.: DS218_CQ_167_Add-field-definitions Naming branches in this way helps to make it very clear what change should be happening in the branch. Sometimes code review will reveal that a branch that was intended to make one specific change in fact makes several changes. This should be avoided and the code review should reject the additional changes.","title":"Branch naming strategy"},{"location":"training_resources/R/git_with_RStudio/#4-add-a-new-file","text":"By adding a new file to the project, you can avoid conflicts as each analyst will create and upload their own file. There are many ways to add a new file to your branch. One way is to upload a file directly to your branch on GitHub. The easiest way is to simply create the file (any type of file) in your working directory, while in your branch. You can also copy a file from another directory. The file you create or copy can be any type of file. For this exercise, create a file while in RStudio, with simply right-clicking on the repository's folder and then selecting the Create New File option: To view and edit the repository's folders and files in RStudio: i. Open RStudio ii. On the top toolbar go File -> New Project, this will open a Create New Project dialogue. iii. In the Create New Project dialogue, select existing directory (since we have already cloned the GitHub repository to your local machine and have a folder) and in Location select the Open Folder icon to select the existing folder repository and click Create Project. iv. You should be able to see the project structure on the lower right hand side window as it is displayed on GitHub. v. On the top toolbar select File -> New File -> Rscript and save the file somewhere in your repository (don't forget to name your Rscript!). vi. Add something into that file, a simple print statement or a comment. vii. Save it on RStudio. Tip: RStudio will have created a .Rproj/.Rhistory configuration file when you create an R project. To avoid having this uploaded to GitHub, a .gitignore file that contains those file formats will automatically block them from being pushed to the repository. See .gitignore for more information.","title":"4. Add a new file"},{"location":"training_resources/R/git_with_RStudio/#5-commit-your-changes-follow-standard-git-command-workflow","text":"In the Anaconda Prompt (or any terminal of your choice) type the following. I suggest you read through the git messages displayed after each command entered in the terminal, to familiarise yourself with the logic. i. Type git status to see the modifications that haven't been staged yet (in red colour). ii. Type git add <filename> to stage your changes. Or git add . to simply stage all changes automatically (use with caution). iii. Type git status to see the file modifications now have a green colour. This means that Git add was successful. iv. Type git commit -m \"your commit message here\" to commit these changes. v. Type git status to view the status of your repo. There should be no modifications visible (or colours!). vi. Type git push . This command will upload all your committed changes to GitHub.","title":"5. Commit your changes (follow standard Git command workflow)"},{"location":"training_resources/R/git_with_RStudio/#6-check-the-repository-on-github-to-view-the-changes-updated","text":"Simply head over to GitHub, and the repository page on GitHub, select your branch and compare the changes between your branch and the master branch.","title":"6. Check the repository on GitHub to view the changes updated"},{"location":"training_resources/R/git_with_RStudio/#using-rstudio-cloud","text":"To use GitHub with RStudio Cloud and not RStudio desktop, first create an account with RStudio Cloud. RStudio Cloud has a detailed guide on how to create projects, how to manage workspaces within the Cloud, how to handle permissions and teaching spaces. To create a project from GitHub, select the \"Your Workspace\" option on the left hand side once in RStudio Cloud and select New Project on the far right of the top toolbar, in the Your Projects space. This will open a new dialogue that will ask for the Git Repository URL, this is the URL you copy, as described in Step 2 of this guide. You will be prompted to enter your username (GitHub username) and your password, which is the access token created from the Create Access token step . Once the Git repository is deployed, you will notice the repository file structure on the right hand side panel. Steps 3 , 4 and 5 will be the same, the only change being that instead of using your local terminal to input Git commands, you will be using the RStudio Cloud built-in terminal.","title":"Using RStudio Cloud"},{"location":"training_resources/R/git_with_RStudio/#how-to-submit-a-pull-request","text":"Pull request (PR) is the application you submit on the GitHub repository, to announce to other collaborators working on the repository that you have a new change ready to be merged/part of the main build, the main branch of the repository. Then, either your collaborators or someone you assign will review the change and the branch you have been working on to determine whether the change is ready to be merged with the main branch. The equivalent term for GitLab users is Merge Request (MR). To submit a pull request: Simply head over to GitHub and to the respective repository Select Pull requests on the top bar menu. Either select New Pull Request (big green button) on the right hand side or select Compare & pull request if you recently pushed new changes to the branch. Base ref will default to the main branch, head ref will be the branch you wish to merge with the main branch. You can change any of these options, there will be occasions you won't always aim to merge every branch to the main branch. Click Create Pull Request. Fill out any information related to the Pull request, title, description, any comments. On the right hand side you can assign yourself as the Assignee, and assign someone in your team to be the Reviewer. Then, select Create Pull Request.","title":"How to submit a pull request"},{"location":"training_resources/R/git_with_RStudio/#how-to-accept-a-pull-request","text":"Once you are assigned the pull request's reviewer, you should receive an email notification from GitHub with a link to the pull request's webpage. You can also view any active pull requests by clicking on Pull Requests on the top bar menu. On the Pull request page, there are four tabs: Conversation, Commits, Checks and Files changed:","title":"How to accept a pull request"},{"location":"training_resources/R/git_with_RStudio/#conversation","text":"Conversation is the main pull request page. It contains the pull request's description, timeline and comments left by the assignee or reviewer. To merge a pull request you have 3 presented options: Merge pull request: this will keep all commits history of the source branch and will be added to the target branch. Creates a symbolic commit. Squash and merge: ticking this will merge the history of commits for the source branch into one commit. This can be useful when the pull request is not a major update and you don't want your repository's history of commits to contain commits with messages such as \"fixed typo\", \"replaced incorrectly placed image\". Creates a symbolic commit. Rebase and merge: adds all source branch commits in front of the target branch's commits. Does not create a symbolic commit.","title":"Conversation"},{"location":"training_resources/R/git_with_RStudio/#commits","text":"Commits tab displays a history of the pull request's commits. So for example, if the reviewer decides that minor adjustments are required and the assignee pushes these changes to Gitlab, then these commits will be captured in this tab.","title":"Commits"},{"location":"training_resources/R/git_with_RStudio/#files-changes","text":"Files changed tab, here you can see all the new changes in the pull request's source branch. You can also opt for a side-by-side comparison, and see line-by-line how the master version and new branch version compare. You can also leave comments on each file's line, which will also be recorded in the Conversation tab's timeline.","title":"Files changes"},{"location":"training_resources/R/git_with_RStudio/#checks","text":"Checks is part of the GitHub Actions functionality, which enhances your code review processes with apps and automated reviewing checks. This is inactive on default. If you are happy to merge the two branches, then select the type of Pull Request you wish to apply and the pull request should be completed in a few seconds.","title":"Checks"},{"location":"training_resources/R/git_with_RStudio/#how-to-update-your-local-main-branch-to-the-latest-remote-main-version-on-githubgitlab","text":"Open the Anaconda Prompt terminal or Git Bash or any other terminal you wish to use. This can be accessed by typing, for example Anaconda Prompt, in the search bar, after opening the search icon on your Windows toolbar. Type cd <paste your work directory address> in the terminal to switch to your working directory. This is where the Git repository is cloned/copied/downloaded to, on your local machine (or RDS environment or any environment you're using). This will cause the default terminal address to be directed at the new directory. (see Git Clone instructions above for more info) Type git status in the terminal to ensure that ii happened successfully and you are on the default main branch. You should see this message: Type git pull to update your local main branch with the latest version of the remote/online GitHub main branch. This might take a few seconds. (Optional) 5. If you're in a different branch and not the main branch, then type in the terminal git checkout main and then git status to confirm you are on the main branch.","title":"How to update your local main branch to the latest remote main version on GitHub/GitLab"},{"location":"training_resources/R/git_with_RStudio/#external-links","text":"GitHub Docs - Pull requests","title":"External links"},{"location":"training_resources/git/intro-to-git/","text":"Intro to Git Overview This page is intended as a starting point for someone who is new to using git. It shows you the basic commands and the workflow to use git on your own. To use git as a team you should complete this section and the following section on using Git collaboratively . What is version control? Version control is the practice of tracking and managing any changes on project's code, files or folders over time. This allows you to observe a detailed history of the changes made and enables your team members to collaborate on the same project. In Git, each user has the entire repository (project's working directory) on their computer (offline), hence, they can work separately offline until they opt to push their updated version of the code to the remote (online) central repository. Remote repositories are versions of your project that are hosted on an online Version Control System. In NHS Digital, we store that in GitLab site for internal users (GitHub site for the public). Why should I care? Using version control is one of the fundamental skills needed by analysts to produce high-quality analytical outputs. Git is the standard for tracking code over time and is the way that NHS Digital has chosen. While git has a steep initial learning curve, the payoff is huge and so we strongly recommend taking the time to learn this as a team. The benefits of using version control include: Understanding what happened in the past. E.g., what did we change last time we ran this code Restoring previous versions Tracking changes - avoid accidentally breaking code The ability to review someone's changes and to leave comments The ability to plan development work more effectively through being able to assign small, discrete changes Avoid code being hidden away on someone's machine The ability to set up an approval process for changes The ability to make changes without breaking anything - through running automated tests The ability to try out experiments without the risk of breaking your main code Glossary of terms Here's a list of commonly used Git terms, that will help you understand the technical jargon mentioned in the next sections: Git terminology Video showing Git workflow and commands This short video tutorial walks you through how to create a repository and use basic Git commands: Link to video tutorial We have also produced a short video that gives some more theoretical explanation of how version control works: Link to video Common basic commands Below is a list of common commands for reference. We only list basic commands here. This is just to flag to you that these commands exist. You can google for more detail on any of them. Git commands cheat-sheet Create a new git repo locally: git init . The init command is short for \"initialise\", it's the command that will do all of the initial setup of a repository. The folder needs to actually exist before we can create a new repository with Git. Clone an existing Git Repository: git clone <url> . Happens only once, when you need to create a local copy of a GitLab repository. Check whether any files have changed in a repository: git status . It lists the files you've changed and those you still need to add or commit. It displays the state of the working directory and the staging area. It lets you see which changes have been staged, which haven't, and which files aren't being tracked by Git. Status output does not show you any information regarding the committed project history. Create and switch to a new branch: git checkout -b <your branch name> . Check out a branch: git checkout <your branch name. . Check out an existing new branch. Add file contents/stages changes from the working directory to the index: git add <filename> or git add . Commit staged files with a message: git commit -m \"<message>\" . Committing changes in this way captures a snapshot of the project's currently staged changes. Committed snapshots can be thought of as \u201csafe\u201d versions of a project Update your branch with the online/remote branch info: git pull . Pulls the latest changes from the repository, only affects the current active branch you\u2019re in. Check the history of commits: git log . Show unstaged changes between your index and working directory: git diff . Press Q to exit the diff log. Ignore files: .git-ignore file . This file specifies untracked files that git should ignore such as the sensitive information related to security or the data itself. Files already tracked by git are not affected. Display a list of all branches in the repository: git branch -a . Delete a local branch: git branch -D <your branch name> . Undoing changes via Git commands GitLab has a guide covering various cases on undoing any changes: undo possibilities in Git . What is a terminal? A terminal acts like a mediator, takes text input (a command) and returns an output (a process, a result, an action) that depends on the command input's process. Command line terminals can be used to input commands that refer to file directories, list of directories and/or moving folder and files but also for version control (Git) and creating isolated environments (conda, venv etc). Anaconda prompt is very similar to other command line terminals (e.g. Windows Command Prompt), but allows you to use Anaconda commands, which do not work in other terminals on Windows. Different terminals use slightly different languages, and so you may find that if you're used to the terminal on a Mac, you need different commands when working on a Windows machine. Some terminals allow you to workaround this- such as Git Bash . Git Bash works on Windows, but emulates the most common terminal experience on a Mac or Linux, allowing to use the same commands. Useful commands Anaconda Prompt takes most or all of the commands that Windows Command Prompt takes, but also includes Conda-specific commands: Command reference . For new starters and for the purposes of this guide, dir (list of files within the directory) and cd (change directory) are the major terminal commands required for Anaconda Prompt. Example of Windows vs Linux commands differences: list files - dir (Windows, incl. Anaconda Prompt) - ls (mac, linux) change directory - cd display current directory - pwd (mac, linux) - chpwd or %cd% (Windows) List of other command differences For more info on conda virtual environments see here . Setup for Git Basics exercise Creating your access token When cloning a Git repository from a web-hosted repository platform such as GitHub or GitLab for the first time, you will be prompted to input your credentials, which is usually your username and password. Where two-factor authentication is enabled, and for internal repository platforms, you will need to generate an access token which you use as your password. For NHS Digital employees using our internal GitLab, your username is your GitLab username and your password is your access token. In Gitlab, navigate to Preferences by clicking the top right profile picture: On the left hand side menu select Access token. You should see this page after clicking: Add a token name, select all boxes for the scope, leave the date blank (token will never expire) and select Create Access Token. The access token that will appear at the top of the page needs to saved in a text document (Notepad etc). Troubleshooting credentials You might see something like this at any point: remote: HTTP Basic: Access denied remote: You must use a personal access with 'read_repository or 'write_repository' scope for Git over HTTP. remote: You can generate one at https:// etc etc... remote: Authentication failed for 'repository address you're attempting to git clone' This can mean a number of things: either your access token has expired or you have entered the wrong access token for your password (or username even). If your access token has expired or about to expire, you can check by going to GitLab Profile -> Preferences -> Access Tokens then scroll all the way down and see if there\u2019s an Active Access Token and check the expiration date. These steps will help you fix your access to the repository: To fix this we need to create a new access token (don\u2019t assign a date this time, so it will never expire). In the command terminal type git pull -v (v stands for verification). This should prompt a new git credentials window. If you don't have an existing repository, and can't do step 2, then: In the terminal type git clone <git repo copied clone url> but with the new credentials included in the URL, so: C:\\My_documents>git clone https://<username>:<password>@<domain>/example.git So, in my case it would be: C:\\My_documents>git clone https://<username>:<MY_NEW_ACCESS_TOKEN>@<domain>/example.git Intro Exercises Complete tasks 1 - 6: Accessing a git repository, creating a branch, adding a new file, uploading your changes to GitLab Getting started - tasks 1 and 2 will be relevant only when accessing a repository for the first time . If stuck at any point feel free to go through the video tutorials mentioned in the previous sections of this guide). If stuck on creating an access token watch this . Disclaimer: the above video will not load for external users. 1. Creating your local git repository folder i. Open a Command terminal. This can be any terminal, Anaconda Prompt or Git Bash etc. To access the terminal, click the Search option on the Windows toolbar and type \"Anaconda Prompt\" and open it. The file directory address that's displayed in the terminal will usually be the default top level C drive address or something similar. (base) C:\\Windows\\system32> ii. Create a folder in your local directory, this is where the GitLab (remote) repository will be copied/downloaded to. In the Anaconda Prompt terminal type cd <windows explorer repo folder address you just created> , cd stands for Change Directory, and what this means is that the default terminal address will be directed to the new directory, the new folder address. Any git commands applied in the terminal will apply to the current directory. (base) C:\\Windows\\system32>cd C:\\Users\\<username>\\Documents\\my_project_folder\\RAP (base) C:\\Users\\<username>\\Documents\\my_project_folder\\RAP> 2. Git clone (copy/download a GitLab repository) i. The repository's clone URL will be on the GitLab repository's main page (see image below). [Git demo repo with practice exercises] ii. Type git clone <repository's clone url> the git-demo repository using your local terminal (Anaconda Prompt). Enter your credentials when prompted (GitLab username and access token for the password.) (base) C:\\Users\\<username>\\Documents\\my_project_folder\\RAP>git clone <paste the repo url address from the image above> iii. Type cd <windows explorer repo folder address> (open the newly downloaded git folder) in the Anaconda Prompt to assign the root level of the directory to the top level of the repository. This because the terminal is assigned to the folder containing the git repository, not the folder that IS the git repository. iv. Type git status in the terminal. You should see the this message: 3. Create and switch to a new branch Notice how in the image above, the first line says \"On branch master\". In Git, \"master\" is a naming convention for a branch. After cloning (downloading) a project from a remote server, the resulting local repository has a single local branch: the so-called \"master\" branch. This means that \"master\" can be seen as a repository's \"default\" branch. The master should always be locked, to protect the main build of your working code and/or documentation. To add new code/documentation/make any edits to the master branch, you need to submit a merge request . \"Branch\" is another word for \"version\". Usually when developing a document or slides for a presentation, we could have numerous versions of the same slides, with small or major differences. By using different branches (or versions) of the same code, we can safely work and test without breaking the publication code, that resides in the default master branch of the repository. i. To create a new branch , head over to GitLab, on the repository's main page, same webpage we used to find the git clone repository's URL. On the left hand side menu select Repository -> Branches. ii. Once in the branches list page, select New Branch (big blue button) on the right hand side. iii. In the New Branch page, type your branch name and select the branch you want to copy from. On most cases it will be the master branch so GitLab has this option select by default. Once happy with your choices, select Create branch. iv. Back to the terminal (e.g. Anaconda Prompt etc), type git branch -a to view a list of all available branches existing in the repository. The branches that contain the remote/ path are the branches online, on GitLab. Have a look at the list. You will notice your new branch is not there yet. To update your local master branch with all the latest updates on GitLab, type git pull . Type git branch -a and you should see your branch available in the list now. v. To start working on your branch and select, type in the terminal git checkout <your branch name> . vi. Type git status and this way you can confirm that you are in a new branch. Git will display a message showing the current branch name and the current branch's information. Reminder: any changes you make while in the master branch won't be uploaded to GitLab, as the master branch is protected and locked, so make sure you are in your own working branch! You can confirm this with step vi. above. Branch naming strategy Following an agreed naming convention will make it much easier to keep the team's work organised. We follow use this approach: <project_id>_<initials>_<ticket_number>_<brief-description> Here the project_id refers to our project ID in confluence (DS218). The initials are from my name. The ticket number refers to the jira ticket number. E.g.: DS218_CQ_167_Add-field-definitions Naming branches in this way helps to make it very clear what change should be happening in the branch. Sometimes code review will reveal that a branch that was intended to make one specific change in fact makes several changes. This should be avoided and the code review should reject the additional changes. 4. Add a new file By adding a new file to the project, you can avoid conflicts as each analyst will create and upload their own file. There are many ways to add a new file to your branch. One way is to upload a file directly to your branch on GitLab. The easiest way is to simply create the file (any type of file) in your working directory, while in your branch. You can also copy a file from another directory. The file you create or copy can be any type of file. For VS Code users To open VS Code from the terminal (this will save you a few extra clicks) simply type in the Command Prompt of your choice (e.g. Anaconda Prompt/Git Bash/Powershell): Once VS Code loads, you will notice the on the left hand side panel the folder structure of the cloned repository: You can continue using the Command Prompt or you can continue with using the VS Code built in terminal. To add a new file in your repository right click in the folder structure, in this case we want to add a file in the practice folder of the repository: Name the file, add a file format in the name, for example .py for a python file, .md for a markdown file etc. Type something in the new file! Save your changes within VS Code, either from the top toolbar File -> Save or by Ctrl + S . For Spyder users For this exercise, create a file while in Spyder, with simply right-clicking on the repository's folder and then selecting the Create New File option: To view and edit the repository's folders and files in Spyder: Open Spyder On the top toolbar go Projects -> New Project, this will open a Create New Project dialogue. In the Create New Project dialogue, select existing directory (since we have already cloned the GitLab repository to your local machine and have a folder) and in Location select the Open Folder icon to select the existing folder repository: In the Select directory dialogue, select the folder that contains your repository's contents. In this example, I have a repository called \"demo\": You should be able to see the project structure on the left hand side menu as it is displayed on GitLab, in the image above. Right-click on the Practice folder and select New and New File. Name the file. Add something into that file. Save it on Spyder. 5. Commit your changes (follow basic git command workflow) In the Anaconda Prompt (or any terminal of your choice) type the following. I suggest you read through the git messages displayed after each command entered in the terminal, to familiarise yourself with the logic. i. Type git status to see the modifications that haven't been staged yet (in red colour). ii. Type git add <filename> to stage your changes. Or git add . to simply stage all changes automatically (use with caution). iii. Type git status to see the file modifications now have a green colour. This means that git add was successful. iv. Type git commit -m \"your commit message here\" to commit these changes. v. Type git status to view the status of your repo. There should be no modifications visible (or colours!). vi. Type git push . This command will upload all your committed changes to GitLab. 6. Check the repository on GitLab to view the changes updated Simply head over to GitLab, and the repository page on GitLab, select your branch and compare the changes between your branch and the master branch. How to submit a merge request To submit a merge request: Simply head over to GitLab and to the respective repository Select Merge requests on the menu on the left hand side. On the right select New Merge Request (big blue button). Target branch will always (in most cases) be the master branch. Source branch is your working branch. Click Compare branches and Continue Fill out any information related to the Merge request, title, description, any comments. Assign yourself as the Assignee, assign someone in your team to be the Reviewer. Then, select Create Merge Request. Mark your merge request as a draft on GitLab After submitting a merge/pull request, you can block the request from being merged if you opt for the \"Mark as draft option\": This will assign your request as a draft merge request, blocking it from being merged. Once the development work is completed then you can unassign the \"Mark as draft\" option and the merge request can be accepted. How to accept a merge request on GitLab Once you are assigned the merge request's reviewer, you should receive an email notification from GitLab with a link to the merge request's webpage. You can also view any active merge requests by clicking on the left hand side menu, once in the repository's page. In the merge request page, there are three tabs: Overview, Commits and Changes. Overview is the main merge request page. It contains the merge request's description, timeline and comments left by the assignee or reviewer. The approve button does not impact the merge request itself. Not selecting the Approve button won't affect the status or progress of the merge request. GitLab has made this available for teams that are interested in 2-step review processes where one analyst submits a merge request, another approves the request and the reviewer confirms the merge. Deleting the Source branch: this will automatically delete the branch created for the merge request. Untick this option to keep the source branch. Squash commits: ticking this will merge the history of commits for the source branch into one commit. This can be useful when the merge request is not a major update and you don't want your repository's history of commits to contain commits with messages such as \"fixed typo\" \"replaced incorrectly placed image\". Commits tab displays a history of the merge request's commits. So for example, if the reviewer decides that minor adjustments are required and the assignee pushes these changes to GitLab, then these commits will be captured in this tab. Changes tab, here you can see all the new changes in the merge request's source branch. You can also opt for a side-by-side comparison, and see line-by-line how the master version and new branch version compare. You can also leave comments on each file's line, which will also be recorded in the Overview tab's timeline. If you are happy with the new changes, then select Merge and the merge request should be completed in a few seconds. How to update your local master branch to the latest remote master version on GitLab/Github Open the Anaconda Prompt terminal or Git Bash or any other terminal you wish to use. This can be accessed by typing, for example Anaconda Prompt, in the search bar, after opening the search icon on your Windows toolbar. Type cd <paste your work directory address> in the terminal to switch to your working directory. This is where the Git repository is cloned/copied/downloaded to, on your local machine (or RDS environment or any environment you're using). This will cause the default terminal address to be directed at the new directory. (see Git Clone instructions above for more info) Type git status in the terminal to ensure that ii happened successfully and you are on the default master branch. You should see this message: Type git pull to update your local master branch with the latest version of the remote/online/GitLab master branch. This might take a few seconds. The .gitignore file .gitignore is a text file that contains file extensions and directories' paths that we wish git to ignore. For example, we have created a repository on GitLab that should never contain data. To ensure this, in the repository's .gitignore file will include .csv , .xlsx etc. Github has a .gitignore template available for analysts and developers to use for their own projects. Notice how in the template, lines 117-118 contain Spyder project settings, which are configuration files created by Spyder when you first open your repository as a Spyder project. Sometimes these configuration files can contain information you don't want to publish on Github/GitLab, thus by including them in the .gitignore file you are ensuring that these files will never leave your local machine.","title":"Intro to Git"},{"location":"training_resources/git/intro-to-git/#intro-to-git","text":"","title":"Intro to Git"},{"location":"training_resources/git/intro-to-git/#overview","text":"This page is intended as a starting point for someone who is new to using git. It shows you the basic commands and the workflow to use git on your own. To use git as a team you should complete this section and the following section on using Git collaboratively .","title":"Overview"},{"location":"training_resources/git/intro-to-git/#what-is-version-control","text":"Version control is the practice of tracking and managing any changes on project's code, files or folders over time. This allows you to observe a detailed history of the changes made and enables your team members to collaborate on the same project. In Git, each user has the entire repository (project's working directory) on their computer (offline), hence, they can work separately offline until they opt to push their updated version of the code to the remote (online) central repository. Remote repositories are versions of your project that are hosted on an online Version Control System. In NHS Digital, we store that in GitLab site for internal users (GitHub site for the public).","title":"What is version control?"},{"location":"training_resources/git/intro-to-git/#why-should-i-care","text":"Using version control is one of the fundamental skills needed by analysts to produce high-quality analytical outputs. Git is the standard for tracking code over time and is the way that NHS Digital has chosen. While git has a steep initial learning curve, the payoff is huge and so we strongly recommend taking the time to learn this as a team. The benefits of using version control include: Understanding what happened in the past. E.g., what did we change last time we ran this code Restoring previous versions Tracking changes - avoid accidentally breaking code The ability to review someone's changes and to leave comments The ability to plan development work more effectively through being able to assign small, discrete changes Avoid code being hidden away on someone's machine The ability to set up an approval process for changes The ability to make changes without breaking anything - through running automated tests The ability to try out experiments without the risk of breaking your main code","title":"Why should I care?"},{"location":"training_resources/git/intro-to-git/#glossary-of-terms","text":"Here's a list of commonly used Git terms, that will help you understand the technical jargon mentioned in the next sections: Git terminology","title":"Glossary of terms"},{"location":"training_resources/git/intro-to-git/#video-showing-git-workflow-and-commands","text":"This short video tutorial walks you through how to create a repository and use basic Git commands: Link to video tutorial We have also produced a short video that gives some more theoretical explanation of how version control works: Link to video","title":"Video showing Git workflow and commands"},{"location":"training_resources/git/intro-to-git/#common-basic-commands","text":"Below is a list of common commands for reference. We only list basic commands here. This is just to flag to you that these commands exist. You can google for more detail on any of them. Git commands cheat-sheet Create a new git repo locally: git init . The init command is short for \"initialise\", it's the command that will do all of the initial setup of a repository. The folder needs to actually exist before we can create a new repository with Git. Clone an existing Git Repository: git clone <url> . Happens only once, when you need to create a local copy of a GitLab repository. Check whether any files have changed in a repository: git status . It lists the files you've changed and those you still need to add or commit. It displays the state of the working directory and the staging area. It lets you see which changes have been staged, which haven't, and which files aren't being tracked by Git. Status output does not show you any information regarding the committed project history. Create and switch to a new branch: git checkout -b <your branch name> . Check out a branch: git checkout <your branch name. . Check out an existing new branch. Add file contents/stages changes from the working directory to the index: git add <filename> or git add . Commit staged files with a message: git commit -m \"<message>\" . Committing changes in this way captures a snapshot of the project's currently staged changes. Committed snapshots can be thought of as \u201csafe\u201d versions of a project Update your branch with the online/remote branch info: git pull . Pulls the latest changes from the repository, only affects the current active branch you\u2019re in. Check the history of commits: git log . Show unstaged changes between your index and working directory: git diff . Press Q to exit the diff log. Ignore files: .git-ignore file . This file specifies untracked files that git should ignore such as the sensitive information related to security or the data itself. Files already tracked by git are not affected. Display a list of all branches in the repository: git branch -a . Delete a local branch: git branch -D <your branch name> .","title":"Common basic commands"},{"location":"training_resources/git/intro-to-git/#undoing-changes-via-git-commands","text":"GitLab has a guide covering various cases on undoing any changes: undo possibilities in Git .","title":"Undoing changes via Git commands"},{"location":"training_resources/git/intro-to-git/#what-is-a-terminal","text":"A terminal acts like a mediator, takes text input (a command) and returns an output (a process, a result, an action) that depends on the command input's process. Command line terminals can be used to input commands that refer to file directories, list of directories and/or moving folder and files but also for version control (Git) and creating isolated environments (conda, venv etc). Anaconda prompt is very similar to other command line terminals (e.g. Windows Command Prompt), but allows you to use Anaconda commands, which do not work in other terminals on Windows. Different terminals use slightly different languages, and so you may find that if you're used to the terminal on a Mac, you need different commands when working on a Windows machine. Some terminals allow you to workaround this- such as Git Bash . Git Bash works on Windows, but emulates the most common terminal experience on a Mac or Linux, allowing to use the same commands.","title":"What is a terminal?"},{"location":"training_resources/git/intro-to-git/#useful-commands","text":"Anaconda Prompt takes most or all of the commands that Windows Command Prompt takes, but also includes Conda-specific commands: Command reference . For new starters and for the purposes of this guide, dir (list of files within the directory) and cd (change directory) are the major terminal commands required for Anaconda Prompt. Example of Windows vs Linux commands differences: list files - dir (Windows, incl. Anaconda Prompt) - ls (mac, linux) change directory - cd display current directory - pwd (mac, linux) - chpwd or %cd% (Windows) List of other command differences For more info on conda virtual environments see here .","title":"Useful commands"},{"location":"training_resources/git/intro-to-git/#setup-for-git-basics-exercise","text":"","title":"Setup for Git Basics exercise"},{"location":"training_resources/git/intro-to-git/#creating-your-access-token","text":"When cloning a Git repository from a web-hosted repository platform such as GitHub or GitLab for the first time, you will be prompted to input your credentials, which is usually your username and password. Where two-factor authentication is enabled, and for internal repository platforms, you will need to generate an access token which you use as your password. For NHS Digital employees using our internal GitLab, your username is your GitLab username and your password is your access token. In Gitlab, navigate to Preferences by clicking the top right profile picture: On the left hand side menu select Access token. You should see this page after clicking: Add a token name, select all boxes for the scope, leave the date blank (token will never expire) and select Create Access Token. The access token that will appear at the top of the page needs to saved in a text document (Notepad etc).","title":"Creating your access token"},{"location":"training_resources/git/intro-to-git/#troubleshooting-credentials","text":"You might see something like this at any point: remote: HTTP Basic: Access denied remote: You must use a personal access with 'read_repository or 'write_repository' scope for Git over HTTP. remote: You can generate one at https:// etc etc... remote: Authentication failed for 'repository address you're attempting to git clone' This can mean a number of things: either your access token has expired or you have entered the wrong access token for your password (or username even). If your access token has expired or about to expire, you can check by going to GitLab Profile -> Preferences -> Access Tokens then scroll all the way down and see if there\u2019s an Active Access Token and check the expiration date. These steps will help you fix your access to the repository: To fix this we need to create a new access token (don\u2019t assign a date this time, so it will never expire). In the command terminal type git pull -v (v stands for verification). This should prompt a new git credentials window. If you don't have an existing repository, and can't do step 2, then: In the terminal type git clone <git repo copied clone url> but with the new credentials included in the URL, so: C:\\My_documents>git clone https://<username>:<password>@<domain>/example.git So, in my case it would be: C:\\My_documents>git clone https://<username>:<MY_NEW_ACCESS_TOKEN>@<domain>/example.git","title":"Troubleshooting credentials"},{"location":"training_resources/git/intro-to-git/#intro-exercises","text":"Complete tasks 1 - 6: Accessing a git repository, creating a branch, adding a new file, uploading your changes to GitLab Getting started - tasks 1 and 2 will be relevant only when accessing a repository for the first time . If stuck at any point feel free to go through the video tutorials mentioned in the previous sections of this guide). If stuck on creating an access token watch this . Disclaimer: the above video will not load for external users.","title":"Intro Exercises"},{"location":"training_resources/git/intro-to-git/#1-creating-your-local-git-repository-folder","text":"i. Open a Command terminal. This can be any terminal, Anaconda Prompt or Git Bash etc. To access the terminal, click the Search option on the Windows toolbar and type \"Anaconda Prompt\" and open it. The file directory address that's displayed in the terminal will usually be the default top level C drive address or something similar. (base) C:\\Windows\\system32> ii. Create a folder in your local directory, this is where the GitLab (remote) repository will be copied/downloaded to. In the Anaconda Prompt terminal type cd <windows explorer repo folder address you just created> , cd stands for Change Directory, and what this means is that the default terminal address will be directed to the new directory, the new folder address. Any git commands applied in the terminal will apply to the current directory. (base) C:\\Windows\\system32>cd C:\\Users\\<username>\\Documents\\my_project_folder\\RAP (base) C:\\Users\\<username>\\Documents\\my_project_folder\\RAP>","title":"1. Creating your local git repository folder"},{"location":"training_resources/git/intro-to-git/#2-git-clone-copydownload-a-gitlab-repository","text":"i. The repository's clone URL will be on the GitLab repository's main page (see image below). [Git demo repo with practice exercises] ii. Type git clone <repository's clone url> the git-demo repository using your local terminal (Anaconda Prompt). Enter your credentials when prompted (GitLab username and access token for the password.) (base) C:\\Users\\<username>\\Documents\\my_project_folder\\RAP>git clone <paste the repo url address from the image above> iii. Type cd <windows explorer repo folder address> (open the newly downloaded git folder) in the Anaconda Prompt to assign the root level of the directory to the top level of the repository. This because the terminal is assigned to the folder containing the git repository, not the folder that IS the git repository. iv. Type git status in the terminal. You should see the this message:","title":"2. Git clone (copy/download a GitLab repository)"},{"location":"training_resources/git/intro-to-git/#3-create-and-switch-to-a-new-branch","text":"Notice how in the image above, the first line says \"On branch master\". In Git, \"master\" is a naming convention for a branch. After cloning (downloading) a project from a remote server, the resulting local repository has a single local branch: the so-called \"master\" branch. This means that \"master\" can be seen as a repository's \"default\" branch. The master should always be locked, to protect the main build of your working code and/or documentation. To add new code/documentation/make any edits to the master branch, you need to submit a merge request . \"Branch\" is another word for \"version\". Usually when developing a document or slides for a presentation, we could have numerous versions of the same slides, with small or major differences. By using different branches (or versions) of the same code, we can safely work and test without breaking the publication code, that resides in the default master branch of the repository. i. To create a new branch , head over to GitLab, on the repository's main page, same webpage we used to find the git clone repository's URL. On the left hand side menu select Repository -> Branches. ii. Once in the branches list page, select New Branch (big blue button) on the right hand side. iii. In the New Branch page, type your branch name and select the branch you want to copy from. On most cases it will be the master branch so GitLab has this option select by default. Once happy with your choices, select Create branch. iv. Back to the terminal (e.g. Anaconda Prompt etc), type git branch -a to view a list of all available branches existing in the repository. The branches that contain the remote/ path are the branches online, on GitLab. Have a look at the list. You will notice your new branch is not there yet. To update your local master branch with all the latest updates on GitLab, type git pull . Type git branch -a and you should see your branch available in the list now. v. To start working on your branch and select, type in the terminal git checkout <your branch name> . vi. Type git status and this way you can confirm that you are in a new branch. Git will display a message showing the current branch name and the current branch's information. Reminder: any changes you make while in the master branch won't be uploaded to GitLab, as the master branch is protected and locked, so make sure you are in your own working branch! You can confirm this with step vi. above.","title":"3. Create and switch to a new branch"},{"location":"training_resources/git/intro-to-git/#branch-naming-strategy","text":"Following an agreed naming convention will make it much easier to keep the team's work organised. We follow use this approach: <project_id>_<initials>_<ticket_number>_<brief-description> Here the project_id refers to our project ID in confluence (DS218). The initials are from my name. The ticket number refers to the jira ticket number. E.g.: DS218_CQ_167_Add-field-definitions Naming branches in this way helps to make it very clear what change should be happening in the branch. Sometimes code review will reveal that a branch that was intended to make one specific change in fact makes several changes. This should be avoided and the code review should reject the additional changes.","title":"Branch naming strategy"},{"location":"training_resources/git/intro-to-git/#4-add-a-new-file","text":"By adding a new file to the project, you can avoid conflicts as each analyst will create and upload their own file. There are many ways to add a new file to your branch. One way is to upload a file directly to your branch on GitLab. The easiest way is to simply create the file (any type of file) in your working directory, while in your branch. You can also copy a file from another directory. The file you create or copy can be any type of file.","title":"4. Add a new file"},{"location":"training_resources/git/intro-to-git/#for-vs-code-users","text":"To open VS Code from the terminal (this will save you a few extra clicks) simply type in the Command Prompt of your choice (e.g. Anaconda Prompt/Git Bash/Powershell): Once VS Code loads, you will notice the on the left hand side panel the folder structure of the cloned repository: You can continue using the Command Prompt or you can continue with using the VS Code built in terminal. To add a new file in your repository right click in the folder structure, in this case we want to add a file in the practice folder of the repository: Name the file, add a file format in the name, for example .py for a python file, .md for a markdown file etc. Type something in the new file! Save your changes within VS Code, either from the top toolbar File -> Save or by Ctrl + S .","title":"For VS Code users"},{"location":"training_resources/git/intro-to-git/#for-spyder-users","text":"For this exercise, create a file while in Spyder, with simply right-clicking on the repository's folder and then selecting the Create New File option: To view and edit the repository's folders and files in Spyder: Open Spyder On the top toolbar go Projects -> New Project, this will open a Create New Project dialogue. In the Create New Project dialogue, select existing directory (since we have already cloned the GitLab repository to your local machine and have a folder) and in Location select the Open Folder icon to select the existing folder repository: In the Select directory dialogue, select the folder that contains your repository's contents. In this example, I have a repository called \"demo\": You should be able to see the project structure on the left hand side menu as it is displayed on GitLab, in the image above. Right-click on the Practice folder and select New and New File. Name the file. Add something into that file. Save it on Spyder.","title":"For Spyder users"},{"location":"training_resources/git/intro-to-git/#5-commit-your-changes-follow-basic-git-command-workflow","text":"In the Anaconda Prompt (or any terminal of your choice) type the following. I suggest you read through the git messages displayed after each command entered in the terminal, to familiarise yourself with the logic. i. Type git status to see the modifications that haven't been staged yet (in red colour). ii. Type git add <filename> to stage your changes. Or git add . to simply stage all changes automatically (use with caution). iii. Type git status to see the file modifications now have a green colour. This means that git add was successful. iv. Type git commit -m \"your commit message here\" to commit these changes. v. Type git status to view the status of your repo. There should be no modifications visible (or colours!). vi. Type git push . This command will upload all your committed changes to GitLab.","title":"5. Commit your changes (follow basic git command workflow)"},{"location":"training_resources/git/intro-to-git/#6-check-the-repository-on-gitlab-to-view-the-changes-updated","text":"Simply head over to GitLab, and the repository page on GitLab, select your branch and compare the changes between your branch and the master branch.","title":"6. Check the repository on GitLab to view the changes updated"},{"location":"training_resources/git/intro-to-git/#how-to-submit-a-merge-request","text":"To submit a merge request: Simply head over to GitLab and to the respective repository Select Merge requests on the menu on the left hand side. On the right select New Merge Request (big blue button). Target branch will always (in most cases) be the master branch. Source branch is your working branch. Click Compare branches and Continue Fill out any information related to the Merge request, title, description, any comments. Assign yourself as the Assignee, assign someone in your team to be the Reviewer. Then, select Create Merge Request.","title":"How to submit a merge request"},{"location":"training_resources/git/intro-to-git/#mark-your-merge-request-as-a-draft-on-gitlab","text":"After submitting a merge/pull request, you can block the request from being merged if you opt for the \"Mark as draft option\": This will assign your request as a draft merge request, blocking it from being merged. Once the development work is completed then you can unassign the \"Mark as draft\" option and the merge request can be accepted.","title":"Mark your merge request as a draft on GitLab"},{"location":"training_resources/git/intro-to-git/#how-to-accept-a-merge-request-on-gitlab","text":"Once you are assigned the merge request's reviewer, you should receive an email notification from GitLab with a link to the merge request's webpage. You can also view any active merge requests by clicking on the left hand side menu, once in the repository's page. In the merge request page, there are three tabs: Overview, Commits and Changes. Overview is the main merge request page. It contains the merge request's description, timeline and comments left by the assignee or reviewer. The approve button does not impact the merge request itself. Not selecting the Approve button won't affect the status or progress of the merge request. GitLab has made this available for teams that are interested in 2-step review processes where one analyst submits a merge request, another approves the request and the reviewer confirms the merge. Deleting the Source branch: this will automatically delete the branch created for the merge request. Untick this option to keep the source branch. Squash commits: ticking this will merge the history of commits for the source branch into one commit. This can be useful when the merge request is not a major update and you don't want your repository's history of commits to contain commits with messages such as \"fixed typo\" \"replaced incorrectly placed image\". Commits tab displays a history of the merge request's commits. So for example, if the reviewer decides that minor adjustments are required and the assignee pushes these changes to GitLab, then these commits will be captured in this tab. Changes tab, here you can see all the new changes in the merge request's source branch. You can also opt for a side-by-side comparison, and see line-by-line how the master version and new branch version compare. You can also leave comments on each file's line, which will also be recorded in the Overview tab's timeline. If you are happy with the new changes, then select Merge and the merge request should be completed in a few seconds.","title":"How to accept a merge request on GitLab"},{"location":"training_resources/git/intro-to-git/#how-to-update-your-local-master-branch-to-the-latest-remote-master-version-on-gitlabgithub","text":"Open the Anaconda Prompt terminal or Git Bash or any other terminal you wish to use. This can be accessed by typing, for example Anaconda Prompt, in the search bar, after opening the search icon on your Windows toolbar. Type cd <paste your work directory address> in the terminal to switch to your working directory. This is where the Git repository is cloned/copied/downloaded to, on your local machine (or RDS environment or any environment you're using). This will cause the default terminal address to be directed at the new directory. (see Git Clone instructions above for more info) Type git status in the terminal to ensure that ii happened successfully and you are on the default master branch. You should see this message: Type git pull to update your local master branch with the latest version of the remote/online/GitLab master branch. This might take a few seconds.","title":"How to update your local master branch to the latest remote master version on GitLab/Github"},{"location":"training_resources/git/intro-to-git/#the-gitignore-file","text":".gitignore is a text file that contains file extensions and directories' paths that we wish git to ignore. For example, we have created a repository on GitLab that should never contain data. To ensure this, in the repository's .gitignore file will include .csv , .xlsx etc. Github has a .gitignore template available for analysts and developers to use for their own projects. Notice how in the template, lines 117-118 contain Spyder project settings, which are configuration files created by Spyder when you first open your repository as a Spyder project. Sometimes these configuration files can contain information you don't want to publish on Github/GitLab, thus by including them in the .gitignore file you are ensuring that these files will never leave your local machine.","title":"The .gitignore file"},{"location":"training_resources/git/using-git-collaboratively/","text":"Using Git collaboratively Working with git collaboratively In the video below, you can view the git workflow that's follows the changes made to a working branch, committing the changes and pushing them to the repository, on Gitlab. Then, a merge request is submitted to merge these changes to the master branch of the repository. The merge request is reviewed and approved, while the working branch is deleted after serving its purpose. Video tutorial: working collaborative with git Disclaimer: the above video will not load for external users. In GitLab , click on \"Branches\" in the \"Repository\" menu on the left hand side. Click the \"New branch\" blue button. Type a suitable name for your branch. Create from master. Click \"Create branch\" In the command prompt , navigate to your repository folder, using cd <folder_name> git pull git branch -a to see all the available branches, including the one you just created git checkout <branch name> In your editor Make your code changes Save the changes In the command prompt , git status to check which files you've altered git add. if you're happy to add all the changed files, or git add <file_name> if you just want one file. git commit -m \"comment on what you've changed\" git push In GitLab , click on \"Merge requests\" in the left hand menu. Click the \"New merge request\" blue button. Choose the branch you want to merge and the target branch (usually master) Write a description which will be helpful to the reviewer. Assign a reviewer. Tick \"Delete source branch when merge request is accepted\" Click the \"Create merge request\" blue button. Review code changes with the reviewer, and ask the reviewer to accept the merge request. In the command prompt , git checkout master , git pull to get the latest code. delete the branch locally, with git branch -D <branch_name> git branch -a to check that your branch has been deleted. git checkout master , then git pull , then open the code in Spyder to check that your changes are in the master. Resolving merge conflicts How to update your branch to the latest version of the master branch, managing conflicts, merge request conflicts and review: Video tutorial: handling and avoiding merge conflicts Disclaimer: the above video will not load for external users. How to resolve merge conflicts using git - if they occur Follow the same steps as above, up to the point of raising the merge request. Before raising a merge request, you should see if there are likely to be any conflicts between the changes you've made and changes another developer might have made to the same files. In the command prompt , navigate to your repository folder, using cd <folder_name> if you are not already there. git checkout master git pull to get the latest version of the master. git checkout <branch_name> if you do not already have it checked out. git merge master This will attempt to automatically merge the latest version of the master into your branch. If no-one else has changed the same lines of code as you, this will be fine and you can continue to raising a merge request as above. If not, you will get an error: Automatic merge failed , Merge conflict in <file name> and CONFLICT messages will appear on the terminal: Auto-merging practice/temperatures_function.py CONFLICT (content): Merge Conflict in practice/temperatures_function.py Automatic merge failed; fix conflicts and then commit the result. Different editors give us different tools for handling these merge conflicts, though the principles remain the same. Spyder In Spyder , open that file (in this case temperatures_function.py). Scroll in the document to find the conflict. You will notice these strange lines where conflict has occurred: <<<<<< HEAD This line is the line in my branch. ======= This line is the line in master. >>>>>> master The top bit above the double line is what we have in our branch (our branch is represented by the <<<<<<< HEAD statement). The bottom bit is the same line but it's saved in the master branch (symbolised as >>>>>>> master ). To resolve the conflict, delete the line which you do not wish to keep, including the arrows and double lines. So for example, if you wish to keep This line is the line in my branch delete everything that's connected to the conflict so that it also shows this in your text editor: This line is the line in my branch. VSCode In Visual Studio Code , you can open your file browser in the tools tab. Any files which have a merge conflict will have a little 'C' next to them here. Open the files which have these conflicts (in this case temperatures_function.py). Scroll in the document to find the conflict. You will notice these strange lines where conflict has occurred: <<<<<< HEAD This line is the line in my branch. ======= This line is the line in master. >>>>>> master The top bit above the double line is what we have in our branch (our branch is represented by the <<<<<<< HEAD statement). This will be called the current change . The bottom bit is the same line but it's saved in the master branch (symbolised as >>>>>>> master ). This will be called the incoming change . Above the conflict, you'll see some options for what you can do. You can click accept current changes , accept incoming changes , or accept both changes . Picking one of these options will resolve the conflict. Alternatively, you can go ahead and simply edit the conflicted file directly. Edit the area of conflict until you're satisfied -- making sure to remove the === line, the >>>>> line, and any duplicated code. So for example, if you wish to keep This line is the line in my branch delete everything that's connected to the conflict so that it also shows this in your text editor: When your're done, simply save the file. This line is the line in my branch. Once the Conflict is Resolved Save your changes, and then update your branch in the usual way: git status git add . git status git commit -m \"your message here\" git status git push In GitLab create a merge request and get it approved. Git branching Branch naming strategy Following an agreed naming convention will make it much easier to keep the team's work organised. We follow use this approach: <project_id>_<initials>_<ticket_number>_<brief-description> Here the project_id refers to our project ID in confluence (DS218). The initials are from my name. The ticket number refers to the jira ticket number. E.g.: DS218_CQ_167_Add-field-definitions Naming branches in this way helps to make it very clear what change should be happening in the branch. Sometimes code review will reveal that a branch that was intended to make one specific change in fact makes several changes. This should be avoided and the code review should reject the additional changes. General guidelines for branching and merge requests The master branch should be kept in a good stable state (always deployable with a relevant README file). Developers create feature branches from the master branch (main trunk) and work on them. The developer only creates a short-lived feature branch (a branch from the trunk) to prevent the challenge of merging later on. Once they are done, they create merge requests in GitLab. The request get reviews by a team member or a group of developers (if it is a refactor or large changes). The comment on changes stay directly on the GitLab page and there may have discussions. Once these are done with compiling and all testing, the merge will be incorporated into the trunk by the reviewer. Using git collaboratively exercise Team exercise This group exercise is designed for a team of four. Everyone, independently, do the following: In GitLab , create a new branch from the master branch. In the command prompt , checkout your new branch ( git branch -a , git checkout <branch_name> , git pull ) Make your code changes: Team member A : If result < 0, print(\"You'll freeze today\") Team member B : If result > 100, print(\"You'll boil today\") Team member C : Change the name of the variable \"result\" to \"temperature\" Team member D : Change the warning \"you'd better wear suncream\" to \"you'd better wear a \"t-shirt\" Commit your changes ( git status , git add. , git commit -m \"comment\" , git push ) In GitLab , Team member A create a new merge request. Assign a reviewer and accept the merge. Approving merge requests requires someone with the relevant assigned role and permissions. In the command prompt , Team member B checkout the master and attempt to merge it into your branch. ( git checkout master , git pull , git checkout <branch_name> , git merge master ) In Spyder or VSCode open the file where there are any conflicts and decide with team member B how the conflicts should be resolved. Save your changes In the command prompt , update your branch in the usual way. ( git status , git add. , git status , git commit -m \"comment\" , git status , git push ) In GitLab , Team member B create a new merge request. Assign a reviewer and accept the merge. Repeat previous 5 steps for Team member C and Team member D Top tips for git Branches should be short-lived Make clean, single-purpose commits Test Before You Commit Write meaningful commit messages Don\u2019t git push straight to master Commit early, commit often Don\u2019t alter published history Don\u2019t commit generated files","title":"Using Git collaboratively"},{"location":"training_resources/git/using-git-collaboratively/#using-git-collaboratively","text":"","title":"Using Git collaboratively"},{"location":"training_resources/git/using-git-collaboratively/#working-with-git-collaboratively","text":"In the video below, you can view the git workflow that's follows the changes made to a working branch, committing the changes and pushing them to the repository, on Gitlab. Then, a merge request is submitted to merge these changes to the master branch of the repository. The merge request is reviewed and approved, while the working branch is deleted after serving its purpose. Video tutorial: working collaborative with git Disclaimer: the above video will not load for external users. In GitLab , click on \"Branches\" in the \"Repository\" menu on the left hand side. Click the \"New branch\" blue button. Type a suitable name for your branch. Create from master. Click \"Create branch\" In the command prompt , navigate to your repository folder, using cd <folder_name> git pull git branch -a to see all the available branches, including the one you just created git checkout <branch name> In your editor Make your code changes Save the changes In the command prompt , git status to check which files you've altered git add. if you're happy to add all the changed files, or git add <file_name> if you just want one file. git commit -m \"comment on what you've changed\" git push In GitLab , click on \"Merge requests\" in the left hand menu. Click the \"New merge request\" blue button. Choose the branch you want to merge and the target branch (usually master) Write a description which will be helpful to the reviewer. Assign a reviewer. Tick \"Delete source branch when merge request is accepted\" Click the \"Create merge request\" blue button. Review code changes with the reviewer, and ask the reviewer to accept the merge request. In the command prompt , git checkout master , git pull to get the latest code. delete the branch locally, with git branch -D <branch_name> git branch -a to check that your branch has been deleted. git checkout master , then git pull , then open the code in Spyder to check that your changes are in the master.","title":"Working with git collaboratively"},{"location":"training_resources/git/using-git-collaboratively/#resolving-merge-conflicts","text":"How to update your branch to the latest version of the master branch, managing conflicts, merge request conflicts and review: Video tutorial: handling and avoiding merge conflicts Disclaimer: the above video will not load for external users.","title":"Resolving merge conflicts"},{"location":"training_resources/git/using-git-collaboratively/#how-to-resolve-merge-conflicts-using-git-if-they-occur","text":"Follow the same steps as above, up to the point of raising the merge request. Before raising a merge request, you should see if there are likely to be any conflicts between the changes you've made and changes another developer might have made to the same files. In the command prompt , navigate to your repository folder, using cd <folder_name> if you are not already there. git checkout master git pull to get the latest version of the master. git checkout <branch_name> if you do not already have it checked out. git merge master This will attempt to automatically merge the latest version of the master into your branch. If no-one else has changed the same lines of code as you, this will be fine and you can continue to raising a merge request as above. If not, you will get an error: Automatic merge failed , Merge conflict in <file name> and CONFLICT messages will appear on the terminal: Auto-merging practice/temperatures_function.py CONFLICT (content): Merge Conflict in practice/temperatures_function.py Automatic merge failed; fix conflicts and then commit the result. Different editors give us different tools for handling these merge conflicts, though the principles remain the same.","title":"How to resolve merge conflicts using git - if they occur"},{"location":"training_resources/git/using-git-collaboratively/#spyder","text":"In Spyder , open that file (in this case temperatures_function.py). Scroll in the document to find the conflict. You will notice these strange lines where conflict has occurred: <<<<<< HEAD This line is the line in my branch. ======= This line is the line in master. >>>>>> master The top bit above the double line is what we have in our branch (our branch is represented by the <<<<<<< HEAD statement). The bottom bit is the same line but it's saved in the master branch (symbolised as >>>>>>> master ). To resolve the conflict, delete the line which you do not wish to keep, including the arrows and double lines. So for example, if you wish to keep This line is the line in my branch delete everything that's connected to the conflict so that it also shows this in your text editor: This line is the line in my branch.","title":"Spyder"},{"location":"training_resources/git/using-git-collaboratively/#vscode","text":"In Visual Studio Code , you can open your file browser in the tools tab. Any files which have a merge conflict will have a little 'C' next to them here. Open the files which have these conflicts (in this case temperatures_function.py). Scroll in the document to find the conflict. You will notice these strange lines where conflict has occurred: <<<<<< HEAD This line is the line in my branch. ======= This line is the line in master. >>>>>> master The top bit above the double line is what we have in our branch (our branch is represented by the <<<<<<< HEAD statement). This will be called the current change . The bottom bit is the same line but it's saved in the master branch (symbolised as >>>>>>> master ). This will be called the incoming change . Above the conflict, you'll see some options for what you can do. You can click accept current changes , accept incoming changes , or accept both changes . Picking one of these options will resolve the conflict. Alternatively, you can go ahead and simply edit the conflicted file directly. Edit the area of conflict until you're satisfied -- making sure to remove the === line, the >>>>> line, and any duplicated code. So for example, if you wish to keep This line is the line in my branch delete everything that's connected to the conflict so that it also shows this in your text editor: When your're done, simply save the file. This line is the line in my branch.","title":"VSCode"},{"location":"training_resources/git/using-git-collaboratively/#once-the-conflict-is-resolved","text":"Save your changes, and then update your branch in the usual way: git status git add . git status git commit -m \"your message here\" git status git push In GitLab create a merge request and get it approved.","title":"Once the Conflict is Resolved"},{"location":"training_resources/git/using-git-collaboratively/#git-branching","text":"","title":"Git branching"},{"location":"training_resources/git/using-git-collaboratively/#branch-naming-strategy","text":"Following an agreed naming convention will make it much easier to keep the team's work organised. We follow use this approach: <project_id>_<initials>_<ticket_number>_<brief-description> Here the project_id refers to our project ID in confluence (DS218). The initials are from my name. The ticket number refers to the jira ticket number. E.g.: DS218_CQ_167_Add-field-definitions Naming branches in this way helps to make it very clear what change should be happening in the branch. Sometimes code review will reveal that a branch that was intended to make one specific change in fact makes several changes. This should be avoided and the code review should reject the additional changes.","title":"Branch naming strategy"},{"location":"training_resources/git/using-git-collaboratively/#general-guidelines-for-branching-and-merge-requests","text":"The master branch should be kept in a good stable state (always deployable with a relevant README file). Developers create feature branches from the master branch (main trunk) and work on them. The developer only creates a short-lived feature branch (a branch from the trunk) to prevent the challenge of merging later on. Once they are done, they create merge requests in GitLab. The request get reviews by a team member or a group of developers (if it is a refactor or large changes). The comment on changes stay directly on the GitLab page and there may have discussions. Once these are done with compiling and all testing, the merge will be incorporated into the trunk by the reviewer.","title":"General guidelines for branching and merge requests"},{"location":"training_resources/git/using-git-collaboratively/#using-git-collaboratively-exercise","text":"","title":"Using git collaboratively exercise"},{"location":"training_resources/git/using-git-collaboratively/#team-exercise","text":"This group exercise is designed for a team of four. Everyone, independently, do the following: In GitLab , create a new branch from the master branch. In the command prompt , checkout your new branch ( git branch -a , git checkout <branch_name> , git pull ) Make your code changes: Team member A : If result < 0, print(\"You'll freeze today\") Team member B : If result > 100, print(\"You'll boil today\") Team member C : Change the name of the variable \"result\" to \"temperature\" Team member D : Change the warning \"you'd better wear suncream\" to \"you'd better wear a \"t-shirt\" Commit your changes ( git status , git add. , git commit -m \"comment\" , git push ) In GitLab , Team member A create a new merge request. Assign a reviewer and accept the merge. Approving merge requests requires someone with the relevant assigned role and permissions. In the command prompt , Team member B checkout the master and attempt to merge it into your branch. ( git checkout master , git pull , git checkout <branch_name> , git merge master ) In Spyder or VSCode open the file where there are any conflicts and decide with team member B how the conflicts should be resolved. Save your changes In the command prompt , update your branch in the usual way. ( git status , git add. , git status , git commit -m \"comment\" , git status , git push ) In GitLab , Team member B create a new merge request. Assign a reviewer and accept the merge. Repeat previous 5 steps for Team member C and Team member D","title":"Team exercise"},{"location":"training_resources/git/using-git-collaboratively/#top-tips-for-git","text":"Branches should be short-lived Make clean, single-purpose commits Test Before You Commit Write meaningful commit messages Don\u2019t git push straight to master Commit early, commit often Don\u2019t alter published history Don\u2019t commit generated files","title":"Top tips for git"},{"location":"training_resources/pyspark/","text":"PySpark We have collated some information on styling in PySpark, logging and error handling , PySpark benefits as well as a tutorial . We have also gathered some information on unit testing , unit testing field definitions and Python function - these are also applicable for PySpark. PySpark Benefits PySpark is a flavour of Python that enables us to make use of the distributed processing available in NHS Digital. This enables analysts to run queries against datasets that are far too big to fit in computer memory. We recommend that analytical teams in NHS Digital should in general use PySpark for their code. There are a number of reasons for this: In NHS Digital we have some very large datasets. Given our tech stack, the best way for us to process these datasets is to use PySpark. Other options risk running out of memory or disrupting the work of other teams by using compute resource inefficiently. If you know that you will need to use PySpark sometimes then it is easier to just use it from the outset instead of trying to adapt Python or R when you run out of memory. PySpark has critical mass in the NHS Digital data engineering community and so there is depth of technical knowledge. PySpark is much easier to learn for people coming from SQL. All the same keywords are used: select, where, group by, etc. This is in sharp contrast to e.g., Pandas which has an extremely steep learning curve for new starters. Choosing one language to support enables us to provide better training and support. Aligning around one language as much as possible means that it is easier for teams to mutually support each other. Note: we strongly believe that teams should have the option to use whatever tool they deem right for their situation. We focus our efforts on supporting PySpark but do not want to prevent teams from choosing another course","title":"About PySpark"},{"location":"training_resources/pyspark/#pyspark","text":"We have collated some information on styling in PySpark, logging and error handling , PySpark benefits as well as a tutorial . We have also gathered some information on unit testing , unit testing field definitions and Python function - these are also applicable for PySpark.","title":"PySpark"},{"location":"training_resources/pyspark/#pyspark-benefits","text":"PySpark is a flavour of Python that enables us to make use of the distributed processing available in NHS Digital. This enables analysts to run queries against datasets that are far too big to fit in computer memory. We recommend that analytical teams in NHS Digital should in general use PySpark for their code. There are a number of reasons for this: In NHS Digital we have some very large datasets. Given our tech stack, the best way for us to process these datasets is to use PySpark. Other options risk running out of memory or disrupting the work of other teams by using compute resource inefficiently. If you know that you will need to use PySpark sometimes then it is easier to just use it from the outset instead of trying to adapt Python or R when you run out of memory. PySpark has critical mass in the NHS Digital data engineering community and so there is depth of technical knowledge. PySpark is much easier to learn for people coming from SQL. All the same keywords are used: select, where, group by, etc. This is in sharp contrast to e.g., Pandas which has an extremely steep learning curve for new starters. Choosing one language to support enables us to provide better training and support. Aligning around one language as much as possible means that it is easier for teams to mutually support each other. Note: we strongly believe that teams should have the option to use whatever tool they deem right for their situation. We focus our efforts on supporting PySpark but do not want to prevent teams from choosing another course","title":"PySpark Benefits"},{"location":"training_resources/pyspark/logging-and-error-handling/","text":"Logging and error handling See our documentation on logging and error handling to find out more. Error messages in PySpark can be especially unhelpful - often returning hundreds of lines of messages for a simple error. We've found it easier to log only the start of these error messages. This snippet of code shows how: try : df . agg ( * aggregate_fields ) except pyspark . sql . utils . AnalysisException as e : logger . error ( f \"This is a PySpark error. Please check your variable: { str ( e ) . split ( ';' )[ 0 ] } \" ) sys . exit ( 1 ) Just as before we are using the control error handling gives us to log a useful error message. However, in this case the exception encountered is a custom PySpark exception rather than an inbuilt python exception. PySpark exceptions produce a different stack trace which is long and sometimes difficult to read. So not only do we use error handling here to log our error message, we also take this opportunity to log a more informative readable message as well.","title":"Logging and error handling in PySpark"},{"location":"training_resources/pyspark/logging-and-error-handling/#logging-and-error-handling","text":"See our documentation on logging and error handling to find out more. Error messages in PySpark can be especially unhelpful - often returning hundreds of lines of messages for a simple error. We've found it easier to log only the start of these error messages. This snippet of code shows how: try : df . agg ( * aggregate_fields ) except pyspark . sql . utils . AnalysisException as e : logger . error ( f \"This is a PySpark error. Please check your variable: { str ( e ) . split ( ';' )[ 0 ] } \" ) sys . exit ( 1 ) Just as before we are using the control error handling gives us to log a useful error message. However, in this case the exception encountered is a custom PySpark exception rather than an inbuilt python exception. PySpark exceptions produce a different stack trace which is long and sometimes difficult to read. So not only do we use error handling here to log our error message, we also take this opportunity to log a more informative readable message as well.","title":"Logging and error handling"},{"location":"training_resources/pyspark/pyspark-style-guide/","text":"PySpark style guide Introduction We tend to use PySpark interleaved with python. That means we follow standard python organisation of modules, functions, tests, etc., but where we do data manipulation we drop to PySpark. PySpark gives you many ways to accomplish operations. We do not lay out all of the different options here because it would be confusing. Instead, these examples give you the single approach that we have chosen to use. it is perfectly valid for you to choose a different way. We avoid using pandas or koalas because it adds another layer of learning. The PySpark method chaining syntax is easy to learn, easy to read, and will be familiar for anyone who has used SQL. There does not yet seem to be a style convention for PySpark so we have adapted this style guide . You can also find a tutorial script containing lots of code snippets here. In-built PySpark functions Using functions is preferred to writing spark SQL to support testing and encapsulation. This preference is not a rule. PySpark makes a lot of use of functions imported from spark.sql.functions library. Almost any function that you might find in SQL should be available in this library. The docs contain the full list of functions . Our convention is to import these all under F: from pyspark.sql import functions as F Then you would use the functions like: F . col () F . max () F . datediff () Reading a table To get started we will usually want to read a table from a database into a DataFrame. df = spark . table ( 'my_database.my_table' ) Select The Spark equivalent of SQL's SELECT is the .select() method. This method takes multiple arguments - one for each column you want to select. These arguments can either be the column name as a string (one for each column) or a column object (using the df.colName syntax). If you are using Spark version before 3.0 then do this: df = ( df . select ( F . col ( \u2018 AGE \u2019 ), F . col ( \u2018 CHOLESTEROL \u2019 )) ) Notice the indentation of the parentheses is different from python. This is following the palantir style guide listed above. This convention become more valuable when we see longer examples. Casting data types Spark differs from SQL in that it is 'schema-on-read'. This means that when you import data into PySpark, it will try to guess the datatypes of your columns unless you explicitly provide a schema. In general, it is good practice to set the data types as soon as you read in data. In our approach, we use the initial .select() statement to (1) rename columns to follow our naming convention and (2) cast data types where necessary. This ensures everything is tidy before we start complex operations. my_df = ( my_df . select ( F . col ( \"NHS_NUMBER\" ) . cast ( \"String\" ) . alias ( \"NHS_NUMBER\" ), F . col ( \"BirthDate\" ) . cast ( \"Date\" ) . alias ( \"BIRTH_DATE\" ), F . col ( \"Appointments_count\" ) . cast ( \"Integer\" ) . alias ( \"APPOINTMENTS_COUNT\" ) ) ) Filter The .filter() method is the Spark counterpart of SQL's WHERE clause.: my_df = ( my_df . filter ( F . col ( \u2018 MY_VALUE \u2019 ) >= 4 ) ) Renaming columns Changing a column name is simple: df = ( df . withColumnRenamed ( \u2018 old name \u2019 , \u2018 NEW_NAME \u2019 ) ) For example: diabetes_data = ( diabetes_data . withColumnRenamed ( \u2018 Birth_date \u2019 , \u2018 BIRTH_DATE \u2019 ) ) Aggregations The syntax for aggregation is very similar to SQL, but with a different order: df = ( df . groupby ( F . col ( \u2018 YEAR \u2019 )) . agg ( F . sum ( F . col ( \u2018 SALES \u2019 )) . alias ( \u2018 TOTAL_SALES \u2019 ), F . min ( F . col ( 'SALES' )) . alias ( 'MIN_SALES' ), F . max ( F . col ( 'SALES' )) . alias ( 'MAX_SALES' ) ) ) Note how alias() is used to rename the aggregations. Method chaining Method chaining is one of the nicest features of PySpark as it allows you to do one operation after another in a very readable manner. You have already seen lots of examples of method chaining in this guide but here we address it directly. Here is an example that contains many of the operations you might want to perform on a dataset: df2 = ( df1 . select ( F . col ( \u2018 FIELD_ONE \u2019 ), F . col ( \u2018 FIELD_TWO \u2019 )) . filter ( F . col ( 'FIELD_TWO' ) > 10 ) . agg () ) Notice that we have put parentheses '()' around the entire query and we put the '.' at the beginning of each new line This approach contrasts to using the other common approach of having backslashes in the code. E.g. (DON\u2019T DO THIS) df2 = df1 \\ . select ( \u2018 FIELD_ONE \u2019 , \u2018 FIELD_TWO \u2019 ) \\ . filter ( \u2018 FIELD_TWO > 10 ) \\ . agg () It is recommended the chain lines to be no more than 6-7 lines. Try to avoid long chains and opt to break the code into logical steps. Joins in PySpark You should always explicitly specify the type of join in PySpark. If you do not specify the type of join in PySpark then it will default to inner join. Explicit is better than implicit. Even if the aim is to do an inner join, always specify the \u2018how\u2019 part of the join: Here is an example of good practice where we specify a left join: df_3 = ( df_1 . join ( df_2 , on = \u2019 NHS_NUMBER \u2019 , how = \u2019 left \u2019 ) . select ( df_1 [ * ], df_2 [ 'FIELD_1' , 'FIELD_2' ]) ) Notice above how the joining field \u2018on\u2019 condition is also specified. In situations where you need to join on multiple fields do this: df_3 = ( df_1 . join ( df_2 , ( df_1 . NHS_NUMBER == df_2 . NHS_NUMBER ) & ( df_1 . CCG == df_2 . CCG ), how = \u2019 left \u2019 ) . select ( df_1 [ * ], df_2 [ 'FIELD_1' , 'FIELD_2' ]) ) In the example above, the single joining field \u2018on\u2019 condition has been replaced with the multiple joining field conditions. Add a new column: .withColumn() To simply add a new column to your dataset: df = df . withColumn ( \"MyNewColumnName\" , < insert your conditions here > ) Suppose you want to add a new flag to your data (Yes = 1, No = 0), when a patient has had a chest x-ray or not: df = ( df . withColumn ( \"X-RAY_FLAG\" , F . when ( F . col ( \"X-RAY_DATE\" ) . isnotNull (), 1 ) . otherwise ( 0 )) ) This will result to an extra column in the dataset that contains the flag values we just assigned with the (case) F.when condition. Adding values to new columns If you want to create an empty column never use \u2018NA\u2019 or a blank string \u2018 \u2018 to fill in the rows of said column. Instead use None: df = df . withColumn ( \u2018 MyEmptyColumnName \u2019 , F . lit ( None )) Or, here's an example where two new columns are created with the constant values of 1 and 2 in each column respectively. df = ( df . withColumn ( \u2018 MyColumnName1 \u2019 , F . lit ( 1 )) . withColumn ( 'MyColumnName2' , F . lit ( 2 )) ) PySpark inside a python function We can create functions like in python, same logic applies, but instead using PySpark style: def group_by_and_count_column ( data : df , column_name : str ): \u201c\u201d Groups by specified column and returns count per grouping and sorts by descending order . Args : dataset we are reading from & the column we wish to group by Returns : groups from column and count \u201c\u201d\u201d # Group by CCG Code and count number of records per CCG df_count = df . groupBy ( df . column_name ) . count () # sort by CCG Code descending order result = df_count . sort ( desc ( \"count\" )) return result Reordering columns using a list To reorder columns in a table, create a list of the columns in the order you wish them to be: columns_order = [ \"NHS_NUMBER\" , \"CCG\" , \"APPOINTMENT\" , \"APPOINTMENT_DATE\" , etc ] And then apply to the dataframe: new_df = ( old_df . select ( * columns_order ) ) The asterisk is required to unpack the list in the select statement. Operations across multiple columns using a list Suppose you have flag columns in your dataset, whether a patient has attended an appointment or not, or a patient has completed a course of treatment. You can sum each of the flags on a record level by using a list and by adding an extra column using .withColumn() to the dataset with the sum result for each record: flags_sum = [ \"APP_ATTENDED\" , \"TREATMENT_COMPLETE\" ] After creating the list of flags we can now proceed to the final step: new_df = ( old_df . withColumn ( \"PATIENT_SCORE\" , sum ( old_df [ item ] for item in flags_sum )) ) Now you have a new column called PATIENT_SCORE that contains the sum of the specified flags for each Patient/row of data. Passing in column definitions to .withColumn() To add derivations and fields definitions to the final processing table, simply follow the .withColumn() logic as before: final_table = ( previous_step_df . withColumn ( \"SMOKING_NUMERATOR\" , smoking_numerator ) . withColumn ( \"TOTAL_NUM_PATIENTS\" , sum_of_patients ) . etc ) The 2nd argument of .withColumn() is a variable created at a separate step in the workflow, for example: smoking_numerator = ( F . when (( F . col ( \"SMOKING_VALUE\" ) >= 5 ) & ( F . col ( \"AGE\" ) >= 18 ), 1 ) . otherwise ( 0 ) ) The above field definition is an application of a SQL-like CASE WHEN in PySpark. Field definitions can be complex, with a lot of condition statements and potential dependencies. To learn more about structuring your code and creating field definitions: To learn more about field definitions and how to use them Using these field definitions to write test cases with customers Once you've written your field definitions, next step is to write the unit tests to ensure that the definitions work as intended and are future-proof to bugs and errors: Writing unit tests for field definitions Passing in a list of aggregations to apply To pass a list of aggregations, as in the previous examples, create a variable that contains the specified aggregations: aggregate_fields = ( F . sum ( \"REGISTRATIONS\" ) . alias ( \"TOTAL_REGISTRATIONS\" ), F . count ( \"NHS_NUMBER\" ) . alias ( \"ALL_PATIENTS\" ), F . round ( F . sum ( \"REGISTRATIONS\" ) * 100 / F . count ( \"NHS_NUMBER\" ), 2 ) . alias ( \"REGISTRATIONS_PERCENTAGE\" ), ) Then apply to the final table to produce the aggregated output: aggregated_output = ( final_table . agg ( * aggregate_fields ) .< apply other conditions here , i . e . adding new columns or sorting the column order > ) Print the output aggregated_output.show() Create your own schema If you wish to create your own custom schema, here's how to do it: schema = StructType ([ StructField ( \"id\" , IntegerType (), True ), StructField ( \"colour\" , StringType (), True ), StructField ( \"rate\" , FloatType (), True ), StructField ( \"date\" , DateType (), True ) ]) Performance - caching and lazy operations Lazy evaluation in Spark means that the execution will not start until an action is triggered. When we call for an operation to execute, it does not execute immediately. Spark maintains the record of which operation is being called. General tips and tricks Try to keep your code in logical structured blocks. For example, if you have multiple lines referencing the same things, try to keep them together. Separating them reduces context and readability. Try to be as explicit and descriptive as possible when naming functions or variables. Strive to capture what the function is actually doing as opposed to naming it based the objects used inside of it. Think twice about introducing new import aliases, unless there is a good reason to do so. Some of the established ones are types and functions from PySpark from pyspark.sql import types as T, functions as F. Avoid using literal strings or integers in filtering conditions, new values of columns etc. Instead, to capture their meaning, extract them into variables, constants, dicts or classes as suitable. This makes the code more readable and enforces consistency across the repository. External links GSS introduction to PySpark Palantir PySpark style guide","title":"PySpark style guide"},{"location":"training_resources/pyspark/pyspark-style-guide/#pyspark-style-guide","text":"","title":"PySpark style guide"},{"location":"training_resources/pyspark/pyspark-style-guide/#introduction","text":"We tend to use PySpark interleaved with python. That means we follow standard python organisation of modules, functions, tests, etc., but where we do data manipulation we drop to PySpark. PySpark gives you many ways to accomplish operations. We do not lay out all of the different options here because it would be confusing. Instead, these examples give you the single approach that we have chosen to use. it is perfectly valid for you to choose a different way. We avoid using pandas or koalas because it adds another layer of learning. The PySpark method chaining syntax is easy to learn, easy to read, and will be familiar for anyone who has used SQL. There does not yet seem to be a style convention for PySpark so we have adapted this style guide . You can also find a tutorial script containing lots of code snippets here.","title":"Introduction"},{"location":"training_resources/pyspark/pyspark-style-guide/#in-built-pyspark-functions","text":"Using functions is preferred to writing spark SQL to support testing and encapsulation. This preference is not a rule. PySpark makes a lot of use of functions imported from spark.sql.functions library. Almost any function that you might find in SQL should be available in this library. The docs contain the full list of functions . Our convention is to import these all under F: from pyspark.sql import functions as F Then you would use the functions like: F . col () F . max () F . datediff ()","title":"In-built PySpark functions"},{"location":"training_resources/pyspark/pyspark-style-guide/#reading-a-table","text":"To get started we will usually want to read a table from a database into a DataFrame. df = spark . table ( 'my_database.my_table' )","title":"Reading a table"},{"location":"training_resources/pyspark/pyspark-style-guide/#select","text":"The Spark equivalent of SQL's SELECT is the .select() method. This method takes multiple arguments - one for each column you want to select. These arguments can either be the column name as a string (one for each column) or a column object (using the df.colName syntax). If you are using Spark version before 3.0 then do this: df = ( df . select ( F . col ( \u2018 AGE \u2019 ), F . col ( \u2018 CHOLESTEROL \u2019 )) ) Notice the indentation of the parentheses is different from python. This is following the palantir style guide listed above. This convention become more valuable when we see longer examples.","title":"Select"},{"location":"training_resources/pyspark/pyspark-style-guide/#casting-data-types","text":"Spark differs from SQL in that it is 'schema-on-read'. This means that when you import data into PySpark, it will try to guess the datatypes of your columns unless you explicitly provide a schema. In general, it is good practice to set the data types as soon as you read in data. In our approach, we use the initial .select() statement to (1) rename columns to follow our naming convention and (2) cast data types where necessary. This ensures everything is tidy before we start complex operations. my_df = ( my_df . select ( F . col ( \"NHS_NUMBER\" ) . cast ( \"String\" ) . alias ( \"NHS_NUMBER\" ), F . col ( \"BirthDate\" ) . cast ( \"Date\" ) . alias ( \"BIRTH_DATE\" ), F . col ( \"Appointments_count\" ) . cast ( \"Integer\" ) . alias ( \"APPOINTMENTS_COUNT\" ) ) )","title":"Casting data types"},{"location":"training_resources/pyspark/pyspark-style-guide/#filter","text":"The .filter() method is the Spark counterpart of SQL's WHERE clause.: my_df = ( my_df . filter ( F . col ( \u2018 MY_VALUE \u2019 ) >= 4 ) )","title":"Filter"},{"location":"training_resources/pyspark/pyspark-style-guide/#renaming-columns","text":"Changing a column name is simple: df = ( df . withColumnRenamed ( \u2018 old name \u2019 , \u2018 NEW_NAME \u2019 ) ) For example: diabetes_data = ( diabetes_data . withColumnRenamed ( \u2018 Birth_date \u2019 , \u2018 BIRTH_DATE \u2019 ) )","title":"Renaming columns"},{"location":"training_resources/pyspark/pyspark-style-guide/#aggregations","text":"The syntax for aggregation is very similar to SQL, but with a different order: df = ( df . groupby ( F . col ( \u2018 YEAR \u2019 )) . agg ( F . sum ( F . col ( \u2018 SALES \u2019 )) . alias ( \u2018 TOTAL_SALES \u2019 ), F . min ( F . col ( 'SALES' )) . alias ( 'MIN_SALES' ), F . max ( F . col ( 'SALES' )) . alias ( 'MAX_SALES' ) ) ) Note how alias() is used to rename the aggregations.","title":"Aggregations"},{"location":"training_resources/pyspark/pyspark-style-guide/#method-chaining","text":"Method chaining is one of the nicest features of PySpark as it allows you to do one operation after another in a very readable manner. You have already seen lots of examples of method chaining in this guide but here we address it directly. Here is an example that contains many of the operations you might want to perform on a dataset: df2 = ( df1 . select ( F . col ( \u2018 FIELD_ONE \u2019 ), F . col ( \u2018 FIELD_TWO \u2019 )) . filter ( F . col ( 'FIELD_TWO' ) > 10 ) . agg () ) Notice that we have put parentheses '()' around the entire query and we put the '.' at the beginning of each new line This approach contrasts to using the other common approach of having backslashes in the code. E.g. (DON\u2019T DO THIS) df2 = df1 \\ . select ( \u2018 FIELD_ONE \u2019 , \u2018 FIELD_TWO \u2019 ) \\ . filter ( \u2018 FIELD_TWO > 10 ) \\ . agg () It is recommended the chain lines to be no more than 6-7 lines. Try to avoid long chains and opt to break the code into logical steps.","title":"Method chaining"},{"location":"training_resources/pyspark/pyspark-style-guide/#joins-in-pyspark","text":"You should always explicitly specify the type of join in PySpark. If you do not specify the type of join in PySpark then it will default to inner join. Explicit is better than implicit. Even if the aim is to do an inner join, always specify the \u2018how\u2019 part of the join: Here is an example of good practice where we specify a left join: df_3 = ( df_1 . join ( df_2 , on = \u2019 NHS_NUMBER \u2019 , how = \u2019 left \u2019 ) . select ( df_1 [ * ], df_2 [ 'FIELD_1' , 'FIELD_2' ]) ) Notice above how the joining field \u2018on\u2019 condition is also specified. In situations where you need to join on multiple fields do this: df_3 = ( df_1 . join ( df_2 , ( df_1 . NHS_NUMBER == df_2 . NHS_NUMBER ) & ( df_1 . CCG == df_2 . CCG ), how = \u2019 left \u2019 ) . select ( df_1 [ * ], df_2 [ 'FIELD_1' , 'FIELD_2' ]) ) In the example above, the single joining field \u2018on\u2019 condition has been replaced with the multiple joining field conditions.","title":"Joins in PySpark"},{"location":"training_resources/pyspark/pyspark-style-guide/#add-a-new-column-withcolumn","text":"To simply add a new column to your dataset: df = df . withColumn ( \"MyNewColumnName\" , < insert your conditions here > ) Suppose you want to add a new flag to your data (Yes = 1, No = 0), when a patient has had a chest x-ray or not: df = ( df . withColumn ( \"X-RAY_FLAG\" , F . when ( F . col ( \"X-RAY_DATE\" ) . isnotNull (), 1 ) . otherwise ( 0 )) ) This will result to an extra column in the dataset that contains the flag values we just assigned with the (case) F.when condition.","title":"Add a new column: .withColumn()"},{"location":"training_resources/pyspark/pyspark-style-guide/#adding-values-to-new-columns","text":"If you want to create an empty column never use \u2018NA\u2019 or a blank string \u2018 \u2018 to fill in the rows of said column. Instead use None: df = df . withColumn ( \u2018 MyEmptyColumnName \u2019 , F . lit ( None )) Or, here's an example where two new columns are created with the constant values of 1 and 2 in each column respectively. df = ( df . withColumn ( \u2018 MyColumnName1 \u2019 , F . lit ( 1 )) . withColumn ( 'MyColumnName2' , F . lit ( 2 )) )","title":"Adding values to new columns"},{"location":"training_resources/pyspark/pyspark-style-guide/#pyspark-inside-a-python-function","text":"We can create functions like in python, same logic applies, but instead using PySpark style: def group_by_and_count_column ( data : df , column_name : str ): \u201c\u201d Groups by specified column and returns count per grouping and sorts by descending order . Args : dataset we are reading from & the column we wish to group by Returns : groups from column and count \u201c\u201d\u201d # Group by CCG Code and count number of records per CCG df_count = df . groupBy ( df . column_name ) . count () # sort by CCG Code descending order result = df_count . sort ( desc ( \"count\" )) return result","title":"PySpark inside a python function"},{"location":"training_resources/pyspark/pyspark-style-guide/#reordering-columns-using-a-list","text":"To reorder columns in a table, create a list of the columns in the order you wish them to be: columns_order = [ \"NHS_NUMBER\" , \"CCG\" , \"APPOINTMENT\" , \"APPOINTMENT_DATE\" , etc ] And then apply to the dataframe: new_df = ( old_df . select ( * columns_order ) ) The asterisk is required to unpack the list in the select statement.","title":"Reordering columns using a list"},{"location":"training_resources/pyspark/pyspark-style-guide/#operations-across-multiple-columns-using-a-list","text":"Suppose you have flag columns in your dataset, whether a patient has attended an appointment or not, or a patient has completed a course of treatment. You can sum each of the flags on a record level by using a list and by adding an extra column using .withColumn() to the dataset with the sum result for each record: flags_sum = [ \"APP_ATTENDED\" , \"TREATMENT_COMPLETE\" ] After creating the list of flags we can now proceed to the final step: new_df = ( old_df . withColumn ( \"PATIENT_SCORE\" , sum ( old_df [ item ] for item in flags_sum )) ) Now you have a new column called PATIENT_SCORE that contains the sum of the specified flags for each Patient/row of data.","title":"Operations across multiple columns using a list"},{"location":"training_resources/pyspark/pyspark-style-guide/#passing-in-column-definitions-to-withcolumn","text":"To add derivations and fields definitions to the final processing table, simply follow the .withColumn() logic as before: final_table = ( previous_step_df . withColumn ( \"SMOKING_NUMERATOR\" , smoking_numerator ) . withColumn ( \"TOTAL_NUM_PATIENTS\" , sum_of_patients ) . etc ) The 2nd argument of .withColumn() is a variable created at a separate step in the workflow, for example: smoking_numerator = ( F . when (( F . col ( \"SMOKING_VALUE\" ) >= 5 ) & ( F . col ( \"AGE\" ) >= 18 ), 1 ) . otherwise ( 0 ) ) The above field definition is an application of a SQL-like CASE WHEN in PySpark. Field definitions can be complex, with a lot of condition statements and potential dependencies. To learn more about structuring your code and creating field definitions: To learn more about field definitions and how to use them","title":"Passing in column definitions to .withColumn()"},{"location":"training_resources/pyspark/pyspark-style-guide/#using-these-field-definitions-to-write-test-cases-with-customers","text":"Once you've written your field definitions, next step is to write the unit tests to ensure that the definitions work as intended and are future-proof to bugs and errors: Writing unit tests for field definitions","title":"Using these field definitions to write test cases with customers"},{"location":"training_resources/pyspark/pyspark-style-guide/#passing-in-a-list-of-aggregations-to-apply","text":"To pass a list of aggregations, as in the previous examples, create a variable that contains the specified aggregations: aggregate_fields = ( F . sum ( \"REGISTRATIONS\" ) . alias ( \"TOTAL_REGISTRATIONS\" ), F . count ( \"NHS_NUMBER\" ) . alias ( \"ALL_PATIENTS\" ), F . round ( F . sum ( \"REGISTRATIONS\" ) * 100 / F . count ( \"NHS_NUMBER\" ), 2 ) . alias ( \"REGISTRATIONS_PERCENTAGE\" ), ) Then apply to the final table to produce the aggregated output: aggregated_output = ( final_table . agg ( * aggregate_fields ) .< apply other conditions here , i . e . adding new columns or sorting the column order > ) Print the output aggregated_output.show()","title":"Passing in a list of aggregations to apply"},{"location":"training_resources/pyspark/pyspark-style-guide/#create-your-own-schema","text":"If you wish to create your own custom schema, here's how to do it: schema = StructType ([ StructField ( \"id\" , IntegerType (), True ), StructField ( \"colour\" , StringType (), True ), StructField ( \"rate\" , FloatType (), True ), StructField ( \"date\" , DateType (), True ) ])","title":"Create your own schema"},{"location":"training_resources/pyspark/pyspark-style-guide/#performance-caching-and-lazy-operations","text":"Lazy evaluation in Spark means that the execution will not start until an action is triggered. When we call for an operation to execute, it does not execute immediately. Spark maintains the record of which operation is being called.","title":"Performance - caching and lazy operations"},{"location":"training_resources/pyspark/pyspark-style-guide/#general-tips-and-tricks","text":"Try to keep your code in logical structured blocks. For example, if you have multiple lines referencing the same things, try to keep them together. Separating them reduces context and readability. Try to be as explicit and descriptive as possible when naming functions or variables. Strive to capture what the function is actually doing as opposed to naming it based the objects used inside of it. Think twice about introducing new import aliases, unless there is a good reason to do so. Some of the established ones are types and functions from PySpark from pyspark.sql import types as T, functions as F. Avoid using literal strings or integers in filtering conditions, new values of columns etc. Instead, to capture their meaning, extract them into variables, constants, dicts or classes as suitable. This makes the code more readable and enforces consistency across the repository.","title":"General tips and tricks"},{"location":"training_resources/pyspark/pyspark-style-guide/#external-links","text":"GSS introduction to PySpark Palantir PySpark style guide","title":"External links"},{"location":"training_resources/python/backtesting/","text":"Back testing What is back testing and why do I care? Now that you are writing code in a reproducible manner, and perhaps using Python instead of another language, it is important that the code still produces the same results as the old code. Mistakes can easily be made in translating from one code base to another. By following the steps in this guide, we can create a set of tests which will check that the outputs of the new code match the outputs of the old code. First, we need the output from the old code in a CSV, to compare to the CSV which is produced by the new code. This should match the format of the new version, i.e. tidy data . So you'll need to translate the output of your old code into tidy data format. You can do this by manipulating the output manually (e.g. in Excel), you don't need to waste time modifying your old code base. Comparing CSVs Create a file in your tests folder called test_compare_results.py . In this file, we will write a set of functions which compare the old CSV to the new CSV. These comparsions are easy if we read the CSV files as Pandas Dataframes using pd.read_csv : import pandas as pd import pathlib2 TARGET_DIR = pathlib2 . Path ( < path > ) GTRUTH_DIR = pathlib2 . Path ( < path > ) target_df = pd . read_csv ( TARGET_DIR / 'target.csv' ) gtruth_df = pd . read_csv ( GTRUTH_DIR / 'gtruth.csv' ) Pandas Dataframes have some inbuilt attributes which we can now compare. For example, to check that they have the same number of rows and columns, we use .shape : assert target_df . shape == gtruth_df . shape , 'Target has a different number of rows and/or columns to ground truth' The assert function runs the condition target_df.shape == gruth_df.shape . It does nothing if the condition is True, but raises an assertion error if the condition is False. We can decide what to write as a helpful error message, in this case: 'Target has a different number of rows and/or columns to ground truth' . Find out more about Python assert here . We can write more tests to run various comparsions of the two CSVs, e.g.: Check that they have the same column names in the same order: assert target_df . columns . tolist () == gtruth_df . columns . tolist () Finally, check the contents of each column are the same. for each_col in gtruth_df . columns : assert ( gtruth_df [ each_col ] . equals ( target_df [ each_col ])) You may think of other tests appropriate to your project. Now, save your new script, and from the command line, simply type python test_compare_results.py to run the script. If any of the tests have failed the error message we composed will be shown. Comparing Multiple CSVs Your code may have multiple outputs which you want to compare to ground truth files. Here's one way to do it, by iterating through a list of the pairs of CSVs. files_to_compare = [( 'target1.csv' , 'gtruth1.csv' ), ( 'target2.csv' , 'gtruth2.csv' ), ( 'target3.csv' , 'gtruth3.csv' )] for ( target_filename , gtruth_filename ) in files_to_compare : target_df = pd . read_csv ( TARGET_DIR / target_filename ) gtruth_df = pd . read_csv ( GTRUTH_DIR / gtruth_filename ) assert target_df . shape == gtruth_df . shape , 'Target has a different number of rows and/or columns to ground truth' assert target_df . columns . tolist () == gtruth_df . columns . tolist (), 'Target has different column names to ground truth' for each_col in gtruth_df . columns : assert ( gtruth_df [ each_col ] . equals ( target_df [ each_col ])), 'The contents of the columns are different' External links Python assert","title":"Back testing"},{"location":"training_resources/python/backtesting/#back-testing","text":"","title":"Back testing"},{"location":"training_resources/python/backtesting/#what-is-back-testing-and-why-do-i-care","text":"Now that you are writing code in a reproducible manner, and perhaps using Python instead of another language, it is important that the code still produces the same results as the old code. Mistakes can easily be made in translating from one code base to another. By following the steps in this guide, we can create a set of tests which will check that the outputs of the new code match the outputs of the old code. First, we need the output from the old code in a CSV, to compare to the CSV which is produced by the new code. This should match the format of the new version, i.e. tidy data . So you'll need to translate the output of your old code into tidy data format. You can do this by manipulating the output manually (e.g. in Excel), you don't need to waste time modifying your old code base.","title":"What is back testing and why do I care? "},{"location":"training_resources/python/backtesting/#comparing-csvs","text":"Create a file in your tests folder called test_compare_results.py . In this file, we will write a set of functions which compare the old CSV to the new CSV. These comparsions are easy if we read the CSV files as Pandas Dataframes using pd.read_csv : import pandas as pd import pathlib2 TARGET_DIR = pathlib2 . Path ( < path > ) GTRUTH_DIR = pathlib2 . Path ( < path > ) target_df = pd . read_csv ( TARGET_DIR / 'target.csv' ) gtruth_df = pd . read_csv ( GTRUTH_DIR / 'gtruth.csv' ) Pandas Dataframes have some inbuilt attributes which we can now compare. For example, to check that they have the same number of rows and columns, we use .shape : assert target_df . shape == gtruth_df . shape , 'Target has a different number of rows and/or columns to ground truth' The assert function runs the condition target_df.shape == gruth_df.shape . It does nothing if the condition is True, but raises an assertion error if the condition is False. We can decide what to write as a helpful error message, in this case: 'Target has a different number of rows and/or columns to ground truth' . Find out more about Python assert here . We can write more tests to run various comparsions of the two CSVs, e.g.: Check that they have the same column names in the same order: assert target_df . columns . tolist () == gtruth_df . columns . tolist () Finally, check the contents of each column are the same. for each_col in gtruth_df . columns : assert ( gtruth_df [ each_col ] . equals ( target_df [ each_col ])) You may think of other tests appropriate to your project. Now, save your new script, and from the command line, simply type python test_compare_results.py to run the script. If any of the tests have failed the error message we composed will be shown.","title":"Comparing CSVs"},{"location":"training_resources/python/backtesting/#comparing-multiple-csvs","text":"Your code may have multiple outputs which you want to compare to ground truth files. Here's one way to do it, by iterating through a list of the pairs of CSVs. files_to_compare = [( 'target1.csv' , 'gtruth1.csv' ), ( 'target2.csv' , 'gtruth2.csv' ), ( 'target3.csv' , 'gtruth3.csv' )] for ( target_filename , gtruth_filename ) in files_to_compare : target_df = pd . read_csv ( TARGET_DIR / target_filename ) gtruth_df = pd . read_csv ( GTRUTH_DIR / gtruth_filename ) assert target_df . shape == gtruth_df . shape , 'Target has a different number of rows and/or columns to ground truth' assert target_df . columns . tolist () == gtruth_df . columns . tolist (), 'Target has different column names to ground truth' for each_col in gtruth_df . columns : assert ( gtruth_df [ each_col ] . equals ( target_df [ each_col ])), 'The contents of the columns are different'","title":"Comparing Multiple CSVs"},{"location":"training_resources/python/backtesting/#external-links","text":"Python assert","title":"External links"},{"location":"training_resources/python/basic-python-data-analysis-operations/","text":"Basic Python Data Analysis operations Python offers many ways to achieve multiple calculations, computations and operations. For data analysis and data science overall, Pandas is the most commonly used package or library to perform these operations, along with NumPy . Reading in data To get started with Pandas import: import pandas as pd import numpy as np Loading data from a .csv file df - dataframe df = pd . read_csv ( 'your_file.csv' ) # or if required to edit headers for example: df = pd . read_csv ( 'your_file.csv' , header =... , na_values =... , sep =... , etc ) Loading data from an Excel file .xlsx df = pd . read_excel ( 'your_file.xlsx' ) Loading data from a SQL table/database import pyodbc import pandas as pd connection_object = pyodbc . connect ( \"Driver={SQL Server};\" \"Server=xxxx;\" \"Database=xxxx;\" \"Trusted_Connection=yes;\" ) df = pd . read_sql ( insert_your_sql_query , connection_object ) Add your Server address and Database name in the respective conditions above. Loading data from an SPSS file (.sav) There are two way to import a .sav file. One way: import pandas as pd df = pd . read_spss ( file_path , convert_categoricals = False ) or import pyreadstat df , metadata = pyreadstat . read_sav ( file_path ) The second option creates the dataframe but also captures the metadata of the .sav file, which is useful when running data validation checks. Common Operations There are many ways to perform various operations in Python, depending on the library you are using or the general approach and design you've opted to apply to your code. This guide presents several examples on how to perform common SAS operations using pandas, any suggestions or feedback is welcome. Cases You will soon notice after importing your data from the .sav file that the column headers are not aligned consistently to upper case or lower case, but a mixture of this. To apply lower or upper case to all column headers: df . columns = df . columns . str . lower () # or df . columns = df . columns . str . upper () Extracting the required columns To select a column: # columns to keep to_keep = [ \"column 1\" , \"column 2\" , \"column 3\" , ... ] # create the new table filtered_df = df [ to_keep ] Filter where a variable is not null/missing To filter rows based on some specific criteria: # not null new_df = df [ df [ \"my_column\" ] . notnull ()] Joins # a left join, one column to join on joined_df = df . merge ( other_df , how = \"left\" , on = \"my_column\" ) # inner join, on multiple columns joined_df = df . merge ( other_df , how = \"inner\" , on = [ \"column 1\" , \"column 2\" ]) Add a new column # create new table with a new column that adds 5 to each value of another selected column new_df = df . assign ( new_column = df [ \"my column\" ] + 5 ) Sorting variables # ascending order can be False or True df . sort_values ( by = \"my column\" , ascending = False ) # if you want to see missing values first, assign na_position df . sort_values ( by = \"my column\" , ascending = False , na_position = \"first\" ) # sort by multiple columns df . sort_values ( by = [ \"my column 1\" , \"my column 2\" , ... ]) Transposing columns There's a few ways to transpose columns: # set the index of columns df . set_index ([ \"my column 1\" , \"my column 2\" , \"my column 3\" , ... ], inplace = True ) # using pandas transpose to transpose rows with columns and vice versa df_transposed = df . T # using pandas stack() to transpose non-index columns into a single new column df = df . stack () . reset_index () To set the name of the axis for the index or columns you can use rename_axis() : df = df . stack () . rename_axis () . reset_index () Grouping by variables # group by one column new_df = df . groupby ( \"my_column\" ) # group by multiple columns # list of columns to group by grouped = [ \"column 1\" , \"column 2\" , \"column 3\" , ... ]] # return new table with grouped columns new_df = df . groupby ( grouped ) Aggregations Once we've done the groupings above, add the aggregations: new_df = df . groupby ( grouped ) . agg ( total_sum = ( \"column to be summarised\" , \"sum\" ), total_count = ( \"column to be counted\" , \"count\" )) . reset_index () Creating totals per row and per column # total per column, adds a new row \"Column Total\" # this will sum all numeric row values for each column df . loc [ \"Column Total\" ] = df . sum ( numeric_only = True , axis = 0 ) # total per row, creates a new column \"Row Total\" # this will sum all numeric column values for each row df . loc [:, \"Row Total\" ] = df . sum ( numeric_only = True , axis = 1 ) Appending totals to a table When creating different aggregations/groupings which are saved in different dataframes, you can then combine these aggregations into one table. For example, suppose you have calculated the totals for age and gender in different dataframes and you wish to append these results to the final output dataframe. # list the final output dataframe to store its aggregations list_df = [ df ] # append the calculated totals list_df . append ( calc_totals_df ) # concatenate into a single dataframe output_df = pd . concat ( list_df , axis = 0 ) Creating derivations To create a derivation based on the equivalent CASE WHEN SQL operation, there are several ways to do this in python: # pandas package CASE WHEN # create the age 11 to 15 derivation df . loc [ df [ \"age\" ] < 0 , \"age11_15\" ] = df [ \"age\" ] df . loc [( df [ \"age\" ] > 0 ) & ( df [ \"age\" ] < 11 ), \"age11_15\" ] = 11 df . loc [( df [ \"age\" ] > 10 ) & ( df [ \"age\" ] < 16 ), \"age11_15\" ] = df [ \"age\" ] df . loc [ df [ \"age\" ] > 14 , \"age11_15\" ] = 15 This results in creating a new column \"age11_15\" in the existing dataframe, based on the CASE WHEN conditions we applied for the new derivation. # NumPy package CASE WHEN # create the age 11 to 15 derivation age11_15 = np . select ( [ df [ 'age' ] == 10 , # WHEN df [ 'age' ] > 15 # WHEN ], [ 11 , # if age == 10 then assign 11 15 # if age > 15 assign 15 ], default = df [ 'age' ] # ELSE assign \"age\" column values ) # assign the result to a new column df [ \"age11_15\" ] = age11_15 In the first bracket you assign the \"WHEN\" part of the condition, second bracket the \"THEN\", and \"default=...\" represents the \"ELSE\" part. The NumPy option is faster and more efficient whereas Pandas is user friendlier and straightforward in its application. For datasets with only thousands of rows, whichever option you apply won't make a difference. Apply a column order # create a list of the column headers in a specific order column_order = [ \"column 1\" , \"column 2\" , \"column 3\" , ... ] # apply list to dataframe df = df [ column_order ] Exporting the output # write output to a .csv df . to_csv ( \"output.csv\" , ... < multiple parameters that can be inserted > ) # write output to an excel workbook df . to_excel ( \"output.xlsx\" , sheet_name = \"Sheet_name_1\" , ... < multiple parameters that can be inserted > ) # write multiple sheets from different dataframes with pd . ExcelWriter ( \"output.xlsx\" ) as writer : df1 . to_excel ( writer , sheet_name = \"Sheet_name_1\" ) df2 . to_excel ( writer , sheet_name = \"Sheet_name_2\" ) To check the parameters which can be adjusted for each file export, the pandas documentation provides useful resources. pandas.DataFrame.to_excel() for example. Further reading pandas documentation NumPy documentation PEP-8 Python style guide","title":"Basic Python data analysis operations"},{"location":"training_resources/python/basic-python-data-analysis-operations/#basic-python-data-analysis-operations","text":"Python offers many ways to achieve multiple calculations, computations and operations. For data analysis and data science overall, Pandas is the most commonly used package or library to perform these operations, along with NumPy .","title":"Basic Python Data Analysis operations"},{"location":"training_resources/python/basic-python-data-analysis-operations/#reading-in-data","text":"To get started with Pandas import: import pandas as pd import numpy as np","title":"Reading in data"},{"location":"training_resources/python/basic-python-data-analysis-operations/#loading-data-from-a-csv-file","text":"df - dataframe df = pd . read_csv ( 'your_file.csv' ) # or if required to edit headers for example: df = pd . read_csv ( 'your_file.csv' , header =... , na_values =... , sep =... , etc )","title":"Loading data from a .csv file"},{"location":"training_resources/python/basic-python-data-analysis-operations/#loading-data-from-an-excel-file-xlsx","text":"df = pd . read_excel ( 'your_file.xlsx' )","title":"Loading data from an Excel file .xlsx"},{"location":"training_resources/python/basic-python-data-analysis-operations/#loading-data-from-a-sql-tabledatabase","text":"import pyodbc import pandas as pd connection_object = pyodbc . connect ( \"Driver={SQL Server};\" \"Server=xxxx;\" \"Database=xxxx;\" \"Trusted_Connection=yes;\" ) df = pd . read_sql ( insert_your_sql_query , connection_object ) Add your Server address and Database name in the respective conditions above.","title":"Loading data from a SQL table/database"},{"location":"training_resources/python/basic-python-data-analysis-operations/#loading-data-from-an-spss-file-sav","text":"There are two way to import a .sav file. One way: import pandas as pd df = pd . read_spss ( file_path , convert_categoricals = False ) or import pyreadstat df , metadata = pyreadstat . read_sav ( file_path ) The second option creates the dataframe but also captures the metadata of the .sav file, which is useful when running data validation checks.","title":"Loading data from an SPSS file (.sav)"},{"location":"training_resources/python/basic-python-data-analysis-operations/#common-operations","text":"There are many ways to perform various operations in Python, depending on the library you are using or the general approach and design you've opted to apply to your code. This guide presents several examples on how to perform common SAS operations using pandas, any suggestions or feedback is welcome.","title":"Common Operations"},{"location":"training_resources/python/basic-python-data-analysis-operations/#cases","text":"You will soon notice after importing your data from the .sav file that the column headers are not aligned consistently to upper case or lower case, but a mixture of this. To apply lower or upper case to all column headers: df . columns = df . columns . str . lower () # or df . columns = df . columns . str . upper ()","title":"Cases"},{"location":"training_resources/python/basic-python-data-analysis-operations/#extracting-the-required-columns","text":"To select a column: # columns to keep to_keep = [ \"column 1\" , \"column 2\" , \"column 3\" , ... ] # create the new table filtered_df = df [ to_keep ]","title":"Extracting the required columns"},{"location":"training_resources/python/basic-python-data-analysis-operations/#filter-where-a-variable-is-not-nullmissing","text":"To filter rows based on some specific criteria: # not null new_df = df [ df [ \"my_column\" ] . notnull ()]","title":"Filter where a variable is not null/missing"},{"location":"training_resources/python/basic-python-data-analysis-operations/#joins","text":"# a left join, one column to join on joined_df = df . merge ( other_df , how = \"left\" , on = \"my_column\" ) # inner join, on multiple columns joined_df = df . merge ( other_df , how = \"inner\" , on = [ \"column 1\" , \"column 2\" ])","title":"Joins"},{"location":"training_resources/python/basic-python-data-analysis-operations/#add-a-new-column","text":"# create new table with a new column that adds 5 to each value of another selected column new_df = df . assign ( new_column = df [ \"my column\" ] + 5 )","title":"Add a new column"},{"location":"training_resources/python/basic-python-data-analysis-operations/#sorting-variables","text":"# ascending order can be False or True df . sort_values ( by = \"my column\" , ascending = False ) # if you want to see missing values first, assign na_position df . sort_values ( by = \"my column\" , ascending = False , na_position = \"first\" ) # sort by multiple columns df . sort_values ( by = [ \"my column 1\" , \"my column 2\" , ... ])","title":"Sorting variables"},{"location":"training_resources/python/basic-python-data-analysis-operations/#transposing-columns","text":"There's a few ways to transpose columns: # set the index of columns df . set_index ([ \"my column 1\" , \"my column 2\" , \"my column 3\" , ... ], inplace = True ) # using pandas transpose to transpose rows with columns and vice versa df_transposed = df . T # using pandas stack() to transpose non-index columns into a single new column df = df . stack () . reset_index () To set the name of the axis for the index or columns you can use rename_axis() : df = df . stack () . rename_axis () . reset_index ()","title":"Transposing columns"},{"location":"training_resources/python/basic-python-data-analysis-operations/#grouping-by-variables","text":"# group by one column new_df = df . groupby ( \"my_column\" ) # group by multiple columns # list of columns to group by grouped = [ \"column 1\" , \"column 2\" , \"column 3\" , ... ]] # return new table with grouped columns new_df = df . groupby ( grouped )","title":"Grouping by variables"},{"location":"training_resources/python/basic-python-data-analysis-operations/#aggregations","text":"Once we've done the groupings above, add the aggregations: new_df = df . groupby ( grouped ) . agg ( total_sum = ( \"column to be summarised\" , \"sum\" ), total_count = ( \"column to be counted\" , \"count\" )) . reset_index ()","title":"Aggregations"},{"location":"training_resources/python/basic-python-data-analysis-operations/#creating-totals-per-row-and-per-column","text":"# total per column, adds a new row \"Column Total\" # this will sum all numeric row values for each column df . loc [ \"Column Total\" ] = df . sum ( numeric_only = True , axis = 0 ) # total per row, creates a new column \"Row Total\" # this will sum all numeric column values for each row df . loc [:, \"Row Total\" ] = df . sum ( numeric_only = True , axis = 1 )","title":"Creating totals per row and per column"},{"location":"training_resources/python/basic-python-data-analysis-operations/#appending-totals-to-a-table","text":"When creating different aggregations/groupings which are saved in different dataframes, you can then combine these aggregations into one table. For example, suppose you have calculated the totals for age and gender in different dataframes and you wish to append these results to the final output dataframe. # list the final output dataframe to store its aggregations list_df = [ df ] # append the calculated totals list_df . append ( calc_totals_df ) # concatenate into a single dataframe output_df = pd . concat ( list_df , axis = 0 )","title":"Appending totals to a table"},{"location":"training_resources/python/basic-python-data-analysis-operations/#creating-derivations","text":"To create a derivation based on the equivalent CASE WHEN SQL operation, there are several ways to do this in python: # pandas package CASE WHEN # create the age 11 to 15 derivation df . loc [ df [ \"age\" ] < 0 , \"age11_15\" ] = df [ \"age\" ] df . loc [( df [ \"age\" ] > 0 ) & ( df [ \"age\" ] < 11 ), \"age11_15\" ] = 11 df . loc [( df [ \"age\" ] > 10 ) & ( df [ \"age\" ] < 16 ), \"age11_15\" ] = df [ \"age\" ] df . loc [ df [ \"age\" ] > 14 , \"age11_15\" ] = 15 This results in creating a new column \"age11_15\" in the existing dataframe, based on the CASE WHEN conditions we applied for the new derivation. # NumPy package CASE WHEN # create the age 11 to 15 derivation age11_15 = np . select ( [ df [ 'age' ] == 10 , # WHEN df [ 'age' ] > 15 # WHEN ], [ 11 , # if age == 10 then assign 11 15 # if age > 15 assign 15 ], default = df [ 'age' ] # ELSE assign \"age\" column values ) # assign the result to a new column df [ \"age11_15\" ] = age11_15 In the first bracket you assign the \"WHEN\" part of the condition, second bracket the \"THEN\", and \"default=...\" represents the \"ELSE\" part. The NumPy option is faster and more efficient whereas Pandas is user friendlier and straightforward in its application. For datasets with only thousands of rows, whichever option you apply won't make a difference.","title":"Creating derivations"},{"location":"training_resources/python/basic-python-data-analysis-operations/#apply-a-column-order","text":"# create a list of the column headers in a specific order column_order = [ \"column 1\" , \"column 2\" , \"column 3\" , ... ] # apply list to dataframe df = df [ column_order ]","title":"Apply a column order"},{"location":"training_resources/python/basic-python-data-analysis-operations/#exporting-the-output","text":"# write output to a .csv df . to_csv ( \"output.csv\" , ... < multiple parameters that can be inserted > ) # write output to an excel workbook df . to_excel ( \"output.xlsx\" , sheet_name = \"Sheet_name_1\" , ... < multiple parameters that can be inserted > ) # write multiple sheets from different dataframes with pd . ExcelWriter ( \"output.xlsx\" ) as writer : df1 . to_excel ( writer , sheet_name = \"Sheet_name_1\" ) df2 . to_excel ( writer , sheet_name = \"Sheet_name_2\" ) To check the parameters which can be adjusted for each file export, the pandas documentation provides useful resources. pandas.DataFrame.to_excel() for example.","title":"Exporting the output"},{"location":"training_resources/python/basic-python-data-analysis-operations/#further-reading","text":"pandas documentation NumPy documentation PEP-8 Python style guide","title":"Further reading"},{"location":"training_resources/python/handling-file-paths/","text":"Handling file paths What is pathlib? pathlib is a built-in (python-3) package for handling filesystem paths. pathlib offers helpful ways to perform a variety of operations, including: Specifying paths to files / folders using simple, clear syntax Traversing the filesystem (i.e. getting the parents / children of a given file / folder) Composing paths based on constituent elements (e.g. by extending a root path to a folder with a subfolder / file name) See How do I use it? for examples illustrating the above. Why should I care? The key strengths of pathlib include: Simplicity : pathlib objects are easy to create, extend and reuse Functionality : pathlib offers functionality that storing paths inside strings does not, including Getting file names and stems (aka names without extensions) Getting the names of parent folders Accessing the name of the current working directory Listing the contents of nested folders Reusability : pathlib is OS-agnostic, meaning code will work with both Windows and Linux filesystems How do I use it? The steps below show briefly how to make use of pathlib. This is by no means an exhaustive walkthrough for everything you can do with pathlib - for more information, see the pathlib-docs . Getting started Import pathlib or pathlib2 (the python-2 backwards compatible version) import pathlib In pathlib, pathlib.Path objects are the key components. You can define Path objects and access attributes / methods to perform a wide variety of operations For example, you can access the current working directory with the cwd attribute. # Print the current working directory (cwd) print ( \"CWD:\" , pathlib . Path . cwd ()) Pass strings to Path constructor to create a Path object # . is the current directory cwd_path = pathlib . Path ( \".\" ) print ( \"CWD (again):\" , cwd_path ) # Use resolve to get the absolute path! cwd_abspath = cwd_path . resolve () print ( \"Absolute CWD:\" , cwd_abspath ) Path attributes The following examples show how pathlib makes it easier to extract specific attributes of a path. Example: absolute path to the current file # Note: __file__ is a global python variable this_file_path = pathlib . Path ( __file__ ) print ( \"Path to file:\" , this_file_path ) Example: get the file name print ( \"File name:\" , this_file_path . name ) Example: get the file name without extension (aka the stem) print ( \"File stem:\" , this_file_path . stem ) Example: get the parent directory print ( \"Parent folder:\" , this_file_path . parent ) To see all the options (there are many!) use help(pathlib.Path) or see the pathlib-docs . Path composition pathlib helps with traversing the directory tree. Slashes join elements of a path. path1 = pathlib . Path ( \"parent\" ) print ( \"Path 1:\" , path1 . resolve ()) path2 = pathlib . Path ( \"child\" ) print ( \"Path 2:\" , path2 . resolve ()) path3 = path1 / path2 print ( \"Path 3:\" , path3 . resolve ()) .. indicates the parent directory path4 = path1 / path2 / pathlib . Path ( \"..\" ) print ( \"Path 4:\" , path4 . resolve ()) # This should be the same as path1 Only the first element needs to be a path - the rest can be strings! path1 = pathlib . Path ( \"parent\" ) print ( \"Path 1:\" , path1 . resolve ()) path2 = path1 / \"child\" print ( \"Path 2:\" , path2 . resolve ()) path3 = path2 / \"..\" print ( \"Path 3:\" , path3 . resolve ()) # This should be the same as path1 Reading a file The open method on the Path object can be used to access a file. file_path = pathlib . Path ( \"..\" ) / \"data\" / \"example.txt\" with file_path . open ( \"r\" ) as file : content = file . read () print ( content ) Example: load data into pandas DataFrame pathlib Paths are accepted by most pandas methods for reading data. This example shows how to do this for a real RAP project: import pandas as pd import pyreadstat # needed to parse sav files in spss import pathlib2 # This is just a backwards compatible pathlib! # https://realpython.com/python-pathlib/ # Add parameters BASE_DIR = pathlib2 . Path ( r \" \\\\ <path>\\Publication\\RAP\" ) PUPIL_DIR = BASE_DIR / \"Inputs\" / \"PupilData\" PUPIL_FILE = \"SDD2018 - Stage 14 - 290519.sav\" PUPIL_DATA_PATH = PUPIL_DIR / PUPIL_FILE pupil_data = pd . read_spss ( PUPIL_DATA_PATH ) External references pathlib-docs","title":"Handling file paths"},{"location":"training_resources/python/handling-file-paths/#handling-file-paths","text":"","title":"Handling file paths"},{"location":"training_resources/python/handling-file-paths/#what-is-pathlib","text":"pathlib is a built-in (python-3) package for handling filesystem paths. pathlib offers helpful ways to perform a variety of operations, including: Specifying paths to files / folders using simple, clear syntax Traversing the filesystem (i.e. getting the parents / children of a given file / folder) Composing paths based on constituent elements (e.g. by extending a root path to a folder with a subfolder / file name) See How do I use it? for examples illustrating the above.","title":"What is pathlib? "},{"location":"training_resources/python/handling-file-paths/#why-should-i-care","text":"The key strengths of pathlib include: Simplicity : pathlib objects are easy to create, extend and reuse Functionality : pathlib offers functionality that storing paths inside strings does not, including Getting file names and stems (aka names without extensions) Getting the names of parent folders Accessing the name of the current working directory Listing the contents of nested folders Reusability : pathlib is OS-agnostic, meaning code will work with both Windows and Linux filesystems","title":"Why should I care? "},{"location":"training_resources/python/handling-file-paths/#how-do-i-use-it","text":"The steps below show briefly how to make use of pathlib. This is by no means an exhaustive walkthrough for everything you can do with pathlib - for more information, see the pathlib-docs .","title":"How do I use it? "},{"location":"training_resources/python/handling-file-paths/#getting-started","text":"Import pathlib or pathlib2 (the python-2 backwards compatible version) import pathlib In pathlib, pathlib.Path objects are the key components. You can define Path objects and access attributes / methods to perform a wide variety of operations For example, you can access the current working directory with the cwd attribute. # Print the current working directory (cwd) print ( \"CWD:\" , pathlib . Path . cwd ()) Pass strings to Path constructor to create a Path object # . is the current directory cwd_path = pathlib . Path ( \".\" ) print ( \"CWD (again):\" , cwd_path ) # Use resolve to get the absolute path! cwd_abspath = cwd_path . resolve () print ( \"Absolute CWD:\" , cwd_abspath )","title":"Getting started"},{"location":"training_resources/python/handling-file-paths/#path-attributes","text":"The following examples show how pathlib makes it easier to extract specific attributes of a path.","title":"Path attributes"},{"location":"training_resources/python/handling-file-paths/#example-absolute-path-to-the-current-file","text":"# Note: __file__ is a global python variable this_file_path = pathlib . Path ( __file__ ) print ( \"Path to file:\" , this_file_path )","title":"Example: absolute path to the current file"},{"location":"training_resources/python/handling-file-paths/#example-get-the-file-name","text":"print ( \"File name:\" , this_file_path . name )","title":"Example: get the file name"},{"location":"training_resources/python/handling-file-paths/#example-get-the-file-name-without-extension-aka-the-stem","text":"print ( \"File stem:\" , this_file_path . stem )","title":"Example: get the file name without extension (aka the stem)"},{"location":"training_resources/python/handling-file-paths/#example-get-the-parent-directory","text":"print ( \"Parent folder:\" , this_file_path . parent ) To see all the options (there are many!) use help(pathlib.Path) or see the pathlib-docs .","title":"Example: get the parent directory"},{"location":"training_resources/python/handling-file-paths/#path-composition","text":"pathlib helps with traversing the directory tree. Slashes join elements of a path. path1 = pathlib . Path ( \"parent\" ) print ( \"Path 1:\" , path1 . resolve ()) path2 = pathlib . Path ( \"child\" ) print ( \"Path 2:\" , path2 . resolve ()) path3 = path1 / path2 print ( \"Path 3:\" , path3 . resolve ()) .. indicates the parent directory path4 = path1 / path2 / pathlib . Path ( \"..\" ) print ( \"Path 4:\" , path4 . resolve ()) # This should be the same as path1 Only the first element needs to be a path - the rest can be strings! path1 = pathlib . Path ( \"parent\" ) print ( \"Path 1:\" , path1 . resolve ()) path2 = path1 / \"child\" print ( \"Path 2:\" , path2 . resolve ()) path3 = path2 / \"..\" print ( \"Path 3:\" , path3 . resolve ()) # This should be the same as path1","title":"Path composition"},{"location":"training_resources/python/handling-file-paths/#reading-a-file","text":"The open method on the Path object can be used to access a file. file_path = pathlib . Path ( \"..\" ) / \"data\" / \"example.txt\" with file_path . open ( \"r\" ) as file : content = file . read () print ( content )","title":"Reading a file"},{"location":"training_resources/python/handling-file-paths/#example-load-data-into-pandas-dataframe","text":"pathlib Paths are accepted by most pandas methods for reading data. This example shows how to do this for a real RAP project: import pandas as pd import pyreadstat # needed to parse sav files in spss import pathlib2 # This is just a backwards compatible pathlib! # https://realpython.com/python-pathlib/ # Add parameters BASE_DIR = pathlib2 . Path ( r \" \\\\ <path>\\Publication\\RAP\" ) PUPIL_DIR = BASE_DIR / \"Inputs\" / \"PupilData\" PUPIL_FILE = \"SDD2018 - Stage 14 - 290519.sav\" PUPIL_DATA_PATH = PUPIL_DIR / PUPIL_FILE pupil_data = pd . read_spss ( PUPIL_DATA_PATH )","title":"Example: load data into pandas DataFrame"},{"location":"training_resources/python/handling-file-paths/#external-references","text":"pathlib-docs","title":"External references"},{"location":"training_resources/python/logging-and-error-handling/","text":"Logging and error handling Logging and error handling are two concepts that will improve the reliability and maintainability of your code. These big topics could each be given their own chapter but here we try to show how the combination of simple logging and simple error handling can be easy to implement while offering substantial benefits. What is logging? Logging is a way to track events that occur when your code runs. The logs will show when different parts of your code was run and what the result was. For example, my ingest_data.py module ran at 08:44 on 2021-09-17 and found 3,452,521 rows of data. You can think of logging as the next evolution of the print statement. When writing code interactively, we use print statements to understand what is happening at each step. By replacing these print statements with logs, we are able to permanently track what is happening in these interesting parts of the code. Python\u2019s standard library \" logging \" includes a flexible built-in logging module, allowing the developer to create different configurations to fulfil their logging needs. For example, the log can be categorised based on levels of severity and the users might select the destination (in console, files, remote server, or email). What is error handling? Error handling provides a way for us to try to handle problems in our code in a constructive manner. Error handling works hand-in-hand with logging to improve code reliability. When our code fails for some reason, we usually see an error message on screen and the running of the code stops. That error message is known as an exception. Exceptions are objects which represent errors which contain infortmation about the kind of error and where it was caused. As a developer, I can often anticipate what parts of my code are likely to have errors. For example, parsing a .csv file where we expect a certain list of column headers. By using error handling at these points in the code, I can make the code fail gracefully while writing an informative error message to my logs. In this way we improve visibility of the functioning of our code. If the code runs every night at midnight but has failed last night, then our error handling and logging should tell me exactly what has gone wrong. Logging in Python Python comes with a standard \" logging \" library that gives us the capabilities to: Diagnose an issue during development or debug bigger problem Track what is going on in the system for better monitoring Control what output is created and how it looks You can read the official python logging tutorial for instructions on how to customise and adapt the logger. As a basic example - here is the code to set up a logger: import logging logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' , handlers = [ logging . FileHandler ( f \"./reports/logs/ { time . strftime ( '%Y-%m- %d _%H-%M-%S' ) } .log\" ), logging . StreamHandler ( sys . stdout ) ] ) logger = logging . getLogger ( __name__ ) The format argument describes how we want our log to look. Here we include the time, the name of the file that is running, and a custom message that we will specify when we use the logger. The handlers argument sets up two outputs for the log - one copy of the log info will be written to a file (the FileHandler) and another copy will be printed on screen (just like a print statement). We can then use the logger to keep track of what is happening in the code by passing in informative messages: logger . info ( \"Step 1: Import all the input data from MSSQL database\" ) The resulting logs might look something like this: 2021-08-02 15:43:13 - __main__ - INFO - Step 1: Import all the input data from MSSQL database 2021-08-02 15:43:14 - diabetes.utilities.processing_steps - INFO - Read and prepare input data 2021-08-02 15:43:15 - diabetes.utilities.data_connections - INFO - Loading 100 rows from dev.NDA_DEMO_E3_202021... 2021-08-02 15:43:16 - diabetes.utilities.data_connections - INFO - Loading 100 rows from dev.NDA_BMI_E3_202021_... You can see that the information we described in the format argument is captured here. We could go further and include additional useful info: Filename where the log was triggered ( filename ) Function name where the log was triggered ( funcName ) Line number where the log was triggered ( lineno ) Process ID of the process where the log happened ( processName ) You can find a full list of things to add in the docs . Video tutorial: Logging demo Watch here . Disclaimer: the above video will not load for external users. Error handling in Python Error handling using try/except is useful and powerful. When python hits an error, it produces something called an exception. An exception is an object that tries can tell you something about what has gone wrong. There are many different types of exceptions - see the full list here . To get started with error handling however, the simplest way is to start with a block of code like below: input = 'some text' try : int ( input ) logger . info ( 'input converted to integer' ) except ValueError as e : logger . exception ( f 'Could not convert { input } to integer. Got error message: \\n\\n { repr ( e ) } ' ) Here we try to run our code and write a message to the log if it works. If the code fails then an exception will be raised - and our code writes that exception to the log so we can examine it later. The important thing is that by combining simple logging with simple error handling, you have really substantially improved the ease with which your code can be run and monitored. You have identified a part of the code where something is likely to go wrong and you have added some guard rails. This is bad practice as instead of handling the specific errors the code could throw out we instead try and ignore them. Lets say our code can throw out a ValueError , a ZeroDivisionError , and a KeyError . Instead of catching a general Exception which provides no control over how each of the exceptions are handled we should instead do the following: try : # Some problematic code that could raise different kinds of exceptions except ValueError as e : print ( 'Found a value error!' ) print ( repr ( e )) exit () except ZeroDivisionError as e : print ( 'Found a division by zero error!' ) print ( repr ( e )) exit () except KeyError as e : print ( 'Found a key error!' ) print ( repr ( e )) exit () Alternatively if we really did want to handle all of those exceptions in the same way we should still not use the generic Exception class. The reason for this is that if someone reads our code it is very unclear what kinds of errors the code can create, it is better to list all the errors as a tuple to provide a form of documentation like so: try : # Some problematic code that could raise different kinds of exceptions except ( ValueError , ZeroDivisionError , KeyError ) as e : print ( 'Found an error!' ) print ( repr ( e )) exit () Error handling dos and don'ts Don't raise a generic exception Raising a generic exception is bad practice: raise Exception ( 'Error I encountered' ) The problem with raising a generic exception like this is that it is difficult to catch correctly. For example: a = 1 b = 0 try : a / b raise Exception ( 'Error I encountered' ) except Exception as e : print ( 'Caught error with message: ' + repr ( e )) While we would expect our code to catch our raised exception and print 'Caught error with message: Error I encountered', it would actually instead print that it encountered a ZeroDivisionError . The reason for this is that ZeroDivisionError is a sub/child class of Exception , it is encountered first so is caught first. There is also a flip-side to this which causes another issue when raising a generic Exception : try : raise Exception ( 'Error I encountered' ) except ValueError : exit () While child classes of Exception are caught by an except block which catches errors of type Exception the same is not true the other way around. The raised Exception will not be caught by the except block here as it is not of type ValueError . As a general rule of thumb avoid using the generic Exception class at all. It is often tempting to write code like this: try : # Some problematic code that could raise different kinds of exceptions except Exception : print ( 'Found an error!' ) exit () Don't lose the stack trace The stack trace (a log of where the error occurred and how it occurred), is incredibly important and is one of the most useful features of exception objects. When an exception is created it has a stack trace. For example: def a (): return b () def b (): c () def c (): raise Exception ( 'Don \\' t call me!' ) a () The code above will obviously raise an exception. That exception will contain within it a stack trace showing that an exception was raised by function c , but it will also show the steps leading up to that: Exception <- c <- b <- a <- a() . This is very useful for debugging. There are cases when writing try/except statements where we want to catch an exception, log some message, and then raise the exception again. For example we may have a function that could create a ZeroDivisionError , inside that function we use a try/except to catch the exception and log a message about it. However, we don't want the function to return any value as the division failed, so instead we want to raise the exception again so that the code that called the function knows it failed: def divide_two_numbers ( a : float , b : float ) -> float : try : return a / b except ZeroDivisionError as e : print ( 'Division failed because of: ' + repr ( e )) raise ZeroDivisionError # In use: a = 1.0 b = 0 try : result = divide_two_numbers ( a , b ) except ZeroDivisionError : print ( 'Division failed can \\' t continue' ) exit () This is wrong . The specific place this is wrong is here: raise ZeroDivisionError Doing this raises a new ZeroDivisionError, which loses the stack trace of the original raised exception. Instead to re-raise the exception do the following: except ZeroDivisionError : # Do stuff raise Don't let the program continue if it can't It is a common mistake to view try/catch statements as a way of 'fixing' or avoiding an error. However, often an exception does signal that the program needs to terminate. The pseudocode below is an example of a common mistake pattern: my_file = None try : my_file = open_file ( path_to_my_file ) except : print ( 'Couldn \\' t find the file!' ) print ( my_file . contents ()) This is a common case where just because an exception has been caught, that doesn't mean the error has been handled. The potential error is the file not being found, the try/catch acknowledges this but then the program moves on as if nothing has happened. This means that inevitablely the final print statement will produce another error as it tries to use a None value. Instead the following should have been done: my_file = None try : my_file = open_file ( path_to_my_file ) except : print ( 'Couldn \\' t find the file!' ) exit () print ( my_file . contents ()) This is a very basic example but this topic is well covered by the official python error handling tutorial and so you can look there for an in-depth summary. There is a balance to be found in applying both logging and error handling. For most code you don't need or want everything wrapped up like this - it becomes messy. Pay attention to places where your code interfaces with anything external. Catching errors in PySpark See our guide on logging and error handling in PySpark Further reading Logging Cookbook Logging HowTo Logging in Python General guide on errors and error handling Page which provides a large list of errors that can be encountered","title":"Logging and error handling in Python"},{"location":"training_resources/python/logging-and-error-handling/#logging-and-error-handling","text":"Logging and error handling are two concepts that will improve the reliability and maintainability of your code. These big topics could each be given their own chapter but here we try to show how the combination of simple logging and simple error handling can be easy to implement while offering substantial benefits.","title":"Logging and error handling"},{"location":"training_resources/python/logging-and-error-handling/#what-is-logging","text":"Logging is a way to track events that occur when your code runs. The logs will show when different parts of your code was run and what the result was. For example, my ingest_data.py module ran at 08:44 on 2021-09-17 and found 3,452,521 rows of data. You can think of logging as the next evolution of the print statement. When writing code interactively, we use print statements to understand what is happening at each step. By replacing these print statements with logs, we are able to permanently track what is happening in these interesting parts of the code. Python\u2019s standard library \" logging \" includes a flexible built-in logging module, allowing the developer to create different configurations to fulfil their logging needs. For example, the log can be categorised based on levels of severity and the users might select the destination (in console, files, remote server, or email).","title":"What is logging?"},{"location":"training_resources/python/logging-and-error-handling/#what-is-error-handling","text":"Error handling provides a way for us to try to handle problems in our code in a constructive manner. Error handling works hand-in-hand with logging to improve code reliability. When our code fails for some reason, we usually see an error message on screen and the running of the code stops. That error message is known as an exception. Exceptions are objects which represent errors which contain infortmation about the kind of error and where it was caused. As a developer, I can often anticipate what parts of my code are likely to have errors. For example, parsing a .csv file where we expect a certain list of column headers. By using error handling at these points in the code, I can make the code fail gracefully while writing an informative error message to my logs. In this way we improve visibility of the functioning of our code. If the code runs every night at midnight but has failed last night, then our error handling and logging should tell me exactly what has gone wrong.","title":"What is error handling?"},{"location":"training_resources/python/logging-and-error-handling/#logging-in-python","text":"Python comes with a standard \" logging \" library that gives us the capabilities to: Diagnose an issue during development or debug bigger problem Track what is going on in the system for better monitoring Control what output is created and how it looks You can read the official python logging tutorial for instructions on how to customise and adapt the logger. As a basic example - here is the code to set up a logger: import logging logging . basicConfig ( level = logging . INFO , format = ' %(asctime)s - %(name)s - %(levelname)s - %(message)s ' , handlers = [ logging . FileHandler ( f \"./reports/logs/ { time . strftime ( '%Y-%m- %d _%H-%M-%S' ) } .log\" ), logging . StreamHandler ( sys . stdout ) ] ) logger = logging . getLogger ( __name__ ) The format argument describes how we want our log to look. Here we include the time, the name of the file that is running, and a custom message that we will specify when we use the logger. The handlers argument sets up two outputs for the log - one copy of the log info will be written to a file (the FileHandler) and another copy will be printed on screen (just like a print statement). We can then use the logger to keep track of what is happening in the code by passing in informative messages: logger . info ( \"Step 1: Import all the input data from MSSQL database\" ) The resulting logs might look something like this: 2021-08-02 15:43:13 - __main__ - INFO - Step 1: Import all the input data from MSSQL database 2021-08-02 15:43:14 - diabetes.utilities.processing_steps - INFO - Read and prepare input data 2021-08-02 15:43:15 - diabetes.utilities.data_connections - INFO - Loading 100 rows from dev.NDA_DEMO_E3_202021... 2021-08-02 15:43:16 - diabetes.utilities.data_connections - INFO - Loading 100 rows from dev.NDA_BMI_E3_202021_... You can see that the information we described in the format argument is captured here. We could go further and include additional useful info: Filename where the log was triggered ( filename ) Function name where the log was triggered ( funcName ) Line number where the log was triggered ( lineno ) Process ID of the process where the log happened ( processName ) You can find a full list of things to add in the docs .","title":"Logging in Python"},{"location":"training_resources/python/logging-and-error-handling/#video-tutorial-logging-demo","text":"Watch here . Disclaimer: the above video will not load for external users.","title":"Video tutorial: Logging demo"},{"location":"training_resources/python/logging-and-error-handling/#error-handling-in-python","text":"Error handling using try/except is useful and powerful. When python hits an error, it produces something called an exception. An exception is an object that tries can tell you something about what has gone wrong. There are many different types of exceptions - see the full list here . To get started with error handling however, the simplest way is to start with a block of code like below: input = 'some text' try : int ( input ) logger . info ( 'input converted to integer' ) except ValueError as e : logger . exception ( f 'Could not convert { input } to integer. Got error message: \\n\\n { repr ( e ) } ' ) Here we try to run our code and write a message to the log if it works. If the code fails then an exception will be raised - and our code writes that exception to the log so we can examine it later. The important thing is that by combining simple logging with simple error handling, you have really substantially improved the ease with which your code can be run and monitored. You have identified a part of the code where something is likely to go wrong and you have added some guard rails. This is bad practice as instead of handling the specific errors the code could throw out we instead try and ignore them. Lets say our code can throw out a ValueError , a ZeroDivisionError , and a KeyError . Instead of catching a general Exception which provides no control over how each of the exceptions are handled we should instead do the following: try : # Some problematic code that could raise different kinds of exceptions except ValueError as e : print ( 'Found a value error!' ) print ( repr ( e )) exit () except ZeroDivisionError as e : print ( 'Found a division by zero error!' ) print ( repr ( e )) exit () except KeyError as e : print ( 'Found a key error!' ) print ( repr ( e )) exit () Alternatively if we really did want to handle all of those exceptions in the same way we should still not use the generic Exception class. The reason for this is that if someone reads our code it is very unclear what kinds of errors the code can create, it is better to list all the errors as a tuple to provide a form of documentation like so: try : # Some problematic code that could raise different kinds of exceptions except ( ValueError , ZeroDivisionError , KeyError ) as e : print ( 'Found an error!' ) print ( repr ( e )) exit ()","title":"Error handling in Python"},{"location":"training_resources/python/logging-and-error-handling/#error-handling-dos-and-donts","text":"","title":"Error handling dos and don'ts"},{"location":"training_resources/python/logging-and-error-handling/#dont-raise-a-generic-exception","text":"Raising a generic exception is bad practice: raise Exception ( 'Error I encountered' ) The problem with raising a generic exception like this is that it is difficult to catch correctly. For example: a = 1 b = 0 try : a / b raise Exception ( 'Error I encountered' ) except Exception as e : print ( 'Caught error with message: ' + repr ( e )) While we would expect our code to catch our raised exception and print 'Caught error with message: Error I encountered', it would actually instead print that it encountered a ZeroDivisionError . The reason for this is that ZeroDivisionError is a sub/child class of Exception , it is encountered first so is caught first. There is also a flip-side to this which causes another issue when raising a generic Exception : try : raise Exception ( 'Error I encountered' ) except ValueError : exit () While child classes of Exception are caught by an except block which catches errors of type Exception the same is not true the other way around. The raised Exception will not be caught by the except block here as it is not of type ValueError . As a general rule of thumb avoid using the generic Exception class at all. It is often tempting to write code like this: try : # Some problematic code that could raise different kinds of exceptions except Exception : print ( 'Found an error!' ) exit ()","title":"Don't raise a generic exception"},{"location":"training_resources/python/logging-and-error-handling/#dont-lose-the-stack-trace","text":"The stack trace (a log of where the error occurred and how it occurred), is incredibly important and is one of the most useful features of exception objects. When an exception is created it has a stack trace. For example: def a (): return b () def b (): c () def c (): raise Exception ( 'Don \\' t call me!' ) a () The code above will obviously raise an exception. That exception will contain within it a stack trace showing that an exception was raised by function c , but it will also show the steps leading up to that: Exception <- c <- b <- a <- a() . This is very useful for debugging. There are cases when writing try/except statements where we want to catch an exception, log some message, and then raise the exception again. For example we may have a function that could create a ZeroDivisionError , inside that function we use a try/except to catch the exception and log a message about it. However, we don't want the function to return any value as the division failed, so instead we want to raise the exception again so that the code that called the function knows it failed: def divide_two_numbers ( a : float , b : float ) -> float : try : return a / b except ZeroDivisionError as e : print ( 'Division failed because of: ' + repr ( e )) raise ZeroDivisionError # In use: a = 1.0 b = 0 try : result = divide_two_numbers ( a , b ) except ZeroDivisionError : print ( 'Division failed can \\' t continue' ) exit () This is wrong . The specific place this is wrong is here: raise ZeroDivisionError Doing this raises a new ZeroDivisionError, which loses the stack trace of the original raised exception. Instead to re-raise the exception do the following: except ZeroDivisionError : # Do stuff raise","title":"Don't lose the stack trace"},{"location":"training_resources/python/logging-and-error-handling/#dont-let-the-program-continue-if-it-cant","text":"It is a common mistake to view try/catch statements as a way of 'fixing' or avoiding an error. However, often an exception does signal that the program needs to terminate. The pseudocode below is an example of a common mistake pattern: my_file = None try : my_file = open_file ( path_to_my_file ) except : print ( 'Couldn \\' t find the file!' ) print ( my_file . contents ()) This is a common case where just because an exception has been caught, that doesn't mean the error has been handled. The potential error is the file not being found, the try/catch acknowledges this but then the program moves on as if nothing has happened. This means that inevitablely the final print statement will produce another error as it tries to use a None value. Instead the following should have been done: my_file = None try : my_file = open_file ( path_to_my_file ) except : print ( 'Couldn \\' t find the file!' ) exit () print ( my_file . contents ()) This is a very basic example but this topic is well covered by the official python error handling tutorial and so you can look there for an in-depth summary. There is a balance to be found in applying both logging and error handling. For most code you don't need or want everything wrapped up like this - it becomes messy. Pay attention to places where your code interfaces with anything external.","title":"Don't let the program continue if it can't"},{"location":"training_resources/python/logging-and-error-handling/#catching-errors-in-pyspark","text":"See our guide on logging and error handling in PySpark","title":"Catching errors in PySpark"},{"location":"training_resources/python/logging-and-error-handling/#further-reading","text":"Logging Cookbook Logging HowTo Logging in Python General guide on errors and error handling Page which provides a large list of errors that can be encountered","title":"Further reading"},{"location":"training_resources/python/project-structure-and-packaging/","text":"Project and Package structuring A python package is a way to bundle your code into a single thing that can be shared and reused. If our goal is to be able to share and reuse code across NHS Digital and externally then there are many benefits to packaging code: Shareable : The most important reason to use packages is that it is the way to share python code. Not using packages runs the risk that other people will not be able to run your code... \"It works fine on my machine\". Databricks : It looks likely that packaging code will be the easiest way to get your code onto databricks. (NB: we are keeping a close eye on Data Refinery to answer this question) Reliability : Packages go a long way to ensuring that other people in your team can open your code and run it without hitting issues. Organised : Packages allow you to organise your code in logical sections. This makes it much easier to test and to maintain over time. Predictable : since all packages follow a similar structure, other analysts will be able to quickly understand how you have organised your code. No more hunting through a random directory structure trying to figure out where everything is kept. This is a tricky topic at first so we recommend asking for some support when you set this up for the first time. How to package python code In order to package your code, you just need to follow the standard templates for python projects. I describe this briefly here but provide links to the comprehensive official guidance below. Here is an outline of a very basic python project for the Smoking, Drinking, and Drugs publication (SDD): SDD/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 sdd/ | \u251c\u2500\u2500 __init__.py | \u251c\u2500\u2500 main.py \u2502 \u2514\u2500\u2500 example_package/ \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 example.py \u2514\u2500\u2500 tests/ Note that the whole directory is called SDD but we also have a sub-folder called sdd . All of our code lives inside this sdd sub-folder. Sometimes you might see the folder that contains all the code called src but the other common convention is to name it the same as the overall project. The README.md file is extremely important as this functions as the package\u2019s landing page. The README provides a bird's eye view of the whole package. You should treat it as the first thing a new starter might read when trying to understand your code. It might contain an overview of the code, details of inputs/outputs, how to install package, ownership, contributing and licence info. The random-seeming files (setup.py, requirements.txt, etc.) are all involved in dependency management. Your package will depend on certain things being in place in order to run - these files manage those dependencies. Be aware that dependency management is a difficult topic in python and other languages and so over the years many different approaches have fallen in and out of favour. Here, we aim to keep it as simple as possible while still achieving the goal of producing robust, shareable code. Also note the funny looking __init__.py . This file tells python that this code is part of a package. With this file in place you can import functions from the different parts of your code. E.g. from sdd.example_package.example import my_function . Again - you can learn more about this in the links below. Here is another for the diabetes publication so you can see the pattern. We have a more elaborate version of this below. diabetes/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 diabetes/ | \u251c\u2500\u2500 __init__.py | \u251c\u2500\u2500 main.py \u2502 \u2514\u2500\u2500 example_package/ \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 example.py \u2514\u2500\u2500 tests/ Generic package template To help get you started, we have created a generic package structure that you can adapt for your own purposes. The repository is stored on Github . You can fork this repo and use it as a starting point for your own package. So what now? Make your own package either by adding the files yourself or by cloning the [generic template] that we provide. Once you have the package structure in place, you can install that package on your machine using these two commands: pip install - e . pip install - r requirements . txt Once the package is installed it will be able to identify all the modules inside your code. If you have populated the setup.py and requirements.txt , python will install all of the bits that your code needs in order to run. More importantly, other people can install finished your package from gitlab using pip install git+path_to_my_repo . For example pip install git+https://<domain>/<path>/diabetes_rap Even better, you can bundle all of your code into a single file called a wheel (.whl). This wheel can be shared, stored in an online repository for others to use, or installed into databricks . Adapting package structure for analytical work Every python project should follow the standard package structure to help ensure portability and reliability. Nevertheless, there is scope to adapt this structure to fit the workflow of specific projects. The cookie cutter data science template shows how you can include folders for output data, validation reports, figures, etc. The figure below shows how we have applied this structure to the National Diabetes Audit code. diabetes \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 setup.py \u251c\u2500\u2500\u2500diabetes \u2502 \u251c\u2500\u2500 create_publication.py \u2502 \u251c\u2500\u2500 params.py \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500utilities \u2502 \u251c\u2500\u2500 data_connections.py \u2502 \u251c\u2500\u2500 field_definitions.py \u2502 \u251c\u2500\u2500 processing_steps.py \u2502 \u251c\u2500\u2500 __init__.py | \u251c\u2500\u2500\u2500reports \u2502 \u251c\u2500\u2500\u2500input_profile \u2502 \u2514\u2500\u2500\u2500output_profile \u2502 \u2514\u2500\u2500\u2500tests \u251c\u2500\u2500\u2500unittests \u2502 \u2502 test_data_connections.py \u2502 \u2502 test_field_definitions.py \u2502 \u2502 test_processing_steps.py Some things to notice about this structure: All of the actual code lives inside the diabetes_code directory. Everything else at the top level has to do with packaging and testing the code. In the diabetes_code repository there are two files: create_publication.py and params.py . These top level files are the highest level of abstraction and should be the main place where users interact with the code. The params.py file contains all of the parameters that we expect to change frequently, e.g. input data. The create_publication.py file organises the steps in a simple, easy-to-understand manner that should be readable by anyone, even if they don't know python. In this way, we aim to reduce risk by make the code accessible to new staff. The next level down contains the meaty parts of the code. By organising the code into logical sections, we make it easier to understand but also to maintain and test. Moreover, tucking the complex code out of the way means that users don't need to understand everything about the code all at once. The data_connections.py file handles reading data in and writing data back out. Since we know that this code will have to migrate to Data Refinery soon, it makes sense to have an interface here. The plan is that when we move to Data Refinery, this should be the only code we need to change. The field_definitions.py file contains the definitions for each of the fields (columns) derived in the process. By abstracting these definitions out of the code and making them reusable, we achieve some great benefits. First, it becomes much easier to maintain. When the specifications change next year, we only need to make the change in one location. Next, it becomes much easier to test. We write unit tests for each of these definitions and can then reuse these definitions in many places without increasing risk. The processing_steps.py file contains the core business logic of the diabetes data. We could consider breaking this down into further steps. Note that we never store passwords or any sensitive credentials in the repo to prevent the situation where it can mistakenly committed into the git. There are several ways to deal with the secret, keys and passwords such as using Git Hooks or final cleansing process before publishing. External links This is a really big topic and we don't want to replicate material that you can find elsewhere. Here are links to some resources that will give you as much detail as you want on this topic. Packaging your Python project Packages and modules from the official docs . Good for understanding on how to organise and import sub-packages. Why we share code as a .whl file","title":"Project structure and packaging"},{"location":"training_resources/python/project-structure-and-packaging/#project-and-package-structuring","text":"A python package is a way to bundle your code into a single thing that can be shared and reused. If our goal is to be able to share and reuse code across NHS Digital and externally then there are many benefits to packaging code: Shareable : The most important reason to use packages is that it is the way to share python code. Not using packages runs the risk that other people will not be able to run your code... \"It works fine on my machine\". Databricks : It looks likely that packaging code will be the easiest way to get your code onto databricks. (NB: we are keeping a close eye on Data Refinery to answer this question) Reliability : Packages go a long way to ensuring that other people in your team can open your code and run it without hitting issues. Organised : Packages allow you to organise your code in logical sections. This makes it much easier to test and to maintain over time. Predictable : since all packages follow a similar structure, other analysts will be able to quickly understand how you have organised your code. No more hunting through a random directory structure trying to figure out where everything is kept. This is a tricky topic at first so we recommend asking for some support when you set this up for the first time.","title":"Project and Package structuring"},{"location":"training_resources/python/project-structure-and-packaging/#how-to-package-python-code","text":"In order to package your code, you just need to follow the standard templates for python projects. I describe this briefly here but provide links to the comprehensive official guidance below. Here is an outline of a very basic python project for the Smoking, Drinking, and Drugs publication (SDD): SDD/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 sdd/ | \u251c\u2500\u2500 __init__.py | \u251c\u2500\u2500 main.py \u2502 \u2514\u2500\u2500 example_package/ \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 example.py \u2514\u2500\u2500 tests/ Note that the whole directory is called SDD but we also have a sub-folder called sdd . All of our code lives inside this sdd sub-folder. Sometimes you might see the folder that contains all the code called src but the other common convention is to name it the same as the overall project. The README.md file is extremely important as this functions as the package\u2019s landing page. The README provides a bird's eye view of the whole package. You should treat it as the first thing a new starter might read when trying to understand your code. It might contain an overview of the code, details of inputs/outputs, how to install package, ownership, contributing and licence info. The random-seeming files (setup.py, requirements.txt, etc.) are all involved in dependency management. Your package will depend on certain things being in place in order to run - these files manage those dependencies. Be aware that dependency management is a difficult topic in python and other languages and so over the years many different approaches have fallen in and out of favour. Here, we aim to keep it as simple as possible while still achieving the goal of producing robust, shareable code. Also note the funny looking __init__.py . This file tells python that this code is part of a package. With this file in place you can import functions from the different parts of your code. E.g. from sdd.example_package.example import my_function . Again - you can learn more about this in the links below. Here is another for the diabetes publication so you can see the pattern. We have a more elaborate version of this below. diabetes/ \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 diabetes/ | \u251c\u2500\u2500 __init__.py | \u251c\u2500\u2500 main.py \u2502 \u2514\u2500\u2500 example_package/ \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 example.py \u2514\u2500\u2500 tests/","title":"How to package python code"},{"location":"training_resources/python/project-structure-and-packaging/#generic-package-template","text":"To help get you started, we have created a generic package structure that you can adapt for your own purposes. The repository is stored on Github . You can fork this repo and use it as a starting point for your own package.","title":"Generic package template"},{"location":"training_resources/python/project-structure-and-packaging/#so-what-now","text":"Make your own package either by adding the files yourself or by cloning the [generic template] that we provide. Once you have the package structure in place, you can install that package on your machine using these two commands: pip install - e . pip install - r requirements . txt Once the package is installed it will be able to identify all the modules inside your code. If you have populated the setup.py and requirements.txt , python will install all of the bits that your code needs in order to run. More importantly, other people can install finished your package from gitlab using pip install git+path_to_my_repo . For example pip install git+https://<domain>/<path>/diabetes_rap Even better, you can bundle all of your code into a single file called a wheel (.whl). This wheel can be shared, stored in an online repository for others to use, or installed into databricks .","title":"So what now?"},{"location":"training_resources/python/project-structure-and-packaging/#adapting-package-structure-for-analytical-work","text":"Every python project should follow the standard package structure to help ensure portability and reliability. Nevertheless, there is scope to adapt this structure to fit the workflow of specific projects. The cookie cutter data science template shows how you can include folders for output data, validation reports, figures, etc. The figure below shows how we have applied this structure to the National Diabetes Audit code. diabetes \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 setup.py \u251c\u2500\u2500\u2500diabetes \u2502 \u251c\u2500\u2500 create_publication.py \u2502 \u251c\u2500\u2500 params.py \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500utilities \u2502 \u251c\u2500\u2500 data_connections.py \u2502 \u251c\u2500\u2500 field_definitions.py \u2502 \u251c\u2500\u2500 processing_steps.py \u2502 \u251c\u2500\u2500 __init__.py | \u251c\u2500\u2500\u2500reports \u2502 \u251c\u2500\u2500\u2500input_profile \u2502 \u2514\u2500\u2500\u2500output_profile \u2502 \u2514\u2500\u2500\u2500tests \u251c\u2500\u2500\u2500unittests \u2502 \u2502 test_data_connections.py \u2502 \u2502 test_field_definitions.py \u2502 \u2502 test_processing_steps.py Some things to notice about this structure: All of the actual code lives inside the diabetes_code directory. Everything else at the top level has to do with packaging and testing the code. In the diabetes_code repository there are two files: create_publication.py and params.py . These top level files are the highest level of abstraction and should be the main place where users interact with the code. The params.py file contains all of the parameters that we expect to change frequently, e.g. input data. The create_publication.py file organises the steps in a simple, easy-to-understand manner that should be readable by anyone, even if they don't know python. In this way, we aim to reduce risk by make the code accessible to new staff. The next level down contains the meaty parts of the code. By organising the code into logical sections, we make it easier to understand but also to maintain and test. Moreover, tucking the complex code out of the way means that users don't need to understand everything about the code all at once. The data_connections.py file handles reading data in and writing data back out. Since we know that this code will have to migrate to Data Refinery soon, it makes sense to have an interface here. The plan is that when we move to Data Refinery, this should be the only code we need to change. The field_definitions.py file contains the definitions for each of the fields (columns) derived in the process. By abstracting these definitions out of the code and making them reusable, we achieve some great benefits. First, it becomes much easier to maintain. When the specifications change next year, we only need to make the change in one location. Next, it becomes much easier to test. We write unit tests for each of these definitions and can then reuse these definitions in many places without increasing risk. The processing_steps.py file contains the core business logic of the diabetes data. We could consider breaking this down into further steps. Note that we never store passwords or any sensitive credentials in the repo to prevent the situation where it can mistakenly committed into the git. There are several ways to deal with the secret, keys and passwords such as using Git Hooks or final cleansing process before publishing.","title":"Adapting package structure for analytical work"},{"location":"training_resources/python/project-structure-and-packaging/#external-links","text":"This is a really big topic and we don't want to replicate material that you can find elsewhere. Here are links to some resources that will give you as much detail as you want on this topic. Packaging your Python project Packages and modules from the official docs . Good for understanding on how to organise and import sub-packages. Why we share code as a .whl file","title":"External links"},{"location":"training_resources/python/python-functions/","text":"Python functions We are currently updating our minimal Python RAP package template, which is freely available to use via Github: RAP package template . A function is a block of organised code that is reusable, and is used to perform a single defined action. Functions can take in multiple values (arguments) and can also return a value after executing. The code of the function only runs when it is called. Features of a function In Python, the standard library provides many useful built-in function such as len(), print(), str(), int(), however the programmer can define their own function to suit their particular needs and requirements. Python functions are defined by using the def keyword. Here is an example of a python function which converts the argument temp which is a value representing a temperature in Fahrenheit, into Celsius: # Define the function def fahrenheit_to_celsius ( temp ): new_temp = ( temp - 32 ) * ( 5 / 9 ) return new_temp # Use the function fahrenheit_to_celsius ( 77 ) >>> 25.0 It's also a common convention to return a Python expression directly, instead of assigning it to a variable. In previous example, we can re-write the same function to be more concise: def fahrenheit_to_celsius ( temp ): return ( temp - 32 ) * ( 5 / 9 ) If you don't specify anything to be returned, your function will return None. In this example we can see all the features of a function: The name of the function is fahrenheit_to_celsius() (you should use snake_case here) The function has one argument temp . Function arguments in python are listed between the parentheses after the function name. Additional function arguments are separated by commas. The code body calculates the value of the input Fahrenheit temperature in Celsius. The function returns the result of this calculation. This is done using the return keyword in python. When the program hits the return keyword it immediately returns the value specified after the keyword and exits the function. If the function doesn't return anything (which it does not need to) then the function is exited when all the code in the function body has been executed. When calling a function, you need to supply argument inputs for all parameters, otherwise, the Python interpreter will raise a TypeError. Remember to supply the arguments in the same order that the parameters are appeared so that it can be assigned correctly. Why use functions? There are several benefits to using functions, we will discuss the following here: They cut down on repetitive code by allowing you to call previously written blocks of code. They allow you to break down a process into steps making code more readable. The use of variable arguments allows functions to adapt better to changing needs. They make testing easier. They provide additional means of documentation. Re-usability Functions allow us to cut down on code repetition, which makes code harder to read and understand. In general, if you need to write the same code pattern more than twice then you should put some time into writing a function for later usage. When you need to change part of this code chunks, it will be more convenience to do it once inside a function instead of changing many places in your code base. For example, we take the following function to implement bubble sort. A sorting algorithm aims to take in an array of numbers and sort them in increasing order. The bubble sort compares each pair of adjacent elements in the input array and swaps them if they are not in the right order. def bubblesort ( arr ): n = len ( arr ) for i in range ( n - 1 ): for j in range ( 0 , n - i - 1 ): if arr [ j ] > arr [ j + 1 ] : arr [ j ], arr [ j + 1 ] = arr [ j + 1 ], arr [ j ] If we had three arrays we wanted to sort (arr1, arr2, arr3) and we didn't use a function we would have to repeat our code three times. This produces messy code that is hard to read. Can you spot the errors in the code below? # Sort array 1 n = len ( arr1 ) for i in range ( n - 1 ): for j in range ( 0 , n - i - 1 ): if arr1 [ j ] > arr1 [ j + 1 ] : arr1 [ j ], arr1 [ j + 1 ] = arr1 [ j + 1 ], arr1 [ j ] # Sort array 2 n = len ( arr2 ) for i in range ( m - 1 ): for j in range ( 0 , n - i - 1 ): if arr2 [ j ] > arr2 [ j + 1 ] : arr2 [ i ], arr2 [ j + 1 ] = arr2 [ j + 1 ], arr2 [ j ] # Sort array 3 n = len ( arr3 ) for j in range ( n - 1 ): for i in range ( 0 , n - i - 1 ): if arr3 [ j ] > arr3 [ j + 1 ] : arr3 [ j ], arr3 [ j + 1 ] = arr3 [ j + 1 ], arr3 [ i ] Writing repetitive non-functional code is bad practice. Copy pasting processes like this and changing variable names can result in mistakes and will make a project excessively verbose. By using the defined function before, all of these issues are fixed with no functionality lost: # Sort array 1 bubblesort ( arr = arr1 ) # Sort array 2 bubblesort ( arr = arr2 ) # Sort array 3 bubblesort ( arr = arr3 ) Re-usability is not the only benefit of functions, and just because a process is only done once does not mean that it should not be placed inside the body of a function instead. In the SQL code for computing \"Cholesterol Numerator\", the code chunk below was repeated multiple times. When transforming to RAP, we define a PySpark function once and reuse the code many times. CASU_DIABETES .[ dbo ].[ fnc_suppression_rm ]( SUM ( CASE WHEN CHOLESTEROL_CP = 1 AND AGE >= 12 THEN 1 ELSE 0 END ), 'count' ) AS [ Cholesterol Numerator ] transform to: cholesterol_numerator = ( F . when (( F . col ( 'AGE' ) >= 12 ) & ( F . col ( 'CHOL_DATE_FLAG' ) == 1 ), 1 ) . otherwise ( 0 ) ) Breaking down a process into logical steps Using functions can better elucidate the steps of a process. A long process typically has several logical steps to it each of which can be delivered by a block of code. If we attempt to write out all the steps of the process in a long sequence of code this will be very difficult to read, and identifying the separate steps of the process is difficult. Below is an example from the diabetes RAP that uses functions, functions aren't being used here to cut down on code re-use but rather to make the steps of the delivered process more clear: # Step 3: Identify the best record for each person best_record = identify_best_record ( record_scores = record_scores ) # Step 4: Use the best record to cut down the record_scores table, creating the golden record golden_record = create_golden_record_table ( record_scores = record_scores , best_record = best_record ) # Step 5: Derive additional fields for the golden record table enriched_golden_record = enrich_golden_record ( golden_record = golden_record , hes_diabetes = hes_diabetes , imd_scores = imd_scores ) # Step 6: Output the golden record table to SQL so we can avoid recalculating next time write_df_to_SQL ( df_to_write = enriched_golden_record , target_table = golden_record_sql , mode = 'overwrite' , database = params [ 'work_db' ]) # Step 7: after attain the golden_record (either by saved table or generate new one), we build final table final_table = produce_aggregates ( golden_record ) It is important to note that overuse of functions like this can be a bad thing. Breaking down a process into too many functions will make code harder to read. An extreme example can demonstrate this: def sort_each_element ( arr , n ): for i in range ( n - 1 ): do_swaps ( arr = arr , i = i , n = n ) def do_swaps ( arr , i , n ): for j in range ( 0 , n - i - 1 ): if arr [ j ] > arr [ j + 1 ]: swap ( arr = arr , j = j ) def swap ( arr , j ): arr [ j ], arr [ j + 1 ] = arr [ j + 1 ], arr [ j ] def bubblesort ( arr ): n = len ( arr ) sort_each_element ( arr , n ) This code delivers a function bubblesort which has the same functionality as the previous bubblesort example. The difference is that in this example the logical steps of a bubble sort have been broken down into far too many functions, and instead of having a positive effect the code is now harder to read as each function needs to be understood. Arguments Another benefit is that using functions allows us to adapt to new changes through the use of arguments. For example lets say in our initial bubble sort example it was decided that calling the arrays arr1 , arr2 , and arr3 was not descriptive enough. The lead on the project decides that instead these arrays should be renamed to array1 , array2 , and array3 . Without using a function we would have to change every occurrence of these variables in our long messy code: # Sort array 1 n = len ( array1 ) for i in range ( n - 1 ): for j in range ( 0 , n - i - 1 ): if array1 [ j ] > array1 [ j + 1 ] : array1 [ j ], array1 [ j + 1 ] = array1 [ j + 1 ], array1 [ j ] # Sort array 2 n = len ( array2 ) for i in range ( n - 1 ): for j in range ( 0 , n - i - 1 ): if array2 [ j ] > array2 [ j + 1 ] : array2 [ j ], array2 [ j + 1 ] = array2 [ j + 1 ], array2 [ j ] # Sort array 3 n = len ( array3 ) for i in range ( n - 1 ): for j in range ( 0 , n - i - 1 ): if array3 [ j ] > array3 [ j + 1 ] : array3 [ j ], array3 [ j + 1 ] = array3 [ j + 1 ], array3 [ j ] This is bad practice. It is time consuming and could also result in mistakes. If instead we used a defined function we would only have to change the name of the variable passed as an argument: # Sort array 1 bubblesort ( arr = array1 ) # Sort array 2 bubblesort ( arr = array2 ) # Sort array 3 bubblesort ( arr = array3 ) Another example could be seen in the diabetes RAP. Lets say that in step 5 we instead want to use some new_hes_diabetes data, we can simply update that one argument without having to rewrite the whole logical step of the process: # OLD Step 5: Derive additional fields for the golden record table golden_record = enrich_golden_record ( golden_record = golden_record , hes_diabetes = hes_diabetes , imd_scores = imd_scores ) # NEW Step 5: Derive additional fields for the golden record table golden_record = enrich_golden_record ( golden_record = golden_record , # change: hes_diabetes = new_hes_diabetes , imd_scores = imd_scores ) So another benefit of functions is that it makes code more maintainable and future-proofed. Without using functions a change in project requirements could mean changing hundreds of lines of code. However, if we use functions and setup our arguments correctly we can simply change what is passed to the functions in the process. Testing Another benefit of using functions is that they aid in testing the project. Functions typically deliver a small unit of functionality that the program needs, for example the program might need to be able to convert Fahrenheit to Celsius. By putting this in a function we can test that function with different input values to see if it produces an expected output. If the function succeeds then we know the program has that unit of functionality; this is called unit testing. Documentation By breaking down processing steps into functions we can individually document each of these functions, whereas if we instead had a long sequence of code the best documentation that is possible are simple code comments throughout. The next section goes into detail on documenting functions and the benefits of doing so. Python docstrings make it possible to document your defined python functions, and describe their functionality. Here is an example of a well-documented python function: def fahrenheit_to_celsius ( temp : float ) -> float : \"\"\" Converts temperatures in Fahrenheit to Celsius. Takes the input temperature (in Fahrenheit), calculates the value of the same temperature in Celsius, and then returns this value. Args: temp: a float value representing a temperature in Fahrenheit. Returns: The input temperature converted into Celsius Example: fahrenheit_to_celsius(temp=77) >>> 25.0 \"\"\" new_temp = ( temp - 32 ) * ( 5 / 9 ) return new_temp The docstring occurs underneath the initial definition of the function. The docstring occurs between a set of two triple quotation marks ( \"\"\" ). The first thing that occurs in the docstring should be a description of what the function does when invoked, it may also contain additional documentation/notes about the design of the function, considerations that had to be made, use case of the function etc. Try and describe the function in one short line (80 characters), any other details about the function description should be written after a blank line. After the general goal of the function has been described, a section should be given to describe what each of the input arguments are, what the return value is (if there is one), and also an example use-case of the calling the function. There are several styles for docstrings, the example above uses the Google style. There are several key benefits to this kind of documentation that make it worthwhile: It allows you to return to code that you have written previously and be quickly reminded what the function does. It makes it easy for team members to understand the purpose of the function. While they can read through the code to understand the logical steps, the docstring can provide additional context and considerations. It makes it easy for new team members to come on board. Reading through many lines of code can be exhausting, reading docstrings very quickly conveys the intended functionality that the function delivers. When using an IDE with coding hints team members can quickly see a description of what the function does, which saves them from having to read through all the code of the function in order to understand what it delivers. Example: There are also some additional kinds of documentation that can be done on functions which will not be discussed here, such as type hints and annotations. Once you have multiple function in your project, you will need to organise them. This can be done by creating their own module so that the functions can be imported into other part of your code. You can also generate methods for classes, these topics are beyond the scope of this wiki for now but it is worth to learn more when you are getting used to Python language. Coding challenge Here are some questions to help you practice writing functions. Challenge 1. Multiplier We are designing a function which returns the result of multiplying 5 by 5. What is the missing keyword below to define a function: ... multiply (): In the code body we need to calculate the value of our calculation. Can you fill in the calculation: def multiply (): value = ... Currently our function only calculates this value, but we have no way of obtaining the result, fill out the last line of the function body so if we call the function we can get the result: def multiply (): value = 5 * 5 ... Our function only serves a very simply basic purpose. It would be better if our function could return the result of the multiplication of any two values. Fill in the gaps so our function now uses arguments: def multiply ( ... , ... ): value = a * b return value Our function now serves its purpose, but we may want to document it so in future other programmers can understand it. Add in a docstring with a single line describing the function: def multiply ( a , b ): ... value = a * b return value Expand on the docstring by describing its parameters: def multiply ( a , b ): \"\"\" Multiplies two values. ... \"\"\" value = a * b return value Expand on the docstring by describing the return value: def multiply ( a , b ): \"\"\" Multiplies two values. Args: a: first value in the calculation. b: second value in the calculation ... \"\"\" value = a * b return value Expand on the docstring by giving an example use of calling the function: def multiply ( a , b ): \"\"\" Multiplies two values. Args: a: first value in the calculation. b: second value in the calculation Returns: The result of multiplying a by b ... \"\"\" value = a * b return value Challenge 2. Interest Calculator The following function returns the amount of money you have after 10 years with 5% interest applied once a year with an initial amount of \u00a31000: def interest (): return math . pow ( 1000 * ( 1 + ( 0.005 / 1 )), 10 ) We can change the number of times the interest applied each year to be an argument in the function called n: def interest ( n ): return math . pow ( 1000 * ( 1 + ( 0.005 / n )), 10 ) Adapt the function by filling in the gaps to allow the interest rate (r) to be changed: def interest ( n , ... ): return math . pow ( 1000 * ( 1 + ( .../ n )), 10 ) Adapt the function to allow the years (t) to be changed: def interest ( n , ... ): return math . pow ( 1000 * ( 1 + ( 0.005 / n )), ... ) Adapt the function to allow the initial amount of money (p) to be changed: def interest ( n , ... ): return math . pow ( ... * ( 1 + ( 0.005 / n )), 10 ) Combine all these steps the define a function which calculates interest: def interest ( n , ... ): return math . pow ( ... ) External links This wiki page is focused on the use cases for functions rather than a general guide on writing python functions. For more general information on writing functions in python see the following: General guide which also discusses recursion General guide which also discusses scope","title":"Python functions"},{"location":"training_resources/python/python-functions/#python-functions","text":"We are currently updating our minimal Python RAP package template, which is freely available to use via Github: RAP package template . A function is a block of organised code that is reusable, and is used to perform a single defined action. Functions can take in multiple values (arguments) and can also return a value after executing. The code of the function only runs when it is called.","title":"Python functions"},{"location":"training_resources/python/python-functions/#features-of-a-function","text":"In Python, the standard library provides many useful built-in function such as len(), print(), str(), int(), however the programmer can define their own function to suit their particular needs and requirements. Python functions are defined by using the def keyword. Here is an example of a python function which converts the argument temp which is a value representing a temperature in Fahrenheit, into Celsius: # Define the function def fahrenheit_to_celsius ( temp ): new_temp = ( temp - 32 ) * ( 5 / 9 ) return new_temp # Use the function fahrenheit_to_celsius ( 77 ) >>> 25.0 It's also a common convention to return a Python expression directly, instead of assigning it to a variable. In previous example, we can re-write the same function to be more concise: def fahrenheit_to_celsius ( temp ): return ( temp - 32 ) * ( 5 / 9 ) If you don't specify anything to be returned, your function will return None. In this example we can see all the features of a function: The name of the function is fahrenheit_to_celsius() (you should use snake_case here) The function has one argument temp . Function arguments in python are listed between the parentheses after the function name. Additional function arguments are separated by commas. The code body calculates the value of the input Fahrenheit temperature in Celsius. The function returns the result of this calculation. This is done using the return keyword in python. When the program hits the return keyword it immediately returns the value specified after the keyword and exits the function. If the function doesn't return anything (which it does not need to) then the function is exited when all the code in the function body has been executed. When calling a function, you need to supply argument inputs for all parameters, otherwise, the Python interpreter will raise a TypeError. Remember to supply the arguments in the same order that the parameters are appeared so that it can be assigned correctly.","title":"Features of a function"},{"location":"training_resources/python/python-functions/#why-use-functions","text":"There are several benefits to using functions, we will discuss the following here: They cut down on repetitive code by allowing you to call previously written blocks of code. They allow you to break down a process into steps making code more readable. The use of variable arguments allows functions to adapt better to changing needs. They make testing easier. They provide additional means of documentation.","title":"Why use functions?"},{"location":"training_resources/python/python-functions/#re-usability","text":"Functions allow us to cut down on code repetition, which makes code harder to read and understand. In general, if you need to write the same code pattern more than twice then you should put some time into writing a function for later usage. When you need to change part of this code chunks, it will be more convenience to do it once inside a function instead of changing many places in your code base. For example, we take the following function to implement bubble sort. A sorting algorithm aims to take in an array of numbers and sort them in increasing order. The bubble sort compares each pair of adjacent elements in the input array and swaps them if they are not in the right order. def bubblesort ( arr ): n = len ( arr ) for i in range ( n - 1 ): for j in range ( 0 , n - i - 1 ): if arr [ j ] > arr [ j + 1 ] : arr [ j ], arr [ j + 1 ] = arr [ j + 1 ], arr [ j ] If we had three arrays we wanted to sort (arr1, arr2, arr3) and we didn't use a function we would have to repeat our code three times. This produces messy code that is hard to read. Can you spot the errors in the code below? # Sort array 1 n = len ( arr1 ) for i in range ( n - 1 ): for j in range ( 0 , n - i - 1 ): if arr1 [ j ] > arr1 [ j + 1 ] : arr1 [ j ], arr1 [ j + 1 ] = arr1 [ j + 1 ], arr1 [ j ] # Sort array 2 n = len ( arr2 ) for i in range ( m - 1 ): for j in range ( 0 , n - i - 1 ): if arr2 [ j ] > arr2 [ j + 1 ] : arr2 [ i ], arr2 [ j + 1 ] = arr2 [ j + 1 ], arr2 [ j ] # Sort array 3 n = len ( arr3 ) for j in range ( n - 1 ): for i in range ( 0 , n - i - 1 ): if arr3 [ j ] > arr3 [ j + 1 ] : arr3 [ j ], arr3 [ j + 1 ] = arr3 [ j + 1 ], arr3 [ i ] Writing repetitive non-functional code is bad practice. Copy pasting processes like this and changing variable names can result in mistakes and will make a project excessively verbose. By using the defined function before, all of these issues are fixed with no functionality lost: # Sort array 1 bubblesort ( arr = arr1 ) # Sort array 2 bubblesort ( arr = arr2 ) # Sort array 3 bubblesort ( arr = arr3 ) Re-usability is not the only benefit of functions, and just because a process is only done once does not mean that it should not be placed inside the body of a function instead. In the SQL code for computing \"Cholesterol Numerator\", the code chunk below was repeated multiple times. When transforming to RAP, we define a PySpark function once and reuse the code many times. CASU_DIABETES .[ dbo ].[ fnc_suppression_rm ]( SUM ( CASE WHEN CHOLESTEROL_CP = 1 AND AGE >= 12 THEN 1 ELSE 0 END ), 'count' ) AS [ Cholesterol Numerator ] transform to: cholesterol_numerator = ( F . when (( F . col ( 'AGE' ) >= 12 ) & ( F . col ( 'CHOL_DATE_FLAG' ) == 1 ), 1 ) . otherwise ( 0 ) )","title":"Re-usability"},{"location":"training_resources/python/python-functions/#breaking-down-a-process-into-logical-steps","text":"Using functions can better elucidate the steps of a process. A long process typically has several logical steps to it each of which can be delivered by a block of code. If we attempt to write out all the steps of the process in a long sequence of code this will be very difficult to read, and identifying the separate steps of the process is difficult. Below is an example from the diabetes RAP that uses functions, functions aren't being used here to cut down on code re-use but rather to make the steps of the delivered process more clear: # Step 3: Identify the best record for each person best_record = identify_best_record ( record_scores = record_scores ) # Step 4: Use the best record to cut down the record_scores table, creating the golden record golden_record = create_golden_record_table ( record_scores = record_scores , best_record = best_record ) # Step 5: Derive additional fields for the golden record table enriched_golden_record = enrich_golden_record ( golden_record = golden_record , hes_diabetes = hes_diabetes , imd_scores = imd_scores ) # Step 6: Output the golden record table to SQL so we can avoid recalculating next time write_df_to_SQL ( df_to_write = enriched_golden_record , target_table = golden_record_sql , mode = 'overwrite' , database = params [ 'work_db' ]) # Step 7: after attain the golden_record (either by saved table or generate new one), we build final table final_table = produce_aggregates ( golden_record ) It is important to note that overuse of functions like this can be a bad thing. Breaking down a process into too many functions will make code harder to read. An extreme example can demonstrate this: def sort_each_element ( arr , n ): for i in range ( n - 1 ): do_swaps ( arr = arr , i = i , n = n ) def do_swaps ( arr , i , n ): for j in range ( 0 , n - i - 1 ): if arr [ j ] > arr [ j + 1 ]: swap ( arr = arr , j = j ) def swap ( arr , j ): arr [ j ], arr [ j + 1 ] = arr [ j + 1 ], arr [ j ] def bubblesort ( arr ): n = len ( arr ) sort_each_element ( arr , n ) This code delivers a function bubblesort which has the same functionality as the previous bubblesort example. The difference is that in this example the logical steps of a bubble sort have been broken down into far too many functions, and instead of having a positive effect the code is now harder to read as each function needs to be understood.","title":"Breaking down a process into logical steps"},{"location":"training_resources/python/python-functions/#arguments","text":"Another benefit is that using functions allows us to adapt to new changes through the use of arguments. For example lets say in our initial bubble sort example it was decided that calling the arrays arr1 , arr2 , and arr3 was not descriptive enough. The lead on the project decides that instead these arrays should be renamed to array1 , array2 , and array3 . Without using a function we would have to change every occurrence of these variables in our long messy code: # Sort array 1 n = len ( array1 ) for i in range ( n - 1 ): for j in range ( 0 , n - i - 1 ): if array1 [ j ] > array1 [ j + 1 ] : array1 [ j ], array1 [ j + 1 ] = array1 [ j + 1 ], array1 [ j ] # Sort array 2 n = len ( array2 ) for i in range ( n - 1 ): for j in range ( 0 , n - i - 1 ): if array2 [ j ] > array2 [ j + 1 ] : array2 [ j ], array2 [ j + 1 ] = array2 [ j + 1 ], array2 [ j ] # Sort array 3 n = len ( array3 ) for i in range ( n - 1 ): for j in range ( 0 , n - i - 1 ): if array3 [ j ] > array3 [ j + 1 ] : array3 [ j ], array3 [ j + 1 ] = array3 [ j + 1 ], array3 [ j ] This is bad practice. It is time consuming and could also result in mistakes. If instead we used a defined function we would only have to change the name of the variable passed as an argument: # Sort array 1 bubblesort ( arr = array1 ) # Sort array 2 bubblesort ( arr = array2 ) # Sort array 3 bubblesort ( arr = array3 ) Another example could be seen in the diabetes RAP. Lets say that in step 5 we instead want to use some new_hes_diabetes data, we can simply update that one argument without having to rewrite the whole logical step of the process: # OLD Step 5: Derive additional fields for the golden record table golden_record = enrich_golden_record ( golden_record = golden_record , hes_diabetes = hes_diabetes , imd_scores = imd_scores ) # NEW Step 5: Derive additional fields for the golden record table golden_record = enrich_golden_record ( golden_record = golden_record , # change: hes_diabetes = new_hes_diabetes , imd_scores = imd_scores ) So another benefit of functions is that it makes code more maintainable and future-proofed. Without using functions a change in project requirements could mean changing hundreds of lines of code. However, if we use functions and setup our arguments correctly we can simply change what is passed to the functions in the process.","title":"Arguments"},{"location":"training_resources/python/python-functions/#testing","text":"Another benefit of using functions is that they aid in testing the project. Functions typically deliver a small unit of functionality that the program needs, for example the program might need to be able to convert Fahrenheit to Celsius. By putting this in a function we can test that function with different input values to see if it produces an expected output. If the function succeeds then we know the program has that unit of functionality; this is called unit testing.","title":"Testing"},{"location":"training_resources/python/python-functions/#documentation","text":"By breaking down processing steps into functions we can individually document each of these functions, whereas if we instead had a long sequence of code the best documentation that is possible are simple code comments throughout. The next section goes into detail on documenting functions and the benefits of doing so. Python docstrings make it possible to document your defined python functions, and describe their functionality. Here is an example of a well-documented python function: def fahrenheit_to_celsius ( temp : float ) -> float : \"\"\" Converts temperatures in Fahrenheit to Celsius. Takes the input temperature (in Fahrenheit), calculates the value of the same temperature in Celsius, and then returns this value. Args: temp: a float value representing a temperature in Fahrenheit. Returns: The input temperature converted into Celsius Example: fahrenheit_to_celsius(temp=77) >>> 25.0 \"\"\" new_temp = ( temp - 32 ) * ( 5 / 9 ) return new_temp The docstring occurs underneath the initial definition of the function. The docstring occurs between a set of two triple quotation marks ( \"\"\" ). The first thing that occurs in the docstring should be a description of what the function does when invoked, it may also contain additional documentation/notes about the design of the function, considerations that had to be made, use case of the function etc. Try and describe the function in one short line (80 characters), any other details about the function description should be written after a blank line. After the general goal of the function has been described, a section should be given to describe what each of the input arguments are, what the return value is (if there is one), and also an example use-case of the calling the function. There are several styles for docstrings, the example above uses the Google style. There are several key benefits to this kind of documentation that make it worthwhile: It allows you to return to code that you have written previously and be quickly reminded what the function does. It makes it easy for team members to understand the purpose of the function. While they can read through the code to understand the logical steps, the docstring can provide additional context and considerations. It makes it easy for new team members to come on board. Reading through many lines of code can be exhausting, reading docstrings very quickly conveys the intended functionality that the function delivers. When using an IDE with coding hints team members can quickly see a description of what the function does, which saves them from having to read through all the code of the function in order to understand what it delivers. Example: There are also some additional kinds of documentation that can be done on functions which will not be discussed here, such as type hints and annotations. Once you have multiple function in your project, you will need to organise them. This can be done by creating their own module so that the functions can be imported into other part of your code. You can also generate methods for classes, these topics are beyond the scope of this wiki for now but it is worth to learn more when you are getting used to Python language.","title":"Documentation"},{"location":"training_resources/python/python-functions/#coding-challenge","text":"Here are some questions to help you practice writing functions.","title":"Coding challenge"},{"location":"training_resources/python/python-functions/#challenge-1-multiplier","text":"We are designing a function which returns the result of multiplying 5 by 5. What is the missing keyword below to define a function: ... multiply (): In the code body we need to calculate the value of our calculation. Can you fill in the calculation: def multiply (): value = ... Currently our function only calculates this value, but we have no way of obtaining the result, fill out the last line of the function body so if we call the function we can get the result: def multiply (): value = 5 * 5 ... Our function only serves a very simply basic purpose. It would be better if our function could return the result of the multiplication of any two values. Fill in the gaps so our function now uses arguments: def multiply ( ... , ... ): value = a * b return value Our function now serves its purpose, but we may want to document it so in future other programmers can understand it. Add in a docstring with a single line describing the function: def multiply ( a , b ): ... value = a * b return value Expand on the docstring by describing its parameters: def multiply ( a , b ): \"\"\" Multiplies two values. ... \"\"\" value = a * b return value Expand on the docstring by describing the return value: def multiply ( a , b ): \"\"\" Multiplies two values. Args: a: first value in the calculation. b: second value in the calculation ... \"\"\" value = a * b return value Expand on the docstring by giving an example use of calling the function: def multiply ( a , b ): \"\"\" Multiplies two values. Args: a: first value in the calculation. b: second value in the calculation Returns: The result of multiplying a by b ... \"\"\" value = a * b return value","title":"Challenge 1. Multiplier"},{"location":"training_resources/python/python-functions/#challenge-2-interest-calculator","text":"The following function returns the amount of money you have after 10 years with 5% interest applied once a year with an initial amount of \u00a31000: def interest (): return math . pow ( 1000 * ( 1 + ( 0.005 / 1 )), 10 ) We can change the number of times the interest applied each year to be an argument in the function called n: def interest ( n ): return math . pow ( 1000 * ( 1 + ( 0.005 / n )), 10 ) Adapt the function by filling in the gaps to allow the interest rate (r) to be changed: def interest ( n , ... ): return math . pow ( 1000 * ( 1 + ( .../ n )), 10 ) Adapt the function to allow the years (t) to be changed: def interest ( n , ... ): return math . pow ( 1000 * ( 1 + ( 0.005 / n )), ... ) Adapt the function to allow the initial amount of money (p) to be changed: def interest ( n , ... ): return math . pow ( ... * ( 1 + ( 0.005 / n )), 10 ) Combine all these steps the define a function which calculates interest: def interest ( n , ... ): return math . pow ( ... )","title":"Challenge 2. Interest Calculator"},{"location":"training_resources/python/python-functions/#external-links","text":"This wiki page is focused on the use cases for functions rather than a general guide on writing python functions. For more general information on writing functions in python see the following: General guide which also discusses recursion General guide which also discusses scope","title":"External links"},{"location":"training_resources/python/unit-testing-field-definitions/","text":"Unit testing field definitions This section focuses on one specific application of unit testing that is very relevant to analysts; testing the definitions of different fields (columns). One of the biggest burdens on stats teams is to maintain accurate definitions of fields over time as the specifications change. This drift in definitions is also one of the biggest sources of errors in stats publications. In the worst cases you may find that a field is defined in dozens of locations across a code-base. Each of these needs to be updated each time a change happens - leading to burden and risk. Here we show that by separating the definitions of fields from the context in which you create those fields, you can easily maintain and manage those definitions over time. Moreover, these atomic field definitions become very easy to include in unit tests. In this way you can use simple data examples to ensure your outputs are accurate. This is not a general guide to unit testing. That guide is called ' unit-testing.md '. Field definitions and how to use them As was previously mentioned analysts will typically have several planned definitions for certain fields/columns of data. What can happen with these definitions is that they become repeated throughout the code base whenever they are needed; this leads to long messy code that is hard to maintain. More specifically having these definitions in many locations leads to two issues arising if the specification for one of these definitions changes: The task of updating the old field definition with the new one in every place the old one occurred is time-consuming and annoying. Furthermore, the process of updating this definition in many locations can lead to bugs and inconsistencies which we have no way of checking for. We have no consistent way of checking if our definition is correct if it occurs in many places in the code. Instead of having field definition code in many places throughout the code base it would be preferable to have the code which defines the field in one place, and then re-use that definition. What this would mean is that if a change in specifications occurred then all we would have to do is change the code of that initial definition rather than the code in every place it is used. PySpark provides a very tidy way of doing this, which has been used in the diabetes RAP: foot_denom = ( F . when (( F . col ( 'AGE' ) >= 12 ), 1 ) . otherwise ( 0 ) ) cvd_admission = ( F . when ( F . col ( 'NHS_NUMBER' ) . isNotNull (), 1 ) . otherwise ( None ) ) no_cvd_on_statins_40_to_80 = ( F . when (( F . col ( 'CVD_ADMISSION' ) . isNull ()) & ( F . col ( 'IHD_VALUE' ) . isNull ()) & ( F . col ( 'STATIN_FLAG' ) == 1 ) & ( F . col ( 'AGE' ) >= 40 ) & ( F . col ( 'AGE' ) <= 80 ), 1 ) . otherwise ( 0 ) ) In the code above three field definitions are given: A foot_denom field which has a value of 1 if the 'AGE' column has a value of 12 or more, otherwise has a value of 0. A cvd_admission field which has a value of 1 if there is a value in the 'NHS_NUMBER' field otherwise has a null value (None). A no_cvd_on_statins_50_to_80 field which has a value of 1 if: - The 'CVD_ADMISSION' and 'IHD_VALUE' columns both have a null value. - And the 'STATIN_FLAG' column has a value of 1. - And the 'AGE' column has a value between 40 and 80 inclusive. - Otherwise no_cvd_on_statins_50_to_80 has a value of 0 With our fields defined we can now re-use them without having to write out their logic every time we do so. For example: my_dataframe . withColumn ( 'FOOT_DENOM' , foot_denom ) . withColumn ( 'CVD_ADMISSION' , cvd_admission ) . withColumn ( 'NO_CVD_ON_STATINS_50_TO_80' , no_cvd_on_statins_50_to_80 ) In the code above several columns are added to the existing table (dataframe) my_dataframe . The first parameter in withColumn is what the name of the column will be in the table, the second parameter is one of our pre-defined field definitions. In the diabetes RAP all of our field definitions are kept in one file called field_definitions.py . Our field definitions can be re-used like this anywhere they are needed. This also solves the first issue mentioned before, as now if the specification of one of these field definitions changed all we would need to do is change the code that occurs in the first example (the initial definition/the code in field_definitions.py ). Testing field definitions There is still the second issue to deal with; namely how do we know our field definition is correct? Another benefit of breaking out atomic field definitions into the smallest units they can be is that we can now apply unit tests to them. Why should we bother to write these unit tests? Especially for something as simple as our field definitions. Unit tests test for functionality not code. This means that we base the expected output of a unit test on our design. This is well suited to field definitions as it means our metric for whether or not our field definitions are correct is based on our initial design decisions. We can then setup these tests with static data. All of our field definitions tests passing means all of our field definitions are correct. This also future-proofs our code. If someone makes a change to a field definition by mistake the test for it will fail and we will catch bugs early. Now when we need to change the specifications for a field definition we go through the following steps: Plan out the adjusted field definition and all the expected values it should output on various inputs. Now we have the design of the adjusted field definition update our tests to reflect it. Rewrite the code defining the field definition so that it passes our test. We have now solved both of our initial worries with field definitions: Our actual field definition code only occurs in one place so changes in specification are not cumbersome to deal with. We now have unit tests as a metric to make sure our field definitions fulfil our design correctly. Choosing data examples to test a field definition The benefits of unit testing field definitions mostly come back to this idea that unit tests reflect our design. Therefore, one of the most important aspects of designing these tests is to select data that covers all the kinds of cases our field definition needs to deal with. This process should ideally function like a dialogue between the developer (D) and the responsible statistician (S), and should elucidate the design of the field definition more. For example: \u2014 S : \"I want a field that has a value of 1 when the column 'AGE' has a value more than 12\" \u2014 D : \"So any value more than 12 should give 1, what if 'AGE' is less than 12?\" \u2014 S : \"In that case we can set the value to 0\" \u2014 D : \"What if the value of 'AGE' is equal to 12?\" \u2014 S : \"It makes sense to have 1 also in that case\" \u2014 D : \"What if the value of 'AGE' is 0, should we do something else?\" \u2014 S : \"0 is still an acceptable value for 'AGE' in this case, so we would still set the value of the field to 0\" \u2014 D : \"What if age is negative?\" \u2014 S : \"While that is a strange case I think for this field all we are trying to show is the value of 'AGE' is more than or equal to 12, so we would still set the value of the new field to 0\" \u2014 D : \"Fair enough, what should we do about a NULL value in the 'AGE' column?\" \u2014 S : \"That's a trickier case but in my opinion it's the same as a negative age, all that matters is that if the 'AGE' column has a value greater than or equal to 12 we set this field to 1, so for any other value we can set it to 0.\" There are three kinds of cases to deal with: Expected cases where the kinds of values we expect give a normal output. Erroneous cases where bad values should be dealt with by our code by throwing out an error. Edge cases where certain boundary/extreme values may need to be dealt with in a special way. Edge cases are the cases that are the most important to define, as they require specific design decisions to be made. Through their dialogue S and D were able to identify a complete range of cases for this new field definition, we can specify example input values as well which we will use in our tests: Case type Case Example input Expected output Edge AGE < 0 -1 0 Edge AGE = 0 0 0 Expected AGE < 12 11 0 Edge/Expected AGE = 12 12 1 Expected AGE > 12 13, 20 1 Edge AGE is NULL None 0 With these cases outlined a test can actually be written for our new cholesterol_denominator field: class TestCholesterolDenominator ( object ): \"\"\" cholesterol_denominator = ( F.when((F.col('AGE') >= 12), 1).otherwise(0) ) \"\"\" @pytest . mark . usefixtures ( \"spark_session\" ) def test_valid_invalid_age_returns_correct_chol_denom ( self , spark_session ): input_df = spark_session . createDataFrame ( [ ( - 1 , '2021-02-13' ), ( 0 , '2021-02-13' ), ( 11 , '2021-02-13' ), ( 12 , '2021-02-13' ), ( 13 , '2021-02-13' ), ( 20 , '2021-02-13' ), ( None , '2021-02-13' ) ], [ \"AGE\" , \"CHOLESTEROL_DATE\" ] ) return_df = ( input_df . withColumn ( 'CHOLESTEROL_DEN' , cholesterol_denominator )) expected = [ 0 , 0 , 0 , 1 , 1 , 1 , 0 ] actual = [ row [ 'CHOLESTEROL_DEN' ] for row in return_df . collect ()] assert actual == expected , f \"When checking cholesterol denominator, expected to find { expected } but found { actual } \" The spark_session is simply a PySpark session which we pass to our tests, this spark session allows us to create static dataframes to test. As we can see in the test above there is a dataframe input_df which has all the values needed to cover the cases outlined before. We the get a new dataframe return_df by adding our new field cholesterol_denominator to input_df using the withColumn method. We can store our expected values in a list in the same order as the table above. Finally we can compare the values in our new column with the expected values to assert they are all correct. We now have a test that can be re-run over and over, that accurately shows whether or not our field definition implemented in our code base actually fulfils it's design.","title":"Unit testing field definitions"},{"location":"training_resources/python/unit-testing-field-definitions/#unit-testing-field-definitions","text":"This section focuses on one specific application of unit testing that is very relevant to analysts; testing the definitions of different fields (columns). One of the biggest burdens on stats teams is to maintain accurate definitions of fields over time as the specifications change. This drift in definitions is also one of the biggest sources of errors in stats publications. In the worst cases you may find that a field is defined in dozens of locations across a code-base. Each of these needs to be updated each time a change happens - leading to burden and risk. Here we show that by separating the definitions of fields from the context in which you create those fields, you can easily maintain and manage those definitions over time. Moreover, these atomic field definitions become very easy to include in unit tests. In this way you can use simple data examples to ensure your outputs are accurate. This is not a general guide to unit testing. That guide is called ' unit-testing.md '.","title":"Unit testing field definitions"},{"location":"training_resources/python/unit-testing-field-definitions/#field-definitions-and-how-to-use-them","text":"As was previously mentioned analysts will typically have several planned definitions for certain fields/columns of data. What can happen with these definitions is that they become repeated throughout the code base whenever they are needed; this leads to long messy code that is hard to maintain. More specifically having these definitions in many locations leads to two issues arising if the specification for one of these definitions changes: The task of updating the old field definition with the new one in every place the old one occurred is time-consuming and annoying. Furthermore, the process of updating this definition in many locations can lead to bugs and inconsistencies which we have no way of checking for. We have no consistent way of checking if our definition is correct if it occurs in many places in the code. Instead of having field definition code in many places throughout the code base it would be preferable to have the code which defines the field in one place, and then re-use that definition. What this would mean is that if a change in specifications occurred then all we would have to do is change the code of that initial definition rather than the code in every place it is used. PySpark provides a very tidy way of doing this, which has been used in the diabetes RAP: foot_denom = ( F . when (( F . col ( 'AGE' ) >= 12 ), 1 ) . otherwise ( 0 ) ) cvd_admission = ( F . when ( F . col ( 'NHS_NUMBER' ) . isNotNull (), 1 ) . otherwise ( None ) ) no_cvd_on_statins_40_to_80 = ( F . when (( F . col ( 'CVD_ADMISSION' ) . isNull ()) & ( F . col ( 'IHD_VALUE' ) . isNull ()) & ( F . col ( 'STATIN_FLAG' ) == 1 ) & ( F . col ( 'AGE' ) >= 40 ) & ( F . col ( 'AGE' ) <= 80 ), 1 ) . otherwise ( 0 ) ) In the code above three field definitions are given: A foot_denom field which has a value of 1 if the 'AGE' column has a value of 12 or more, otherwise has a value of 0. A cvd_admission field which has a value of 1 if there is a value in the 'NHS_NUMBER' field otherwise has a null value (None). A no_cvd_on_statins_50_to_80 field which has a value of 1 if: - The 'CVD_ADMISSION' and 'IHD_VALUE' columns both have a null value. - And the 'STATIN_FLAG' column has a value of 1. - And the 'AGE' column has a value between 40 and 80 inclusive. - Otherwise no_cvd_on_statins_50_to_80 has a value of 0 With our fields defined we can now re-use them without having to write out their logic every time we do so. For example: my_dataframe . withColumn ( 'FOOT_DENOM' , foot_denom ) . withColumn ( 'CVD_ADMISSION' , cvd_admission ) . withColumn ( 'NO_CVD_ON_STATINS_50_TO_80' , no_cvd_on_statins_50_to_80 ) In the code above several columns are added to the existing table (dataframe) my_dataframe . The first parameter in withColumn is what the name of the column will be in the table, the second parameter is one of our pre-defined field definitions. In the diabetes RAP all of our field definitions are kept in one file called field_definitions.py . Our field definitions can be re-used like this anywhere they are needed. This also solves the first issue mentioned before, as now if the specification of one of these field definitions changed all we would need to do is change the code that occurs in the first example (the initial definition/the code in field_definitions.py ).","title":"Field definitions and how to use them"},{"location":"training_resources/python/unit-testing-field-definitions/#testing-field-definitions","text":"There is still the second issue to deal with; namely how do we know our field definition is correct? Another benefit of breaking out atomic field definitions into the smallest units they can be is that we can now apply unit tests to them. Why should we bother to write these unit tests? Especially for something as simple as our field definitions. Unit tests test for functionality not code. This means that we base the expected output of a unit test on our design. This is well suited to field definitions as it means our metric for whether or not our field definitions are correct is based on our initial design decisions. We can then setup these tests with static data. All of our field definitions tests passing means all of our field definitions are correct. This also future-proofs our code. If someone makes a change to a field definition by mistake the test for it will fail and we will catch bugs early. Now when we need to change the specifications for a field definition we go through the following steps: Plan out the adjusted field definition and all the expected values it should output on various inputs. Now we have the design of the adjusted field definition update our tests to reflect it. Rewrite the code defining the field definition so that it passes our test. We have now solved both of our initial worries with field definitions: Our actual field definition code only occurs in one place so changes in specification are not cumbersome to deal with. We now have unit tests as a metric to make sure our field definitions fulfil our design correctly.","title":"Testing field definitions"},{"location":"training_resources/python/unit-testing-field-definitions/#choosing-data-examples-to-test-a-field-definition","text":"The benefits of unit testing field definitions mostly come back to this idea that unit tests reflect our design. Therefore, one of the most important aspects of designing these tests is to select data that covers all the kinds of cases our field definition needs to deal with. This process should ideally function like a dialogue between the developer (D) and the responsible statistician (S), and should elucidate the design of the field definition more. For example: \u2014 S : \"I want a field that has a value of 1 when the column 'AGE' has a value more than 12\" \u2014 D : \"So any value more than 12 should give 1, what if 'AGE' is less than 12?\" \u2014 S : \"In that case we can set the value to 0\" \u2014 D : \"What if the value of 'AGE' is equal to 12?\" \u2014 S : \"It makes sense to have 1 also in that case\" \u2014 D : \"What if the value of 'AGE' is 0, should we do something else?\" \u2014 S : \"0 is still an acceptable value for 'AGE' in this case, so we would still set the value of the field to 0\" \u2014 D : \"What if age is negative?\" \u2014 S : \"While that is a strange case I think for this field all we are trying to show is the value of 'AGE' is more than or equal to 12, so we would still set the value of the new field to 0\" \u2014 D : \"Fair enough, what should we do about a NULL value in the 'AGE' column?\" \u2014 S : \"That's a trickier case but in my opinion it's the same as a negative age, all that matters is that if the 'AGE' column has a value greater than or equal to 12 we set this field to 1, so for any other value we can set it to 0.\" There are three kinds of cases to deal with: Expected cases where the kinds of values we expect give a normal output. Erroneous cases where bad values should be dealt with by our code by throwing out an error. Edge cases where certain boundary/extreme values may need to be dealt with in a special way. Edge cases are the cases that are the most important to define, as they require specific design decisions to be made. Through their dialogue S and D were able to identify a complete range of cases for this new field definition, we can specify example input values as well which we will use in our tests: Case type Case Example input Expected output Edge AGE < 0 -1 0 Edge AGE = 0 0 0 Expected AGE < 12 11 0 Edge/Expected AGE = 12 12 1 Expected AGE > 12 13, 20 1 Edge AGE is NULL None 0 With these cases outlined a test can actually be written for our new cholesterol_denominator field: class TestCholesterolDenominator ( object ): \"\"\" cholesterol_denominator = ( F.when((F.col('AGE') >= 12), 1).otherwise(0) ) \"\"\" @pytest . mark . usefixtures ( \"spark_session\" ) def test_valid_invalid_age_returns_correct_chol_denom ( self , spark_session ): input_df = spark_session . createDataFrame ( [ ( - 1 , '2021-02-13' ), ( 0 , '2021-02-13' ), ( 11 , '2021-02-13' ), ( 12 , '2021-02-13' ), ( 13 , '2021-02-13' ), ( 20 , '2021-02-13' ), ( None , '2021-02-13' ) ], [ \"AGE\" , \"CHOLESTEROL_DATE\" ] ) return_df = ( input_df . withColumn ( 'CHOLESTEROL_DEN' , cholesterol_denominator )) expected = [ 0 , 0 , 0 , 1 , 1 , 1 , 0 ] actual = [ row [ 'CHOLESTEROL_DEN' ] for row in return_df . collect ()] assert actual == expected , f \"When checking cholesterol denominator, expected to find { expected } but found { actual } \" The spark_session is simply a PySpark session which we pass to our tests, this spark session allows us to create static dataframes to test. As we can see in the test above there is a dataframe input_df which has all the values needed to cover the cases outlined before. We the get a new dataframe return_df by adding our new field cholesterol_denominator to input_df using the withColumn method. We can store our expected values in a list in the same order as the table above. Finally we can compare the values in our new column with the expected values to assert they are all correct. We now have a test that can be re-run over and over, that accurately shows whether or not our field definition implemented in our code base actually fulfils it's design.","title":"Choosing data examples to test a field definition"},{"location":"training_resources/python/unit-testing/","text":"Unit testing Tests are functions which make logical assertions. If all assertions are correct then the test passes, if at least one assertion is incorrect then the test fails. Tests are a useful metric for deciding if an application has met its requirements. Unit tests test a single piece of functionality, this functionality is delivered by a single unit of code such as a method. The philosophy behind unit tests is that if the functionality of the smallest units of the program can be guaranteed, then it is significantly more likely that the project as a whole is succeeding in delivering its functionality. Benefits of testing When discussing the benefits of testing this guide will use the example field definitions to show testing in the context of RAP. For example take the field \"CREATININE_DATE\" we want another field \"CREATININE_DATE_FLAG\" to be set to 1 when there is a date value in \"CREATININE_DATE\" and 0 otherwise. We write some PySpark code to achieve this: creatinine_date_flag = ( F . when ( F . col ( \"CREATININE_DATE\" ) . isNotNull (), 1 ) . otherwise ( 0 ) ) What are the benefits of writing a test for this small piece of code? General Benefits It confirms for us that the code we have written has been successful in delivering the field definition we want. We could of course do this manually, but writing tests also gives other benefits. Tests persist as functions and can be re-run over and over; this is cumbersome with manual testing. This future-proofs our code. If we write our field definition code and it passes the test then we know it works. If in future when we re-run our tests we see that the test now fails then we know that a fundamental part of our program has been broken. Tests allow us to catch bugs early. Tests provide an additional kind of documentation. As was mentioned in the introduction the goal of unit tests is to test a single piece of functionality. Therefore what a collection of tests passing describe is not that our code is good but rather that our code is delivering the desired functionality of the program. By viewing the results of our testing and seeing which test functions pass we are able to see what the planned and delivered functionality of the project is and what considerations have been made. Tests improve general code quality not just correctness. Unit tests should be testing for a smallest unit of functionality and that functionality is typically delivered by a function in our code. If we struggle to write a unit test because we can't figure out what functions to use then our code is not modular enough and that we should break down our processing steps in our code into more functions. Tests allow us to run smaller parts of code in isolation. This means that we could check to see if all the code we wrote for each of our field definitions is delivering its desired functionality. If instead we tried to run the whole program to do this it would be hard to tell if issues were arising because our code delivering the function was wrong, or because some other part of the program is causing an issue. Tests are simple to write. They often use a repeated pattern of code and for such a small amount of effort a huge reward is reaped. Test-driven development Test driven development (TDD) is a developmental approach in which tests are written before the code is, and then development begins. This may seem obtuse but the benefit is that it forces us to design our tests first based only on the desired functionality that the program should deliver. This leads to a couple of things: If all of our tests pass then we know we have delivered all the functionality the program needs. We have to break down our project and write functions to deliver these units of functionality, rather than writing tests for our functions. It prevents us from having tunnel-vision when it comes to code. As our attention will always be brought back to our tests which are based on the higher-level context of the functionality the project should deliver. It forces us to make sure that we understand the goals of the project before we start writing code. While a perfect TDD approach can be hard to achieve, maintaining its general goals will keep a project more focused. Field definitions provide an excellent example of where TDD could be useful. To apply TDD we would need to properly plan out our field definitions before hand (perhaps by looking at pre-existing SQL code if we are transitioning to PySpark). We can then write tests for each of these field definitions with the expected values we have planned to show that they are a fundamental part of the project. Initially all these tests will fail but over the course of development more will pass until we know that all of our field definitions have been implemented according to our initial plan. Migration Whether we follow TDD or not our collection of tests will grow as we notice the requirement for more functionalities in the project. When the project is completed we should ideally have a complete suite of tests which all pass, and which covers all the planned functionality of the project. If at a later date our project needs to be migrated to a new domain we will still have all of our tests. Re-running our tests after migration will help reveal two things: If our migration failed our tests failing should pinpoint what, if anything, is now broken in the project. If our migration appears to have succeeded our tests failing will pinpoint functionality that is now broken in the project that we would otherwise have missed. How to design tests It is important to take the step of designing and planning tests before you write them. Writing a bad test which doesn't accurately reflect the design of the project can do more harm than good. What should the test(s) be about? The first thing to be settled is what the test should be testing. Always keep in mind is that what is being tested should be the desired functionality of the program rather than testing code that has been written. For example the wrong process would be: I have written the field definition registrations_denominator in the diabetes RAP project code. I expect this code to return a data frame with a new \"REGISTRATIONS\" field with values set to 0 when no NHS number is present and 1 if one is present. I should write a test using this definition with these values. This is bad as the focus has shifted from what the project should be delivering in terms of functionality, to instead being about what we think the expected values of some abstract function/definition should be. We need to focus on functionality. In the case of unit tests this functionality should be the smallest units of functionality that need to be delivered. For example we might propose the following: In the RAP diabetes project there are a number of field definitions. We need to guarantee this core functionality should work. We should write a test to check if all of our field definitions are correct. This is incorrect. The functionality can be broken down into smaller units, for example: The RAP diabetes project needs a definition for a registrations field. The RAP diabetes project needs a definition for a creatinine date flag field. The RAP diabetes project needs a definition for a ... field. ... We need to write tests for each of these core units of functionality. We have now identified several pieces of functionality that need to be tested. Always keep in mind that a unit test only tests for that one piece of functionality; it should not test for the interaction between different parts of the program. How many tests and what data should the tests use? Once we have identified our functionality to test we need to determine how many tests we need to demonstrate that functionality has been delivered. It is important to note two general practices around unit tests: The code for the test itself is as short as possible while still being readable. The test only makes one logical assertion (check if an input leads to the expected output). Too many logical assertions obscure the goal of the test and the test should instead be split into multiple tests. Based on these features we will need a unit test for each case the functionality needs to handle. There are typically three kinds of cases: Expected cases. Cases where the input is 'good' and our code delivers the expected output. Erroneous cases. Cases where the input is 'bad' and our code handles this in some way (such as by throwing an exception) and exits gracefully. Edge cases. Cases where the input is likely to require special handling by our function (extreme cases). Lets take the example of sorting a list. We have identified that our project needs to support the small unit of functionality to sort a list of integers into ascending order. We can identify some cases to test quickly: Type of case Behaviour Expected A list in ascending order should be returned unchanged Expected A list in descending order should be returned in ascending order Expected A list in a random order should be returned in ascending order Erroneous A list that contains a null/none value should throw an exception Erroneous A list that contains data of the wrong type (string, float etc) should throw an exception Edge An empty list should not throw an exception, and an empty list should be returned Edge A list with repeated values should be sorted like normal with repeated values interchangeable We have now identified 7 unit tests for this piece of functionality. The process of figuring out cases has also elucidated two more things: The input data to our tests has been outlined as well as the expected results. We have made our design decisions more explicit and open. For example our two edge cases could be seen as erroneous cases by someone with a different view. They may want lists which are empty/contain repeated values to throw an exception rather than be treated as correct. Because our tests reflect our design the only acceptable result for our tests is all of them passing. We cannot ignore a failed test as it means our code does not live up to our design. It is also debatable if we need all three of our expected cases and whether one would just do. This is generally up to how much test coverage you feel a feature needs but it is important to note that you should not have tests which repeat themselves and you should not have tests which test for the same thing as this will make your collection of tests cluttered. What should the test be named? It is important in general to have good function names but this is especially true in the case of unit tests. There are some reasons for this: Test suites grow to be very large, and test code is difficult to read. Team members who have not written some of the tests need to run the test suite and understand the result. Sometimes non-programmers/those not as involved in development need to run the test suite and understand the result. Therefore when naming a test we need to describe what functionality the test is for, what kind of data is being used as input, and what is the expected output. There is no set method for naming tests but a good starting point is to simply describe the test as if you were explaining it to someone with knowledge of the programs domain but not much programming knowledge. This generally leads to clear test names. Here are some more specific tips for writing test names: Make sure every word in the test name is separated by an underscore. A test called dividingTwoNumbersIsInvalidWhenDividingByZero is difficult to read compared to dividing_two_numbers_is_invalid_when_dividing_by_zero . Don't worry too much about conforming to a specific test naming scheme such as '[method name]_Should[expected result]_When[conditions]'. While a scheme can make test names more uniform it doesn't necessarily make them more readable. The thought you should always have when naming the test is to simply explain what the test is testing for as if you were explaining it to someone with basic knowledge of the project. Try to avoid using the function name in the test such as divideTwoNumbers_invalid_when_dividing_by_zero as this suggests you are testing a function rather than trying to get the function to deliver the desired functionality. Something like division_by_zero_is_invalid is a much better test name. When using pytest always prepend test_ at the start of your test's name. Drop hopeful language such as should or will and instead just describe the desired functionality. Avoid technical jargon where possible, words such as returns in a test name make it difficult for someone with less programming knowledge to understand the goal of the test. While keeping these tips in mind is beneficial don't stress too much about following them exactly, and it is ok to ignore some or all of them as long as you come up with a test name that quickly conveys what the test is about. How to write unit tests This guide will not focus on all the basic setup and features of pytest as this is better covered in external resources (one of which can be found at the top of this guide). There are however some other tips for actually writing tests that can be helpful. Test suites (where to put your tests) Your test functions will obviously be stored in a file but having all of your tests in one file can become messy. At some point we need to plan how to organise and lay out our tests in order to make best use of them. In a RAP project you will typically have three kinds of tests: Unit tests which have been covered in this guide. Integration tests which test the interaction between modules of the program (occurs after unit testing). Backtesting used to check how successful the pipeline is on historical data. These three classes of tests need to be separated out. The simplest way to do this is to have a subfolder in your main tests folder for each fo these kinds of tests, and then within these subfolders have python files containing the tests: \"tests\": \"unittests\": unit test code \"integration\" integration test code \"backtesting\" backtesting code We may also want to separate out our tests further into domains. For example we could have all of our unit tests in one file in the unit tests subfolder. Alternatively we may have several files for unit tests for different parts of the project. For example in the diabetes RAP we have: test_data_connections.py test_field_definitions.py test_schema_definitions.py The goal with separating out tests like this is to make our collection of tests as readable and as uncluttered as possible. A final pattern which is useful to follow to keep test collections tidy is to contain the unit test functions themselves in a class related to the functionality they deliver. For example in the diabetes RAP we have the following: class TestCholesterolNumerator ( object ): \"\"\" cholesterol_numerator = ( F.when((F.col('AGE') >= 12) & (F.col(\"CHOLESTEROL_DATE\").isNotNull()), 1).otherwise(0) ) \"\"\" @pytest . mark . usefixtures ( \"spark_session\" ) def test_valid_invalid_age_returns_correct_chol_numerator ( self , spark_session ): ... @pytest . mark . usefixtures ( \"spark_session\" ) def test_valid_invalid_cholesterol_date_returns_correct_chol_numer ( self , spark_session ): (Test function bodies omitted for brevity). The functionality we are testing for is a cholesterol_numerator field definition so we create a class for this functionality called TestCholesterolNumerator . We have identified we need two tests to guarantee this functionality so we write these tests as we normally would but put them within the TestCholesterolNumerator class. This makes it clear what functionality each test relates to. External links Guide covering all the core features of pytest","title":"Unit testing"},{"location":"training_resources/python/unit-testing/#unit-testing","text":"Tests are functions which make logical assertions. If all assertions are correct then the test passes, if at least one assertion is incorrect then the test fails. Tests are a useful metric for deciding if an application has met its requirements. Unit tests test a single piece of functionality, this functionality is delivered by a single unit of code such as a method. The philosophy behind unit tests is that if the functionality of the smallest units of the program can be guaranteed, then it is significantly more likely that the project as a whole is succeeding in delivering its functionality.","title":"Unit testing"},{"location":"training_resources/python/unit-testing/#benefits-of-testing","text":"When discussing the benefits of testing this guide will use the example field definitions to show testing in the context of RAP. For example take the field \"CREATININE_DATE\" we want another field \"CREATININE_DATE_FLAG\" to be set to 1 when there is a date value in \"CREATININE_DATE\" and 0 otherwise. We write some PySpark code to achieve this: creatinine_date_flag = ( F . when ( F . col ( \"CREATININE_DATE\" ) . isNotNull (), 1 ) . otherwise ( 0 ) ) What are the benefits of writing a test for this small piece of code?","title":"Benefits of testing"},{"location":"training_resources/python/unit-testing/#general-benefits","text":"It confirms for us that the code we have written has been successful in delivering the field definition we want. We could of course do this manually, but writing tests also gives other benefits. Tests persist as functions and can be re-run over and over; this is cumbersome with manual testing. This future-proofs our code. If we write our field definition code and it passes the test then we know it works. If in future when we re-run our tests we see that the test now fails then we know that a fundamental part of our program has been broken. Tests allow us to catch bugs early. Tests provide an additional kind of documentation. As was mentioned in the introduction the goal of unit tests is to test a single piece of functionality. Therefore what a collection of tests passing describe is not that our code is good but rather that our code is delivering the desired functionality of the program. By viewing the results of our testing and seeing which test functions pass we are able to see what the planned and delivered functionality of the project is and what considerations have been made. Tests improve general code quality not just correctness. Unit tests should be testing for a smallest unit of functionality and that functionality is typically delivered by a function in our code. If we struggle to write a unit test because we can't figure out what functions to use then our code is not modular enough and that we should break down our processing steps in our code into more functions. Tests allow us to run smaller parts of code in isolation. This means that we could check to see if all the code we wrote for each of our field definitions is delivering its desired functionality. If instead we tried to run the whole program to do this it would be hard to tell if issues were arising because our code delivering the function was wrong, or because some other part of the program is causing an issue. Tests are simple to write. They often use a repeated pattern of code and for such a small amount of effort a huge reward is reaped.","title":"General Benefits"},{"location":"training_resources/python/unit-testing/#test-driven-development","text":"Test driven development (TDD) is a developmental approach in which tests are written before the code is, and then development begins. This may seem obtuse but the benefit is that it forces us to design our tests first based only on the desired functionality that the program should deliver. This leads to a couple of things: If all of our tests pass then we know we have delivered all the functionality the program needs. We have to break down our project and write functions to deliver these units of functionality, rather than writing tests for our functions. It prevents us from having tunnel-vision when it comes to code. As our attention will always be brought back to our tests which are based on the higher-level context of the functionality the project should deliver. It forces us to make sure that we understand the goals of the project before we start writing code. While a perfect TDD approach can be hard to achieve, maintaining its general goals will keep a project more focused. Field definitions provide an excellent example of where TDD could be useful. To apply TDD we would need to properly plan out our field definitions before hand (perhaps by looking at pre-existing SQL code if we are transitioning to PySpark). We can then write tests for each of these field definitions with the expected values we have planned to show that they are a fundamental part of the project. Initially all these tests will fail but over the course of development more will pass until we know that all of our field definitions have been implemented according to our initial plan.","title":"Test-driven development"},{"location":"training_resources/python/unit-testing/#migration","text":"Whether we follow TDD or not our collection of tests will grow as we notice the requirement for more functionalities in the project. When the project is completed we should ideally have a complete suite of tests which all pass, and which covers all the planned functionality of the project. If at a later date our project needs to be migrated to a new domain we will still have all of our tests. Re-running our tests after migration will help reveal two things: If our migration failed our tests failing should pinpoint what, if anything, is now broken in the project. If our migration appears to have succeeded our tests failing will pinpoint functionality that is now broken in the project that we would otherwise have missed.","title":"Migration"},{"location":"training_resources/python/unit-testing/#how-to-design-tests","text":"It is important to take the step of designing and planning tests before you write them. Writing a bad test which doesn't accurately reflect the design of the project can do more harm than good.","title":"How to design tests"},{"location":"training_resources/python/unit-testing/#what-should-the-tests-be-about","text":"The first thing to be settled is what the test should be testing. Always keep in mind is that what is being tested should be the desired functionality of the program rather than testing code that has been written. For example the wrong process would be: I have written the field definition registrations_denominator in the diabetes RAP project code. I expect this code to return a data frame with a new \"REGISTRATIONS\" field with values set to 0 when no NHS number is present and 1 if one is present. I should write a test using this definition with these values. This is bad as the focus has shifted from what the project should be delivering in terms of functionality, to instead being about what we think the expected values of some abstract function/definition should be. We need to focus on functionality. In the case of unit tests this functionality should be the smallest units of functionality that need to be delivered. For example we might propose the following: In the RAP diabetes project there are a number of field definitions. We need to guarantee this core functionality should work. We should write a test to check if all of our field definitions are correct. This is incorrect. The functionality can be broken down into smaller units, for example: The RAP diabetes project needs a definition for a registrations field. The RAP diabetes project needs a definition for a creatinine date flag field. The RAP diabetes project needs a definition for a ... field. ... We need to write tests for each of these core units of functionality. We have now identified several pieces of functionality that need to be tested. Always keep in mind that a unit test only tests for that one piece of functionality; it should not test for the interaction between different parts of the program.","title":"What should the test(s) be about?"},{"location":"training_resources/python/unit-testing/#how-many-tests-and-what-data-should-the-tests-use","text":"Once we have identified our functionality to test we need to determine how many tests we need to demonstrate that functionality has been delivered. It is important to note two general practices around unit tests: The code for the test itself is as short as possible while still being readable. The test only makes one logical assertion (check if an input leads to the expected output). Too many logical assertions obscure the goal of the test and the test should instead be split into multiple tests. Based on these features we will need a unit test for each case the functionality needs to handle. There are typically three kinds of cases: Expected cases. Cases where the input is 'good' and our code delivers the expected output. Erroneous cases. Cases where the input is 'bad' and our code handles this in some way (such as by throwing an exception) and exits gracefully. Edge cases. Cases where the input is likely to require special handling by our function (extreme cases). Lets take the example of sorting a list. We have identified that our project needs to support the small unit of functionality to sort a list of integers into ascending order. We can identify some cases to test quickly: Type of case Behaviour Expected A list in ascending order should be returned unchanged Expected A list in descending order should be returned in ascending order Expected A list in a random order should be returned in ascending order Erroneous A list that contains a null/none value should throw an exception Erroneous A list that contains data of the wrong type (string, float etc) should throw an exception Edge An empty list should not throw an exception, and an empty list should be returned Edge A list with repeated values should be sorted like normal with repeated values interchangeable We have now identified 7 unit tests for this piece of functionality. The process of figuring out cases has also elucidated two more things: The input data to our tests has been outlined as well as the expected results. We have made our design decisions more explicit and open. For example our two edge cases could be seen as erroneous cases by someone with a different view. They may want lists which are empty/contain repeated values to throw an exception rather than be treated as correct. Because our tests reflect our design the only acceptable result for our tests is all of them passing. We cannot ignore a failed test as it means our code does not live up to our design. It is also debatable if we need all three of our expected cases and whether one would just do. This is generally up to how much test coverage you feel a feature needs but it is important to note that you should not have tests which repeat themselves and you should not have tests which test for the same thing as this will make your collection of tests cluttered.","title":"How many tests and what data should the tests use?"},{"location":"training_resources/python/unit-testing/#what-should-the-test-be-named","text":"It is important in general to have good function names but this is especially true in the case of unit tests. There are some reasons for this: Test suites grow to be very large, and test code is difficult to read. Team members who have not written some of the tests need to run the test suite and understand the result. Sometimes non-programmers/those not as involved in development need to run the test suite and understand the result. Therefore when naming a test we need to describe what functionality the test is for, what kind of data is being used as input, and what is the expected output. There is no set method for naming tests but a good starting point is to simply describe the test as if you were explaining it to someone with knowledge of the programs domain but not much programming knowledge. This generally leads to clear test names. Here are some more specific tips for writing test names: Make sure every word in the test name is separated by an underscore. A test called dividingTwoNumbersIsInvalidWhenDividingByZero is difficult to read compared to dividing_two_numbers_is_invalid_when_dividing_by_zero . Don't worry too much about conforming to a specific test naming scheme such as '[method name]_Should[expected result]_When[conditions]'. While a scheme can make test names more uniform it doesn't necessarily make them more readable. The thought you should always have when naming the test is to simply explain what the test is testing for as if you were explaining it to someone with basic knowledge of the project. Try to avoid using the function name in the test such as divideTwoNumbers_invalid_when_dividing_by_zero as this suggests you are testing a function rather than trying to get the function to deliver the desired functionality. Something like division_by_zero_is_invalid is a much better test name. When using pytest always prepend test_ at the start of your test's name. Drop hopeful language such as should or will and instead just describe the desired functionality. Avoid technical jargon where possible, words such as returns in a test name make it difficult for someone with less programming knowledge to understand the goal of the test. While keeping these tips in mind is beneficial don't stress too much about following them exactly, and it is ok to ignore some or all of them as long as you come up with a test name that quickly conveys what the test is about.","title":"What should the test be named?"},{"location":"training_resources/python/unit-testing/#how-to-write-unit-tests","text":"This guide will not focus on all the basic setup and features of pytest as this is better covered in external resources (one of which can be found at the top of this guide). There are however some other tips for actually writing tests that can be helpful.","title":"How to write unit tests"},{"location":"training_resources/python/unit-testing/#test-suites-where-to-put-your-tests","text":"Your test functions will obviously be stored in a file but having all of your tests in one file can become messy. At some point we need to plan how to organise and lay out our tests in order to make best use of them. In a RAP project you will typically have three kinds of tests: Unit tests which have been covered in this guide. Integration tests which test the interaction between modules of the program (occurs after unit testing). Backtesting used to check how successful the pipeline is on historical data. These three classes of tests need to be separated out. The simplest way to do this is to have a subfolder in your main tests folder for each fo these kinds of tests, and then within these subfolders have python files containing the tests: \"tests\": \"unittests\": unit test code \"integration\" integration test code \"backtesting\" backtesting code We may also want to separate out our tests further into domains. For example we could have all of our unit tests in one file in the unit tests subfolder. Alternatively we may have several files for unit tests for different parts of the project. For example in the diabetes RAP we have: test_data_connections.py test_field_definitions.py test_schema_definitions.py The goal with separating out tests like this is to make our collection of tests as readable and as uncluttered as possible. A final pattern which is useful to follow to keep test collections tidy is to contain the unit test functions themselves in a class related to the functionality they deliver. For example in the diabetes RAP we have the following: class TestCholesterolNumerator ( object ): \"\"\" cholesterol_numerator = ( F.when((F.col('AGE') >= 12) & (F.col(\"CHOLESTEROL_DATE\").isNotNull()), 1).otherwise(0) ) \"\"\" @pytest . mark . usefixtures ( \"spark_session\" ) def test_valid_invalid_age_returns_correct_chol_numerator ( self , spark_session ): ... @pytest . mark . usefixtures ( \"spark_session\" ) def test_valid_invalid_cholesterol_date_returns_correct_chol_numer ( self , spark_session ): (Test function bodies omitted for brevity). The functionality we are testing for is a cholesterol_numerator field definition so we create a class for this functionality called TestCholesterolNumerator . We have identified we need two tests to guarantee this functionality so we write these tests as we normally would but put them within the TestCholesterolNumerator class. This makes it clear what functionality each test relates to.","title":"Test suites (where to put your tests)"},{"location":"training_resources/python/unit-testing/#external-links","text":"Guide covering all the core features of pytest","title":"External links"},{"location":"training_resources/python/virtual-environments/","text":"Virtual environments Virtual environments are a way to make sure your code will run correctly for you and others. By always coding inside a virtual environment, you make it more likely that your work will be usable by others. If someone tries to run your code but they are using a different version of python then it might fail. Likewise, if your code depends on some packages but your users have a different version of that package installed that might also fail. Worse than this - if you have multiple projects then one project depends on 'my_example_library_v1' while another project needs to use 'my_example_library_v2' then both projects will break. Sometimes the code you have might depend on outdated versions of a package as the latest package update introduces bugs and issues. Or, you might have multiple versions of code that need different versions of packages to run. Virtual environments address these situations by keeping all of the packages and versions for each project separate. I can create a virtual environment called 'project-1-env' that uses python 2.7 and 'my_example_library_v1'. I can create a second virtual environment called 'project-2-env' that uses python 3.8 and 'my_example_library_v2'. As you move from working on one project to another, you just need to switch to the environment associated with that work. It is good practice to always code inside a virtual environment. Conda environment Conda is a package manager that we use to create and manage our virtual environments. Conda is bundled with Anaconda, which is a distribution software that contains package managers and applications in R and Python. See these instructions on how to install and setup Anaconda. You can interact with conda via the Anaconda Prompt for Windows, or in a terminal window for Mac/Linux. By inputting commands in the command prompt you can then create and manage your virtual environments using the conda package manager. See the Anaconda user guides for more information on getting started with conda. How to create a new virtual environment using conda To create a new conda virtual environment for your project, open the Anaconda Prompt (Windows) or a terminal window on Mac/Linus and enter: conda create --name myenvironment python=3.9 The --name tag specifies a name for the environment: in this example the environment will be named \"myenvironment\", but you can replace this will something better suited to your project. python=3.9 specifies python version you wish the virtual environment to run, in this case version 3.9. To check the packages that are installed in the active environment , enter: conda list To install a package the active environment: conda install pandas To create an environment, specify the Python version and install multiple packages in one line : conda create --name mynewenvironment python=3.8 pandas 3.1.0 flake8 3.9.2 numpy Notice how the versions for pandas and flake8 are specified but no version is given for numpy. This will result in all versions of numpy being installed. TIP: It is recommended to install all packages in one go (e.g. numpy, pandas, pytest etc). Installing 1 package at a time could cause potential package dependency conflicts (see Dependency Hell ). If you're unsure a specific package is installed in the current environment simply search: conda search flake8 For more information on managing environments and other commands such as updating your environment's packages, check out Managing environments with Conda . How to activate an environment The 'active' environment is the one that conda will reference when you enter any commands, e.g. new packages will be installed into the active environment, the installed package list will be based on the active environment, etc. When you start a new Anaconda Prompt (Windows) or open a new terminal (Mac/Linux), the active current environment is set to base . To activate an environment, enter: conda activate mynewenvironment As above, \"myenvironment\" specifies the name of the environment and can be replaced with the specific name of the environment you wish to activate. How to export and share environments To ensure reproducibility, it is important that we can export virtual environments and share them alongside the code. In this way, someone else will be able to run your code with exactly the same environment (packages, dependencies etc) as you did. This helps address the classic 'it works on my machine' problem. There are a few options for how to export environments and recreated environments from exported files. Pip requirements.txt If you followed the Project structure and package organisation guide, you will have created a requirements.txt file in your repository, which specifies all the python packages you wish to install. The benefit of a requirements.txt file created with Pip is that anyone with a Python installation should be able to install it (i.e. someone else wouldn't need to have conda installed) The drawback is that the requirements.txt file offers an incomplete specification for the environment (for example, it does not specify the python version), so projects using requirements.txt files must be careful to specify any additional dependencies in another way (e.g. via a README). To create a working conda environment using the requirements.txt file, simply follow the conda environment creation commands from above and instead of the simple package installation, first activate the target environment: conda activate mynewenviroment And then enter: conda install --file requirements.txt Conda environment.yml Conda offers a way to export and share environments via a yaml file. The benefit of this approach is that the output file gives a more complete picture of the dependencies for a project than a requirements.txt file from Pip. The drawback is the someone would need to have conda installed to use your project: if you work in an organisation or team that consistently uses conda then this is not as relevant, but it may be more relevant if you want to distribute you project to others who may not use conda. To export the active environment: conda env export > environment.yml The resulting file can be used to completely rebuild a conda environment: conda env create --file environment.yml How to remove an environment Make sure your environment is not active by typing: conda deactivate This will take you back to the base conda environment. Then to delete your specified environment: conda env remove --name mynewenvironment Conda help command Type the following command for a list of helpful terminal commands: conda env --help See Also Using Spyder with conda environments Spyder is a Python IDE that is bundled with Anaconda during installation. It can be tricky to set up Spyder to work with multiple conda environments: see this guide for instructions on how to do this. Using Docker with Data Refinery Docker is a container manager that offers many solutions and applications for managing your environments. Currently at NHS Digital we cannot use Docker due to compatibility issues. This may change in the future so we will update the resources on this page accordingly. External links Python virtual environments and package Installing Anaconda Getting started with Conda Managing environments with Conda Conda commands cheatsheet List of other package managers Using up Spyder with conda environments","title":"Virtual environments"},{"location":"training_resources/python/virtual-environments/#virtual-environments","text":"Virtual environments are a way to make sure your code will run correctly for you and others. By always coding inside a virtual environment, you make it more likely that your work will be usable by others. If someone tries to run your code but they are using a different version of python then it might fail. Likewise, if your code depends on some packages but your users have a different version of that package installed that might also fail. Worse than this - if you have multiple projects then one project depends on 'my_example_library_v1' while another project needs to use 'my_example_library_v2' then both projects will break. Sometimes the code you have might depend on outdated versions of a package as the latest package update introduces bugs and issues. Or, you might have multiple versions of code that need different versions of packages to run. Virtual environments address these situations by keeping all of the packages and versions for each project separate. I can create a virtual environment called 'project-1-env' that uses python 2.7 and 'my_example_library_v1'. I can create a second virtual environment called 'project-2-env' that uses python 3.8 and 'my_example_library_v2'. As you move from working on one project to another, you just need to switch to the environment associated with that work. It is good practice to always code inside a virtual environment.","title":"Virtual environments"},{"location":"training_resources/python/virtual-environments/#conda-environment","text":"Conda is a package manager that we use to create and manage our virtual environments. Conda is bundled with Anaconda, which is a distribution software that contains package managers and applications in R and Python. See these instructions on how to install and setup Anaconda. You can interact with conda via the Anaconda Prompt for Windows, or in a terminal window for Mac/Linux. By inputting commands in the command prompt you can then create and manage your virtual environments using the conda package manager. See the Anaconda user guides for more information on getting started with conda.","title":"Conda environment"},{"location":"training_resources/python/virtual-environments/#how-to-create-a-new-virtual-environment-using-conda","text":"To create a new conda virtual environment for your project, open the Anaconda Prompt (Windows) or a terminal window on Mac/Linus and enter: conda create --name myenvironment python=3.9 The --name tag specifies a name for the environment: in this example the environment will be named \"myenvironment\", but you can replace this will something better suited to your project. python=3.9 specifies python version you wish the virtual environment to run, in this case version 3.9. To check the packages that are installed in the active environment , enter: conda list To install a package the active environment: conda install pandas To create an environment, specify the Python version and install multiple packages in one line : conda create --name mynewenvironment python=3.8 pandas 3.1.0 flake8 3.9.2 numpy Notice how the versions for pandas and flake8 are specified but no version is given for numpy. This will result in all versions of numpy being installed. TIP: It is recommended to install all packages in one go (e.g. numpy, pandas, pytest etc). Installing 1 package at a time could cause potential package dependency conflicts (see Dependency Hell ). If you're unsure a specific package is installed in the current environment simply search: conda search flake8 For more information on managing environments and other commands such as updating your environment's packages, check out Managing environments with Conda .","title":"How to create a new virtual environment using conda"},{"location":"training_resources/python/virtual-environments/#how-to-activate-an-environment","text":"The 'active' environment is the one that conda will reference when you enter any commands, e.g. new packages will be installed into the active environment, the installed package list will be based on the active environment, etc. When you start a new Anaconda Prompt (Windows) or open a new terminal (Mac/Linux), the active current environment is set to base . To activate an environment, enter: conda activate mynewenvironment As above, \"myenvironment\" specifies the name of the environment and can be replaced with the specific name of the environment you wish to activate.","title":"How to activate an environment"},{"location":"training_resources/python/virtual-environments/#how-to-export-and-share-environments","text":"To ensure reproducibility, it is important that we can export virtual environments and share them alongside the code. In this way, someone else will be able to run your code with exactly the same environment (packages, dependencies etc) as you did. This helps address the classic 'it works on my machine' problem. There are a few options for how to export environments and recreated environments from exported files.","title":"How to export and share environments"},{"location":"training_resources/python/virtual-environments/#pip-requirementstxt","text":"If you followed the Project structure and package organisation guide, you will have created a requirements.txt file in your repository, which specifies all the python packages you wish to install. The benefit of a requirements.txt file created with Pip is that anyone with a Python installation should be able to install it (i.e. someone else wouldn't need to have conda installed) The drawback is that the requirements.txt file offers an incomplete specification for the environment (for example, it does not specify the python version), so projects using requirements.txt files must be careful to specify any additional dependencies in another way (e.g. via a README). To create a working conda environment using the requirements.txt file, simply follow the conda environment creation commands from above and instead of the simple package installation, first activate the target environment: conda activate mynewenviroment And then enter: conda install --file requirements.txt","title":"Pip requirements.txt"},{"location":"training_resources/python/virtual-environments/#conda-environmentyml","text":"Conda offers a way to export and share environments via a yaml file. The benefit of this approach is that the output file gives a more complete picture of the dependencies for a project than a requirements.txt file from Pip. The drawback is the someone would need to have conda installed to use your project: if you work in an organisation or team that consistently uses conda then this is not as relevant, but it may be more relevant if you want to distribute you project to others who may not use conda. To export the active environment: conda env export > environment.yml The resulting file can be used to completely rebuild a conda environment: conda env create --file environment.yml","title":"Conda environment.yml"},{"location":"training_resources/python/virtual-environments/#how-to-remove-an-environment","text":"Make sure your environment is not active by typing: conda deactivate This will take you back to the base conda environment. Then to delete your specified environment: conda env remove --name mynewenvironment","title":"How to remove an environment"},{"location":"training_resources/python/virtual-environments/#conda-help-command","text":"Type the following command for a list of helpful terminal commands: conda env --help","title":"Conda help command"},{"location":"training_resources/python/virtual-environments/#see-also","text":"","title":"See Also"},{"location":"training_resources/python/virtual-environments/#using-spyder-with-conda-environments","text":"Spyder is a Python IDE that is bundled with Anaconda during installation. It can be tricky to set up Spyder to work with multiple conda environments: see this guide for instructions on how to do this.","title":"Using Spyder with conda environments"},{"location":"training_resources/python/virtual-environments/#using-docker-with-data-refinery","text":"Docker is a container manager that offers many solutions and applications for managing your environments. Currently at NHS Digital we cannot use Docker due to compatibility issues. This may change in the future so we will update the resources on this page accordingly.","title":"Using Docker with Data Refinery"},{"location":"training_resources/python/virtual-environments/#external-links","text":"Python virtual environments and package Installing Anaconda Getting started with Conda Managing environments with Conda Conda commands cheatsheet List of other package managers Using up Spyder with conda environments","title":"External links"},{"location":"training_resources/python/visualisation-in-python/","text":"Visualisations in Python Creating visualisations can require a lot of effort. See Matplotlib Examples Gallery: Matplotlib Examples gallery There are many python packages that provide different features in order to create all kinds of plots. We will present one of the most commonly used packages in this guide: Matplotlib. Basic plots Using Matplotlib you can plot all kinds of charts such as histograms, barplots, scatterplots, pie charts etc. For example, if we import the matplotlib module: from matplotlib import pyplot as plt plt.plot(df['My value 1'], df['My value 2']) # this will produce a basic line chart for two selected columns from dataframe df plt.show() # this command is required to display the plot Using the famous iris dataset we can produce a scatter plot of the sepal length and the sepal width: We load the Seaborn package to import the iris dataset: from matplotlib import pyplot as plt import seaborn as sns # View a list of pre-loaded Seaborn datasets for dataset in sns.get_dataset_names(): print(dataset) df = sns.load_dataset(\"iris\") # we import iris data print(df.head(3)) # view first 3 rows of the data # Create a scatter plot plt.scatter(df['sepal_length'], df['sepal_width'], color='green', marker='s') plt.xlabel('Sepal length') # x axis title/label plt.ylabel('Sepal width') # y axis title/label plt.show() Notice how we set the colour of the data points (color parameter) and the shape (marker parameter = square). To edit the transparency degree (alpha parameter) of the points: # Edit data points transparency with the parameter alpha plt.scatter(df['sepal_length'], df['sepal_width'], alpha=0.2) plt.xlabel('Sepal length') plt.ylabel('Sepal width') plt.show() Applications Using the Smoking, Drinking and Drug Use among Young People in England 2018 [NS] plots displayed on the webpage, we attempt to recreate them using Matplotlib. To create the first plot (line chart): Plot 1 - Pupils who have ever smoked, by year [Line chart] import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns path_1 = \"python\\data\\csv_v1.csv\" # load the dummy data csv we created df1 = pd.read_csv(path_1) # Create a Pandas dataframe After loading our Python packages and dummy data we can start working on producing the first plot: # Create a line chart (Plot 1) plt.figure(figsize=(10, 5)) # set the figure size plt.plot(df1['Year'], df1['Percent'], label=\"Sepal\", linewidth=2, linestyle='-') # create the plot # Define labels and ticks plt.ylabel(\"Percent\", loc='top', rotation=\"horizontal\") # y label title and location plt.xticks(np.arange(1982, 2020, step=2)) # x axis ticks range plt.yticks(np.arange(0, 70, step=10)) # y axis ticks range plt.grid(axis='y') # opting for y axis gridlines # Create annotations (done here for one annotation to avoid redundancy) plt.annotate('Jan 2003: Large new health warnings on cigarette packs', xy=(2003, 42), xytext=(2003, 52), size=9, bbox=dict(boxstyle=\"square\", fc='0.95', pad=1, ec=\"none\"), arrowprops=dict(facecolor='black', shrink=0.05, width=0.5, headwidth=6)) plt.box(False) # remove outer borders plt.savefig('SDD_YP_England_2018_plot1.pdf', bbox_inches='tight') # save plot as .pdf file plt.savefig('SDD_YP_England_2018_plot1.png', bbox_inches='tight') # save as .png plt.savefig('SDD_YP_England_2018_plot1.svg', bbox_inches='tight') # save as .svg Notice how plt.show() which displays the plot is not included in this code as it's not necessary as we are saving our plots with the plt.savefig() function. The three plots will be saved in the local folder your code is also stored or in a folder of your choice. Presenting the .svg image: The advantages of a .svg file compared to using a .png file is better outlined through reading Benefits of using SVG (scalable vector graphics) . Plot 3 - Pupils who have ever smoked, by age [Horizontal bar chart] Similarly, to create the third plot (horizontal bar chart) on the publication webpage: # Create the Pandas dataframe percent_y = [2, 7, 14, 22, 31] index = ['11 years', '12 years', '13 years', '14 years', '15 years'] df = pd.DataFrame({'Percent': percent_y}, index = index) # Plot the horizontal bar chart ax = df.plot.barh() # plot the chart ax.invert_yaxis() # invert the y axis plt.xlabel(\"Percent\", loc='right', rotation=\"horizontal\") # place the x label ax.get_legend().remove() # remove the unnecessary legend ax.set_axisbelow(True) ax.grid(color='gray', which='major', axis='x', linestyle='-', alpha=0.2) # last 2 commands create and style the x axis gridlines plt.box(False) # remove plot borders plt.show() In this case we opted to display the chart, if you wish to save it then plt.savefig() function should be added at the end of the code. Further reading An Intuitive Guide to Data Visualisation in Python Top 6 Python Libraries for Visualisation Matplotlib Examples gallery Matplotlib functions summary","title":"Visualisation in Python"},{"location":"training_resources/python/visualisation-in-python/#visualisations-in-python","text":"Creating visualisations can require a lot of effort. See Matplotlib Examples Gallery: Matplotlib Examples gallery There are many python packages that provide different features in order to create all kinds of plots. We will present one of the most commonly used packages in this guide: Matplotlib.","title":"Visualisations in Python"},{"location":"training_resources/python/visualisation-in-python/#basic-plots","text":"Using Matplotlib you can plot all kinds of charts such as histograms, barplots, scatterplots, pie charts etc. For example, if we import the matplotlib module: from matplotlib import pyplot as plt plt.plot(df['My value 1'], df['My value 2']) # this will produce a basic line chart for two selected columns from dataframe df plt.show() # this command is required to display the plot Using the famous iris dataset we can produce a scatter plot of the sepal length and the sepal width: We load the Seaborn package to import the iris dataset: from matplotlib import pyplot as plt import seaborn as sns # View a list of pre-loaded Seaborn datasets for dataset in sns.get_dataset_names(): print(dataset) df = sns.load_dataset(\"iris\") # we import iris data print(df.head(3)) # view first 3 rows of the data # Create a scatter plot plt.scatter(df['sepal_length'], df['sepal_width'], color='green', marker='s') plt.xlabel('Sepal length') # x axis title/label plt.ylabel('Sepal width') # y axis title/label plt.show() Notice how we set the colour of the data points (color parameter) and the shape (marker parameter = square). To edit the transparency degree (alpha parameter) of the points: # Edit data points transparency with the parameter alpha plt.scatter(df['sepal_length'], df['sepal_width'], alpha=0.2) plt.xlabel('Sepal length') plt.ylabel('Sepal width') plt.show()","title":"Basic plots"},{"location":"training_resources/python/visualisation-in-python/#applications","text":"Using the Smoking, Drinking and Drug Use among Young People in England 2018 [NS] plots displayed on the webpage, we attempt to recreate them using Matplotlib. To create the first plot (line chart):","title":"Applications"},{"location":"training_resources/python/visualisation-in-python/#plot-1-pupils-who-have-ever-smoked-by-year-line-chart","text":"import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns path_1 = \"python\\data\\csv_v1.csv\" # load the dummy data csv we created df1 = pd.read_csv(path_1) # Create a Pandas dataframe After loading our Python packages and dummy data we can start working on producing the first plot: # Create a line chart (Plot 1) plt.figure(figsize=(10, 5)) # set the figure size plt.plot(df1['Year'], df1['Percent'], label=\"Sepal\", linewidth=2, linestyle='-') # create the plot # Define labels and ticks plt.ylabel(\"Percent\", loc='top', rotation=\"horizontal\") # y label title and location plt.xticks(np.arange(1982, 2020, step=2)) # x axis ticks range plt.yticks(np.arange(0, 70, step=10)) # y axis ticks range plt.grid(axis='y') # opting for y axis gridlines # Create annotations (done here for one annotation to avoid redundancy) plt.annotate('Jan 2003: Large new health warnings on cigarette packs', xy=(2003, 42), xytext=(2003, 52), size=9, bbox=dict(boxstyle=\"square\", fc='0.95', pad=1, ec=\"none\"), arrowprops=dict(facecolor='black', shrink=0.05, width=0.5, headwidth=6)) plt.box(False) # remove outer borders plt.savefig('SDD_YP_England_2018_plot1.pdf', bbox_inches='tight') # save plot as .pdf file plt.savefig('SDD_YP_England_2018_plot1.png', bbox_inches='tight') # save as .png plt.savefig('SDD_YP_England_2018_plot1.svg', bbox_inches='tight') # save as .svg Notice how plt.show() which displays the plot is not included in this code as it's not necessary as we are saving our plots with the plt.savefig() function. The three plots will be saved in the local folder your code is also stored or in a folder of your choice. Presenting the .svg image: The advantages of a .svg file compared to using a .png file is better outlined through reading Benefits of using SVG (scalable vector graphics) .","title":"Plot 1 - Pupils who have ever smoked, by year [Line chart]"},{"location":"training_resources/python/visualisation-in-python/#plot-3-pupils-who-have-ever-smoked-by-age-horizontal-bar-chart","text":"Similarly, to create the third plot (horizontal bar chart) on the publication webpage: # Create the Pandas dataframe percent_y = [2, 7, 14, 22, 31] index = ['11 years', '12 years', '13 years', '14 years', '15 years'] df = pd.DataFrame({'Percent': percent_y}, index = index) # Plot the horizontal bar chart ax = df.plot.barh() # plot the chart ax.invert_yaxis() # invert the y axis plt.xlabel(\"Percent\", loc='right', rotation=\"horizontal\") # place the x label ax.get_legend().remove() # remove the unnecessary legend ax.set_axisbelow(True) ax.grid(color='gray', which='major', axis='x', linestyle='-', alpha=0.2) # last 2 commands create and style the x axis gridlines plt.box(False) # remove plot borders plt.show() In this case we opted to display the chart, if you wish to save it then plt.savefig() function should be added at the end of the code.","title":"Plot 3 - Pupils who have ever smoked, by age [Horizontal bar chart]"},{"location":"training_resources/python/visualisation-in-python/#further-reading","text":"An Intuitive Guide to Data Visualisation in Python Top 6 Python Libraries for Visualisation Matplotlib Examples gallery Matplotlib functions summary","title":"Further reading"}]}